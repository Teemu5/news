{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "dataset = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12456032966886774649\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# List all devices\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "label=[]\n",
    "for row in dataset['train']['text']+dataset['test']['text']:\n",
    "    text.append(wordpunct_tokenize(row.lower()))\n",
    "for row in dataset['train']['label']+dataset['test']['label']:\n",
    "    label.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':0}\n",
    "for sent in text:    \n",
    "    for token in sent:        \n",
    "        if token not in word_dict:\n",
    "            word_dict[token]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH=256\n",
    "\n",
    "news_words = []\n",
    "for sent in text:       \n",
    "    sample=[]\n",
    "    for token in sent:     \n",
    "        sample.append(word_dict[token])\n",
    "    sample = sample[:MAX_SENT_LENGTH]\n",
    "    news_words.append(sample+[0]*(MAX_SENT_LENGTH-len(sample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "label=np.array(label,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.arange(len(label))\n",
    "train_index=index[:120000]\n",
    "np.random.shuffle(train_index)\n",
    "test_index=index[120000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "label=np.array(label,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index=np.arange(len(label))\n",
    "train_index=index[:120000]\n",
    "test_index=index[120000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Fastformer(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.now_input_shape=None\n",
    "        super(Fastformer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.now_input_shape=input_shape\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True) \n",
    "        self.Wq = self.add_weight(name='Wq', \n",
    "                                  shape=(self.output_dim,self.nb_head),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.Wk = self.add_weight(name='Wk', \n",
    "                                  shape=(self.output_dim,self.nb_head),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        self.WP = self.add_weight(name='WP', \n",
    "                                  shape=(self.output_dim,self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        \n",
    "        \n",
    "        super(Fastformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if len(x) == 2:\n",
    "            Q_seq,K_seq = x\n",
    "        elif len(x) == 4:\n",
    "            Q_seq,K_seq,Q_mask,K_mask = x #different mask lengths, reserved for cross attention\n",
    "        Q_seq = tf.linalg.matmul(Q_seq, self.WQ)        \n",
    "        Q_seq_reshape = tf.reshape(Q_seq, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "\n",
    "        Q_att = tf.transpose(tf.linalg.matmul(Q_seq_reshape, self.Wq), perm=(0, 2, 1)) / (self.size_per_head**0.5)\n",
    "        #Q_seq = K.dot(Q_seq, self.WQ)        \n",
    "        #Q_seq_reshape = K.reshape(Q_seq, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "\n",
    "        #Q_att=  K.permute_dimensions(K.dot(Q_seq_reshape, self.Wq),(0,2,1))/ self.size_per_head**0.5\n",
    "\n",
    "        if len(x)  == 4:\n",
    "            #Q_att = Q_att-(1-K.expand_dims(Q_mask,axis=1))*1e8\n",
    "            Q_att = Q_att - (1 - tf.expand_dims(Q_mask, axis=1)) * 1e8\n",
    "        \"\"\"\n",
    "        Q_att = K.softmax(Q_att)\n",
    "        Q_seq = K.reshape(Q_seq, (-1,self.now_input_shape[0][1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        \n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1,self.now_input_shape[1][1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "\n",
    "        Q_att = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=3),self.size_per_head,axis=3))(Q_att)\n",
    "        global_q = K.sum(multiply([Q_att, Q_seq]),axis=2)\n",
    "        \n",
    "        global_q_repeat = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=2), self.now_input_shape[1][1],axis=2))(global_q)\n",
    "\n",
    "        QK_interaction = multiply([K_seq, global_q_repeat])\n",
    "        QK_interaction_reshape = K.reshape(QK_interaction, (-1, self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        K_att = K.permute_dimensions(K.dot(QK_interaction_reshape, self.Wk),(0,2,1))/ self.size_per_head**0.5\n",
    "        \n",
    "        if len(x)  == 4:\n",
    "            K_att = K_att-(1-K.expand_dims(K_mask,axis=1))*1e8\n",
    "            \n",
    "        K_att = K.softmax(K_att)\n",
    "\n",
    "        K_att = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=3),self.size_per_head,axis=3))(K_att)\n",
    "\n",
    "        global_k = K.sum(multiply([K_att, QK_interaction]),axis=2)\n",
    "     \n",
    "        global_k_repeat = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,axis=2), self.now_input_shape[0][1],axis=2))(global_k)\n",
    "        #Q=V\n",
    "        QKQ_interaction = multiply([global_k_repeat, Q_seq])\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        QKQ_interaction = K.dot(QKQ_interaction, self.WP)\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head,self.size_per_head))\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = QKQ_interaction+Q_seq\n",
    "        QKQ_interaction = K.permute_dimensions(QKQ_interaction, (0,2,1,3))\n",
    "        QKQ_interaction = K.reshape(QKQ_interaction, (-1,self.now_input_shape[0][1], self.nb_head*self.size_per_head))\n",
    "        \"\"\"\n",
    "        #many operations can be optimized if higher versions are used. \n",
    "        # Softmax\n",
    "        #Q_att = tf.nn.softmax(Q_att)\n",
    "\n",
    "        # Reshape and transpose\n",
    "        #Q_seq = tf.reshape(Q_seq, (-1, self.now_input_shape[0][1], self.nb_head, self.size_per_head))\n",
    "        #Q_seq = tf.transpose(Q_seq, perm=(0, 2, 1, 3))\n",
    "        # Ensure WK is shaped as [size_per_head, size_per_head]\n",
    "        #if self.WK.shape != (self.size_per_head, self.size_per_head):\n",
    "        #    self.WK = tf.Variable(tf.random.normal([self.size_per_head, self.size_per_head]))\n",
    "        Q_att = K.softmax(Q_att)\n",
    "        Q_seq = K.reshape(Q_seq, (-1,self.now_input_shape[0][1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        # Matrix multiplication and reshaping\n",
    "        #K_seq = tf.matmul(Q_seq, self.WK)\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        \n",
    "        K_seq = tf.reshape(K_seq, (-1, self.now_input_shape[1][1], self.nb_head, self.size_per_head))\n",
    "        K_seq = tf.transpose(K_seq, perm=(0, 2, 1, 3))\n",
    "\n",
    "        # Repeat and sum\n",
    "        Q_att = tf.repeat(tf.expand_dims(Q_att, axis=3), repeats=self.size_per_head, axis=3)\n",
    "        global_q = tf.reduce_sum(tf.multiply(Q_att, Q_seq), axis=2)\n",
    "\n",
    "        # Repeat elements and reshape for interaction\n",
    "        global_q_repeat = tf.repeat(tf.expand_dims(global_q, axis=2), self.now_input_shape[1][1], axis=2)\n",
    "        QK_interaction = tf.multiply(K_seq, global_q_repeat)\n",
    "        QK_interaction_reshape = tf.reshape(QK_interaction, (-1, self.now_input_shape[0][1], self.nb_head * self.size_per_head))\n",
    "\n",
    "        # Another matrix multiplication and reshaping\n",
    "        K_att = tf.matmul(QK_interaction_reshape, self.Wk)\n",
    "        K_att = tf.transpose(K_att, perm=(0, 2, 1)) / (self.size_per_head ** 0.5)\n",
    "\n",
    "        if len(x) == 4:\n",
    "            K_att = K_att - (1 - tf.expand_dims(K_mask, axis=1)) * 1e8\n",
    "\n",
    "        # Final operations\n",
    "        K_att = tf.nn.softmax(K_att)\n",
    "        K_att = tf.repeat(tf.expand_dims(K_att, axis=3), repeats=self.size_per_head, axis=3)\n",
    "        global_k = tf.reduce_sum(tf.multiply(K_att, QK_interaction), axis=2)\n",
    "        global_k_repeat = tf.repeat(tf.expand_dims(global_k, axis=2), self.now_input_shape[0][1], axis=2)\n",
    "\n",
    "        # Final interaction\n",
    "        QKQ_interaction = tf.multiply(global_k_repeat, Q_seq)\n",
    "        QKQ_interaction = tf.transpose(QKQ_interaction, perm=(0, 2, 1, 3))\n",
    "        QKQ_interaction = tf.reshape(QKQ_interaction, (-1, self.now_input_shape[0][1], self.nb_head * self.size_per_head))\n",
    "        QKQ_interaction = tf.matmul(QKQ_interaction, self.WP)\n",
    "        QKQ_interaction = tf.reshape(QKQ_interaction, (-1, self.now_input_shape[0][1], self.nb_head, self.size_per_head))\n",
    "        QKQ_interaction = tf.transpose(QKQ_interaction, perm=(0, 2, 1, 3))\n",
    "        QKQ_interaction = QKQ_interaction + Q_seq\n",
    "        QKQ_interaction = tf.transpose(QKQ_interaction, perm=(0, 2, 1, 3))\n",
    "        QKQ_interaction = tf.reshape(QKQ_interaction, (-1, self.now_input_shape[0][1], self.nb_head * self.size_per_head))\n",
    "        return QKQ_interaction\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tempfile import TemporaryDirectory\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)  # Create directories if needed\n",
    "    with open(dest_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:  # Filter out keep-alive new chunks\n",
    "                file.write(chunk)\n",
    "\n",
    "# Initialize temporary directory and paths\n",
    "tmpdir = TemporaryDirectory()\n",
    "data_path = \"dataset\"#tmpdir.name\n",
    "print(data_path)\n",
    "train_news_file = os.path.join('news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', 'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', 'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', 'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path, \"utils\", 'nrms.yaml')\n",
    "\n",
    "# Get MIND dataset URLs\n",
    "#mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "mind_url = \"https://recodatasets.z20.web.core.windows.net/newsrec/\"\n",
    "mind_dev_dataset = \"MINDlarge_dev.zip\"\n",
    "mind_train_dataset = \"MINDlarge_train.zip\"\n",
    "train_news_dataset = os.path.join(data_path, 'train', mind_train_dataset)\n",
    "valid_news_dataset = os.path.join(data_path, 'valid', mind_dev_dataset)\n",
    "# Download train dataset if not already present\n",
    "if not os.path.exists(mind_train_dataset):\n",
    "    download_file(os.path.join(mind_url, mind_train_dataset), os.path.join(data_path, 'train', mind_train_dataset))\n",
    "\n",
    "# Download validation dataset if not already present\n",
    "if not os.path.exists(mind_dev_dataset):\n",
    "    download_file(os.path.join(mind_url, mind_dev_dataset), os.path.join(data_path, 'valid', mind_dev_dataset))\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(os.path.join(data_path, 'train', mind_train_dataset), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "tokenizer start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\t\\fastformer-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer end\n",
      "tokenizer start\n",
      "tokenizer end\n",
      "replaced nan\n",
      "news df fit transformed\n",
      "behaviours fillna\n",
      "process behaviours start\n",
      "process behaviours end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download utility files if not already present\n",
    "#if not os.path.exists(yaml_file):\n",
    "#    for util_file in mind_utils:\n",
    "#        download_file(os.path.join('https://recodatasets.z20.web.core.windows.net/newsrec/', util_file),\n",
    "#                      os.path.join(data_path, 'utils', util_file))\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load data\n",
    "behaviors_df = pd.read_csv('behaviors.tsv', sep='\\t', header=None, names=['UserID', 'Time', 'ClickedNews', 'Impressions'])\n",
    "news_df = pd.read_csv('news.tsv', sep='\\t', header=None, names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Entities', 'Keywords'])\n",
    "print(\"data loaded\")\n",
    "print(\"tokenizer start\")\n",
    "# Tokenize text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "news_df['TitleTokens'] = news_df['Title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=32))\n",
    "print(\"tokenizer end\")\n",
    "# Replace NaN values in the 'Abstract' column with an empty string\n",
    "news_df['Abstract'] = news_df['Abstract'].fillna('')\n",
    "# Tokenize the 'Abstract' column\n",
    "\n",
    "print(\"tokenizer start\")\n",
    "news_df['AbstractTokens'] = news_df['Abstract'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=128))\n",
    "\n",
    "print(\"tokenizer end\")\n",
    "print(\"replaced nan\")\n",
    "# Encode categories and subcategories\n",
    "category_encoder = LabelEncoder()\n",
    "sub_category_encoder = LabelEncoder()\n",
    "\n",
    "news_df['CategoryID'] = category_encoder.fit_transform(news_df['Category'])\n",
    "news_df['SubCategoryID'] = sub_category_encoder.fit_transform(news_df['SubCategory'])\n",
    "print(\"news df fit transformed\")\n",
    "# Map news ID to the processed news information\n",
    "news_dict = {row['NewsID']: row for _, row in news_df.iterrows()}\n",
    "\n",
    "# Replace NaN values in 'ClickedNews' and 'Impressions' columns with empty strings\n",
    "behaviors_df['ClickedNews'] = behaviors_df['ClickedNews'].fillna('')\n",
    "behaviors_df['Impressions'] = behaviors_df['Impressions'].fillna('')\n",
    "print(\"behaviours fillna\")\n",
    "# Process user behaviors\n",
    "def process_behaviors(row):\n",
    "    #print(row['ClickedNews'])\n",
    "    clicked_news = row['ClickedNews'].split(' ')\n",
    "    impressions = [imp.split('-') for imp in row['Impressions'].split(' ')]\n",
    "    return clicked_news, impressions\n",
    "print(\"process behaviours start\")\n",
    "behaviors_df['ClickedNewsList'], behaviors_df['ImpressionsList'] = zip(*behaviors_df.apply(process_behaviors, axis=1))\n",
    "print(\"process behaviours end\")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, behaviors, news_dict):\n",
    "        self.behaviors = behaviors\n",
    "        self.news_dict = news_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.behaviors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clicked_news = [self.news_dict[nid]['TitleTokens'] for nid in self.behaviors.iloc[idx]['ClickedNewsList']]\n",
    "        candidate_news = [self.news_dict[nid]['TitleTokens'] for nid, _ in self.behaviors.iloc[idx]['ImpressionsList']]\n",
    "        labels = [int(label) for _, label in self.behaviors.iloc[idx]['ImpressionsList']]\n",
    "        return torch.tensor(clicked_news), torch.tensor(candidate_news), torch.tensor(labels)\n",
    "\n",
    "# Next step, training models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users:711222\n",
      "len(clicked_news_global)=67183379\n",
      "len(targets_global)=67183379\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(targets_global)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(targets_global)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Tokenize the input sequences and pad them\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m clicked_news_padded \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m([[news_dict[nid][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitleTokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m seq] \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m clicked_news_global],\n\u001b[0;32m     76\u001b[0m                                     maxlen\u001b[38;5;241m=\u001b[39mMAX_SENT_LENGTH, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, truncating\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Convert the target articles into embeddings (you may tokenize them similarly)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m target_news_padded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([news_dict[nid][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitleTokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m targets_global])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, Dropout, LayerNormalization, add, Flatten, Dense, Activation, Dot, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "@tf.function\n",
    "def predict_user_model(model, clicked_news):\n",
    "    return model(clicked_news, training=False)  # Ensure training=False for inference\n",
    "\n",
    "def build_model():\n",
    "    text_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    qmask = Lambda(lambda x: tf.cast(tf.cast(x, tf.bool), tf.float32), \n",
    "                   output_shape=lambda s: s)(text_input)\n",
    "    word_emb = Embedding(word_dict_size, 256, trainable=True)(text_input)\n",
    "\n",
    "    word_emb = Dropout(0.2)(word_emb)\n",
    "    hidden_word_emb = Fastformer(16, 16)([word_emb, word_emb, qmask, qmask])\n",
    "    hidden_word_emb = Dropout(0.2)(hidden_word_emb)\n",
    "    hidden_word_emb = LayerNormalization()(add([word_emb, hidden_word_emb]))\n",
    "\n",
    "    hidden_word_emb_layer2 = Fastformer(16, 16)([hidden_word_emb, hidden_word_emb, qmask, qmask])\n",
    "    hidden_word_emb_layer2 = Dropout(0.2)(hidden_word_emb_layer2)\n",
    "    hidden_word_emb_layer2 = LayerNormalization()(add([hidden_word_emb, hidden_word_emb_layer2]))\n",
    "\n",
    "    word_att = Flatten()(Dense(1)(hidden_word_emb_layer2))\n",
    "    word_att = Activation('softmax')(word_att)\n",
    "    text_emb = Dot((1, 1))([hidden_word_emb_layer2, word_att])\n",
    "    classifier = Dense(4, activation='softmax')(text_emb)\n",
    "\n",
    "    model = Model([text_input], [classifier])\n",
    "    model.compile(loss=['categorical_crossentropy'], \n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "# Parameters for the model\n",
    "MAX_SENT_LENGTH = 256  # adjust based on your dataset\n",
    "word_dict_size = len(word_dict)  # Vocabulary size from tokenizer or word_dict\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "print(f\"number of users:{len(behaviors_df['UserID'].unique())}\")\n",
    "save_dir = \"user_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "def generate_input_target_pairs(clicked_news_list, window_size=3):\n",
    "    input_sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sliding window input-target pairs\n",
    "    for i in range(len(clicked_news_list) - window_size):\n",
    "        input_seq = clicked_news_list[i:i + window_size]  # Input: a sequence of articles\n",
    "        target = clicked_news_list[i + window_size]  # Target: the next clicked article\n",
    "        input_sequences.append(input_seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return input_sequences, targets\n",
    "\n",
    "# Apply this to the behavior data for each user\n",
    "clicked_news_global = []\n",
    "targets_global = []\n",
    "\n",
    "for idx, row in behaviors_df.iterrows():\n",
    "    clicked_news_list = row['ClickedNewsList']  # This is a list of clicked article IDs\n",
    "    input_seqs, target_articles = generate_input_target_pairs(clicked_news_list)\n",
    "    \n",
    "    clicked_news_global.extend(input_seqs)  # Append all sequences to the global list\n",
    "    targets_global.extend(target_articles)  # Append all targets\n",
    "print(f\"len(clicked_news_global)={len(clicked_news_global)}\")\n",
    "print(f\"len(targets_global)={len(targets_global)}\")\n",
    "# Tokenize the input sequences and pad them\n",
    "clicked_news_padded = pad_sequences([[news_dict[nid]['TitleTokens'] for nid in seq] for seq in clicked_news_global],\n",
    "                                    maxlen=MAX_SENT_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Convert the target articles into embeddings (you may tokenize them similarly)\n",
    "target_news_padded = np.array([news_dict[nid]['TitleTokens'] for nid in targets_global])\n",
    "print(f\"len(clicked_news_padded)={len(clicked_news_padded)}\")\n",
    "print(f\"len(targets_global)={len(targets_global)}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "split_idx = int(0.8 * len(clicked_news_padded))\n",
    "train_index = np.arange(split_idx)\n",
    "test_index = np.arange(split_idx, len(clicked_news_padded))\n",
    "\n",
    "# Convert target articles to one-hot encoding if necessary (for classification)\n",
    "# If predicting classes, use `to_categorical` to one-hot encode the target labels\n",
    "target_labels_onehot = to_categorical(target_news_padded, num_classes=4)\n",
    "\n",
    "print(f\"len(target_labels_onehot)={len(target_labels_onehot)}\")\n",
    "print(f\"clicked_news_padded={clicked_news_padded}\")\n",
    "print(f\"target_labels_onehot={target_labels_onehot}\")\n",
    "# Now fit the model\n",
    "model.fit(clicked_news_padded[train_index], \n",
    "          target_labels_onehot[train_index], \n",
    "          shuffle=True, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          verbose=1)\n",
    "model_save_path = os.path.join(save_dir, f\"global_fastformer_model.h5\")\n",
    "model.save(model_save_path)\n",
    "\n",
    "if True:\n",
    "    print(f\"Evaluating model: {model_save_path}\")\n",
    "    y_pred = predict_user_model(model, clicked_news_padded[test_index])\n",
    "    #y_pred = model.predict(clicked_news[test_index], batch_size=128, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = labels[test_index]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\" model: {model_save_path} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() \n",
    "import tensorflow as tf\n",
    "text_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "#qmask=Lambda(lambda x:  K.cast(K.cast(x,'bool'),'float32'))(text_input)\n",
    "#qmask = Lambda(lambda x: K.cast(K.cast(x, 'bool'), 'float32'), \n",
    "#               output_shape=lambda s: s)(text_input)\n",
    "qmask = Lambda(lambda x: tf.cast(tf.cast(x, tf.bool), tf.float32), \n",
    "               output_shape=lambda s: s)(text_input)\n",
    "word_emb = Embedding(len(word_dict),256, trainable=True)(text_input)\n",
    "\n",
    "#pos_emb = Embedding(MAX_SENT_LENGTH, 256, trainable=True)(Lambda(lambda x:K.zeros_like(x,dtype='int32')+K.arange(x.shape[1]))(text_input))\n",
    "#word_emb  =add([word_emb ,pos_emb])\n",
    "#We find that position embedding is not important on this dataset and we removed it for simplicity. If needed, please uncomment the two lines above\n",
    "\n",
    "word_emb=Dropout(0.2)(word_emb)\n",
    "\n",
    "hidden_word_emb = Fastformer(16,16)([word_emb,word_emb,qmask,qmask])\n",
    "hidden_word_emb = Dropout(0.2)(hidden_word_emb)\n",
    "hidden_word_emb = LayerNormalization()(add([word_emb,hidden_word_emb])) \n",
    "#if there is no layer norm in old version, please import an external layernorm class from a higher version.\n",
    "\n",
    "hidden_word_emb_layer2 = Fastformer(16,16)([hidden_word_emb,hidden_word_emb,qmask,qmask])\n",
    "hidden_word_emb_layer2 = Dropout(0.2)(hidden_word_emb_layer2)\n",
    "hidden_word_emb_layer2 = LayerNormalization()(add([hidden_word_emb,hidden_word_emb_layer2]))\n",
    "\n",
    "#without FFNN for simplicity\n",
    "\n",
    "word_att = Flatten()(Dense(1)(hidden_word_emb_layer2))\n",
    "word_att = Activation('softmax')(word_att)\n",
    "text_emb = Dot((1, 1))([hidden_word_emb_layer2 , word_att])\n",
    "classifier = Dense(4, activation='softmax')(text_emb)\n",
    "                                      \n",
    "model = Model([text_input], [classifier])\n",
    "#model.compile(loss=['categorical_crossentropy'],optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "model.compile(loss=['categorical_crossentropy'], \n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['acc'])\n",
    "\n",
    "for i in range(1):\n",
    "    model.fit(news_words[train_index],to_categorical(label)[train_index],shuffle=True,batch_size=64, epochs=1,verbose=1)\n",
    "\n",
    "\n",
    "    y_pred = model.predict([news_words[test_index] ], batch_size=128, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_true = label[test_index]\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = f1_score(y_true, y_pred, average='macro')  \n",
    "    print(acc)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_categorical(label, num_classes=4)[11])\n",
    "print(to_categorical(labels, num_classes=4)[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(clicked_news))\n",
    "print(labels)\n",
    "print(clicked_news)\n",
    "print(clicked_news_padded)\n",
    "user_id = \"U532401\"\n",
    "user_behaviors = behaviors_df[behaviors_df['UserID'] == user_id]\n",
    "print(user_behaviors['ClickedNewsList'].explode())\n",
    "# Extract impressions for this user, which includes both the articles shown and the labels\n",
    "impressions = user_behaviors['ImpressionsList'].explode()\n",
    "\n",
    "# Extract the news articles and the corresponding labels from the impressions\n",
    "clicked_news2 = [news_dict[nid]['TitleTokens'] for nid, label in impressions]\n",
    "labels2 = [int(label) for nid, label in impressions]\n",
    "print(impressions)\n",
    "print(clicked_news2)\n",
    "print(labels2)\n",
    "print(len(clicked_news2))\n",
    "print(len(labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_for_each_user:\n",
    "    # Loop through each user and train a model based on their specific data\n",
    "    for user_id in behaviors_df['UserID'].unique():\n",
    "        keras.backend.clear_session()  # Clear previous model to start fresh for each user\n",
    "        \n",
    "        # Filter the dataset for the current user\n",
    "        user_behaviors = behaviors_df[behaviors_df['UserID'] == user_id]\n",
    "        \"\"\"\n",
    "        # Extract the news interactions for this user\n",
    "        clicked_news = [news_dict[nid]['TitleTokens'] for nid in user_behaviors['ClickedNewsList'].explode()]\n",
    "        labels = [int(label) for nid, label in user_behaviors['ImpressionsList'].explode()]\n",
    "    \n",
    "        # Convert clicked news and labels to numpy arrays\n",
    "        clicked_news = np.array(clicked_news)\n",
    "        labels = np.array(labels)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "        \n",
    "        # Assume MAX_SENT_LENGTH is defined as 256\n",
    "        MAX_SENT_LENGTH = 256\n",
    "        \n",
    "        # Extract the news interactions for this user and pad sequences to the same length\n",
    "        #clicked_news = [news_dict[nid]['TitleTokens'] for nid in user_behaviors['ClickedNewsList'].explode()]\n",
    "        \n",
    "        # Pad sequences to ensure they have the same length (MAX_SENT_LENGTH)\n",
    "        #clicked_news_padded = pad_sequences(clicked_news, maxlen=MAX_SENT_LENGTH, padding='post', truncating='post')\n",
    "        \n",
    "        # Extract labels for this user\n",
    "        #labels = [int(label) for nid, label in user_behaviors['ImpressionsList'].explode()]\n",
    "    \n",
    "        impressions = user_behaviors['ImpressionsList'].explode()\n",
    "        \n",
    "        # Extract the news articles and the corresponding labels from the impressions\n",
    "        clicked_news = [news_dict[nid]['TitleTokens'] for nid, label in impressions]\n",
    "        labels = [int(label) for nid, label in impressions]\n",
    "        clicked_news_padded = pad_sequences(clicked_news, maxlen=MAX_SENT_LENGTH, padding='post', truncating='post')\n",
    "        # Convert clicked news and labels to numpy arrays\n",
    "        clicked_news_padded = np.array(clicked_news_padded)\n",
    "        labels = np.array(labels)\n",
    "        clicked_news = clicked_news_padded\n",
    "        # Train/test split (you can adjust this as needed)\n",
    "        split_idx = int(0.8 * len(clicked_news))\n",
    "        import random\n",
    "        #index=np.arange(len(label))\n",
    "        #train_index=index[:120000]\n",
    "        #test_index=index[120000:]\n",
    "        train_index = np.arange(split_idx)\n",
    "        test_index = np.arange(split_idx, len(clicked_news))\n",
    "    \n",
    "        # Build the model (same model as in the original code)\n",
    "        model = build_model()\n",
    "    \n",
    "        # Train the model using the user-specific data\n",
    "        print(f\"Training model for user {user_id}. Click news:{len(clicked_news)}\")\n",
    "        model.fit(clicked_news[train_index], \n",
    "                  to_categorical(labels, num_classes=4)[train_index], \n",
    "                  shuffle=True, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  verbose=1)\n",
    "        model_save_path = os.path.join(save_dir, f\"user_{user_id}_model.h5\")\n",
    "        model.save(model_save_path)\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Model for user {user_id} saved to {model_save_path}\")\n",
    "        # Predict and evaluate on the test data\n",
    "        if False:\n",
    "            print(f\"Evaluating model for user {user_id}...\")\n",
    "            y_pred = predict_user_model(model, clicked_news_padded[test_index])\n",
    "            #y_pred = model.predict(clicked_news[test_index], batch_size=128, verbose=1)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            y_true = labels[test_index]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro')\n",
    "            \n",
    "            print(f\"User {user_id} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
