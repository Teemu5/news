{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2747c3fa-3611-4cb1-af69-f8466e726bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "\n",
      "Sample HistoryCategories:\n",
      "    UserID                                  HistoryCategories\n",
      "0   U87243  [tv, news, music, health, music, news, lifesty...\n",
      "1  U598644  [travel, sports, video, travel, foodanddrink, ...\n",
      "2  U532401  [tv, movies, video, tv, news, music, lifestyle...\n",
      "3  U593596  [news, lifestyle, movies, entertainment, trave...\n",
      "4  U239687  [finance, movies, movies, news, news, news, sp...\n",
      "\n",
      "Created user_category_profiles:\n",
      "         Category_tv  Category_news  Category_music  Category_health  \\\n",
      "U87243           8.0           12.0             8.0              4.0   \n",
      "U598644          0.0           90.0             9.0              0.0   \n",
      "U532401         12.0            9.0             3.0              0.0   \n",
      "U593596          0.0           16.0             0.0              0.0   \n",
      "U239687        168.0         3388.0           112.0            140.0   \n",
      "\n",
      "         Category_lifestyle  Category_foodanddrink  Category_finance  \\\n",
      "U87243                 12.0                    8.0               4.0   \n",
      "U598644                18.0                    9.0               0.0   \n",
      "U532401                15.0                    0.0               0.0   \n",
      "U593596                 8.0                   16.0               0.0   \n",
      "U239687               252.0                   84.0            1400.0   \n",
      "\n",
      "         Category_travel  Category_entertainment  Category_sports  \\\n",
      "U87243               4.0                     4.0              0.0   \n",
      "U598644             18.0                     0.0             45.0   \n",
      "U532401              0.0                     0.0              0.0   \n",
      "U593596             24.0                     8.0              8.0   \n",
      "U239687            644.0                   168.0           2660.0   \n",
      "\n",
      "         Category_video  Category_autos  Category_movies  Category_weather  \\\n",
      "U87243              0.0             0.0              0.0               0.0   \n",
      "U598644            18.0             9.0              0.0               0.0   \n",
      "U532401             6.0             0.0              3.0               0.0   \n",
      "U593596             8.0             8.0              8.0               0.0   \n",
      "U239687            84.0            84.0            112.0             196.0   \n",
      "\n",
      "         Category_kids  Category_middleeast  Category_games  \n",
      "U87243             0.0                  0.0             0.0  \n",
      "U598644            0.0                  0.0             0.0  \n",
      "U532401            0.0                  0.0             0.0  \n",
      "U593596            0.0                  0.0             0.0  \n",
      "U239687            0.0                  0.0             0.0  \n",
      "\n",
      "Shape of user_category_profiles: (698365, 17)\n",
      "\n",
      "Created user_category_profiles with 711222 users and 18 categories.\n",
      "\n",
      "Filtered user_category_profiles with Top N Categories and 'Other':\n",
      "         Category_autos  Category_entertainment  Category_finance  \\\n",
      "UserID                                                              \n",
      "U87243                0                       4                 4   \n",
      "U598644               9                       0                 0   \n",
      "U532401               0                       0                 0   \n",
      "U593596               8                       8                 0   \n",
      "U239687              84                     168              1400   \n",
      "\n",
      "         Category_foodanddrink  Category_games  Category_health  \\\n",
      "UserID                                                            \n",
      "U87243                       8               0                4   \n",
      "U598644                      9               0                0   \n",
      "U532401                      0               0                0   \n",
      "U593596                     16               0                0   \n",
      "U239687                     84               0              140   \n",
      "\n",
      "         Category_kids  Category_lifestyle  Category_middleeast  \\\n",
      "UserID                                                            \n",
      "U87243               0                  12                    0   \n",
      "U598644              0                  18                    0   \n",
      "U532401              0                  15                    0   \n",
      "U593596              0                   8                    0   \n",
      "U239687              0                 252                    0   \n",
      "\n",
      "         Category_movies  Category_music  Category_news  Category_sports  \\\n",
      "UserID                                                                     \n",
      "U87243                 0               8             12                0   \n",
      "U598644                0               9             90               45   \n",
      "U532401                3               3              9                0   \n",
      "U593596                8               0             16                8   \n",
      "U239687              112             112           3388             2660   \n",
      "\n",
      "         Category_travel  Category_tv  Category_video  Category_weather  \\\n",
      "UserID                                                                    \n",
      "U87243                 4            8               0                 0   \n",
      "U598644               18            0              18                 0   \n",
      "U532401                0           12               6                 0   \n",
      "U593596               24            0               8                 0   \n",
      "U239687              644          168              84               196   \n",
      "\n",
      "         Category_Other  \n",
      "UserID                   \n",
      "U87243                0  \n",
      "U598644               0  \n",
      "U532401               0  \n",
      "U593596               0  \n",
      "U239687               0  \n",
      "\n",
      "Shape of filtered_user_category_profiles: (711222, 18)\n",
      "\n",
      "Saved user_category_profiles to user_category_profiles.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def is_colab():\n",
    "    return 'COLAB_GPU' in os.environ\n",
    "zip_file = f\"MINDlarge_train.zip\"\n",
    "valid_zip_file = f\"MINDlarge_test.zip\"\n",
    "data_dir = 'dataset/train/'  # Adjust path as necessary\n",
    "valid_data_dir = 'dataset/valid/'  # Adjust path as necessary\n",
    "if is_colab():\n",
    "    print(\"Running on Google colab\")\n",
    "    data_dir = '/content/train/'\n",
    "    valid_data_dir = '/content/valid/'\n",
    "#data_dir = 'dataset/small/train/'  # Adjust path as necessary\n",
    "#zip_file = f\"MINDsmall_train.zip\"\n",
    "zip_file_path = f\"{data_dir}{zip_file}\"\n",
    "valid_zip_file_path = f\"{valid_data_dir}{valid_zip_file}\"\n",
    "# Get the directory where the zip file is located\n",
    "output_folder = os.path.dirname(zip_file_path)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_folder)\n",
    "if is_colab():\n",
    "  valid_output_folder = os.path.dirname(valid_zip_file_path)\n",
    "  with zipfile.ZipFile(valid_zip_file_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(os.path.dirname(valid_output_folder))\n",
    "news_file = 'news.tsv'\n",
    "behaviors_file = 'behaviors.tsv'\n",
    "\n",
    "# Load news data\n",
    "news_path = os.path.join(data_dir, news_file)\n",
    "news_df = pd.read_csv(\n",
    "    news_path,\n",
    "    sep='\\t',\n",
    "    names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"Loaded news data:\")\n",
    "print(news_df.head())\n",
    "\n",
    "# Load behaviors data\n",
    "behaviors_path = os.path.join(data_dir, behaviors_file)\n",
    "behaviors_df = pd.read_csv(\n",
    "    behaviors_path,\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded behaviors data:\")\n",
    "print(behaviors_df.head())\n",
    "# Handle missing 'HistoryText' by replacing NaN with empty string\n",
    "behaviors_df['HistoryText'] = behaviors_df['HistoryText'].fillna('')\n",
    "\n",
    "# Create a NewsID to Category mapping\n",
    "newsid_to_category = news_df.set_index('NewsID')['Category'].to_dict()\n",
    "\n",
    "# Function to extract categories from HistoryText\n",
    "def extract_categories(history_text):\n",
    "    if not history_text:\n",
    "        return []\n",
    "    news_ids = history_text.split(' ')\n",
    "    categories = [newsid_to_category.get(news_id, 'Unknown') for news_id in news_ids]\n",
    "    return categories\n",
    "\n",
    "# Apply the function to extract categories\n",
    "behaviors_df['HistoryCategories'] = behaviors_df['HistoryText'].apply(extract_categories)\n",
    "\n",
    "print(\"\\nSample HistoryCategories:\")\n",
    "print(behaviors_df[['UserID', 'HistoryCategories']].head())\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to hold category counts per user\n",
    "user_category_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Populate the dictionary\n",
    "for idx, row in behaviors_df.iterrows():\n",
    "    user_id = row['UserID']\n",
    "    categories = row['HistoryCategories']\n",
    "    for category in categories:\n",
    "        user_category_counts[user_id][category] += 1\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "user_category_profiles = pd.DataFrame(user_category_counts).T.fillna(0)\n",
    "\n",
    "# Optionally, rename columns to indicate category\n",
    "user_category_profiles.columns = [f'Category_{cat}' for cat in user_category_profiles.columns]\n",
    "\n",
    "print(\"\\nCreated user_category_profiles:\")\n",
    "print(user_category_profiles.head())\n",
    "print(f\"\\nShape of user_category_profiles: {user_category_profiles.shape}\")\n",
    "# Handle missing 'HistoryText' by replacing NaN with empty string\n",
    "behaviors_df['HistoryText'] = behaviors_df['HistoryText'].fillna('')\n",
    "\n",
    "# Create a NewsID to Category mapping\n",
    "newsid_to_category = news_df.set_index('NewsID')['Category'].to_dict()\n",
    "\n",
    "# Get all unique UserIDs from behaviors_df\n",
    "unique_user_ids = behaviors_df['UserID'].unique()\n",
    "\n",
    "# Function to extract categories from HistoryText\n",
    "def extract_categories(history_text):\n",
    "    if not history_text:\n",
    "        return []\n",
    "    news_ids = history_text.split(' ')\n",
    "    categories = [newsid_to_category.get(news_id, 'Unknown') for news_id in news_ids]\n",
    "    return categories\n",
    "\n",
    "# Apply the function to extract categories\n",
    "behaviors_df['HistoryCategories'] = behaviors_df['HistoryText'].apply(extract_categories)\n",
    "\n",
    "# Explode 'HistoryCategories' to have one category per row\n",
    "behaviors_exploded = behaviors_df[['UserID', 'HistoryCategories']].explode('HistoryCategories')\n",
    "\n",
    "# Replace missing categories with 'Unknown'\n",
    "behaviors_exploded['HistoryCategories'] = behaviors_exploded['HistoryCategories'].fillna('Unknown')\n",
    "\n",
    "# Create a cross-tabulation (pivot table) of counts\n",
    "user_category_counts = pd.crosstab(\n",
    "    index=behaviors_exploded['UserID'],\n",
    "    columns=behaviors_exploded['HistoryCategories']\n",
    ")\n",
    "\n",
    "# Rename columns to include 'Category_' prefix\n",
    "user_category_counts.columns = [f'Category_{col}' for col in user_category_counts.columns]\n",
    "\n",
    "# Reindex to include all users, filling missing values with zero\n",
    "user_category_profiles = user_category_counts.reindex(unique_user_ids, fill_value=0)\n",
    "\n",
    "print(f\"\\nCreated user_category_profiles with {user_category_profiles.shape[0]} users and {user_category_profiles.shape[1]} categories.\")\n",
    "\n",
    "# Determine top N categories\n",
    "top_n = 20\n",
    "category_counts = news_df['Category'].value_counts()\n",
    "top_categories = category_counts.nlargest(top_n).index.tolist()\n",
    "\n",
    "# Get the category names without the 'Category_' prefix\n",
    "user_category_columns = user_category_profiles.columns.str.replace('Category_', '')\n",
    "\n",
    "# Filter columns in user_category_profiles that are in top_categories\n",
    "filtered_columns = user_category_profiles.columns[user_category_columns.isin(top_categories)]\n",
    "\n",
    "# Create filtered_user_category_profiles with these columns\n",
    "filtered_user_category_profiles = user_category_profiles[filtered_columns]\n",
    "\n",
    "# Identify columns that are not in top_categories to sum them into 'Category_Other'\n",
    "other_columns = user_category_profiles.columns[~user_category_columns.isin(top_categories)]\n",
    "\n",
    "# Sum the 'Other' categories\n",
    "filtered_user_category_profiles['Category_Other'] = user_category_profiles[other_columns].sum(axis=1)\n",
    "\n",
    "# Now, get the actual categories present after filtering\n",
    "actual_categories = filtered_columns.str.replace('Category_', '').tolist()\n",
    "\n",
    "# Add 'Other' to the list\n",
    "actual_categories.append('Other')\n",
    "\n",
    "# Assign new column names\n",
    "filtered_user_category_profiles.columns = [f'Category_{cat}' for cat in actual_categories]\n",
    "print(\"\\nFiltered user_category_profiles with Top N Categories and 'Other':\")\n",
    "print(filtered_user_category_profiles.head())\n",
    "print(f\"\\nShape of filtered_user_category_profiles: {filtered_user_category_profiles.shape}\")\n",
    "\n",
    "# Save the user_category_profiles to a file for future use\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "filtered_user_category_profiles.to_pickle(user_category_profiles_path)\n",
    "user_category_profiles = filtered_user_category_profiles\n",
    "print(f\"\\nSaved user_category_profiles to {user_category_profiles_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0340fc-fdb5-4829-9ada-e0ae091be118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in user_category_profiles: 18\n",
      "Number of new column names: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns in user_category_profiles: {len(user_category_profiles.columns)}\")\n",
    "print(f\"Number of new column names: {len(actual_categories)}\")\n",
    "# Number of unique users in behaviors_df\n",
    "unique_user_ids = behaviors_df['UserID'].unique()\n",
    "print(f\"Number of unique users in behaviors_df: {len(unique_user_ids)}\")\n",
    "# Number of unique users in behaviors_df\n",
    "unique_user_ids = behaviors_df['UserID'].unique()\n",
    "print(f\"Number of unique users in behaviors_df: {len(unique_user_ids)}\")\n",
    "\n",
    "# Number of users in user_category_profiles\n",
    "user_profile_ids = user_category_profiles.index.unique()\n",
    "print(f\"Number of users in user_category_profiles: {len(user_profile_ids)}\")\n",
    "\n",
    "# Find missing UserIDs\n",
    "missing_user_ids = set(unique_user_ids) - set(user_profile_ids)\n",
    "print(f\"Number of missing UserIDs in user_category_profiles: {len(missing_user_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29cb85-56bd-4062-95ff-3252eb6e920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 23:17:58.959006: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-20 23:17:59.746524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-20 23:18:00.222727: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-20 23:18:00.339766: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-20 23:18:00.571190: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 23:18:01.492273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in behaviors_df: 711222\n",
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Vocabulary Size: 88583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 2232748/2232748 [30:02<00:00, 1238.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train_df with 83507374 samples.\n",
      "Columns in train_df:\n",
      "Index(['UserID', 'HistoryTitles', 'CandidateTitleTokens', 'Label'], dtype='object')\n",
      "All 'UserID's are present in user_category_profiles.\n",
      "Assigned clusters to users. Number of clusters: 3\n",
      "Assigning cluster labels to train_df using map...\n",
      "All samples have cluster assignments.\n",
      "Cluster 0: 74917882 test samples.\n",
      "Cluster 1: 7964412 test samples.\n",
      "Cluster 2: 625080 test samples.\n",
      "\n",
      "After label balancing (0 vs 1):\n",
      "Label\n",
      "0    91017\n",
      "1    91017\n",
      "Name: count, dtype: int64\n",
      "Balanced cluster sizes:\n",
      "Cluster 0: 55246 samples\n",
      "Cluster 1: 60494 samples\n",
      "Cluster 2: 66294 samples\n",
      "Balanced dataset:\n",
      "Cluster\n",
      "2    66294\n",
      "1    60494\n",
      "0    55246\n",
      "Name: count, dtype: int64\n",
      "Original size: 182034\n",
      "Sampled size: 182034\n",
      "Columns in sampled train_df:\n",
      "Index(['UserID', 'HistoryTitles', 'CandidateTitleTokens', 'Label', 'Cluster'], dtype='object')\n",
      "Cluster:122208    1\n",
      "95960     2\n",
      "23136     1\n",
      "178058    0\n",
      "9570      1\n",
      "         ..\n",
      "119879    2\n",
      "103694    2\n",
      "131932    0\n",
      "146867    0\n",
      "121958    2\n",
      "Name: Cluster, Length: 182034, dtype: int64\n",
      "Splitting data into training and validation sets for each cluster...\n",
      "Cluster 0: 44196 training samples, 11050 validation samples.\n",
      "Cluster 1: 48395 training samples, 12099 validation samples.\n"
     ]
    }
   ],
   "source": [
    "# --- [Imports and Constants] ---\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dot, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- [Cleaning Function] ---\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# --- [Data Preparation Function] ---\n",
    "def prepare_train_df(\n",
    "    data_dir,\n",
    "    news_file,\n",
    "    behaviours_file,\n",
    "    user_category_profiles,\n",
    "    num_clusters=3,\n",
    "    fraction=1,\n",
    "    max_title_length=30,\n",
    "    max_history_length=50\n",
    "):\n",
    "    # Load news data\n",
    "    news_path = os.path.join(data_dir, news_file)\n",
    "    news_df = pd.read_csv(\n",
    "        news_path,\n",
    "        sep='\\t',\n",
    "        names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'],\n",
    "        index_col=False\n",
    "    )\n",
    "    print(\"Loaded news data:\")\n",
    "    print(news_df.head())\n",
    "\n",
    "    # Load behaviors data\n",
    "    behaviors_path = os.path.join(data_dir, behaviours_file)\n",
    "    behaviors_df = pd.read_csv(\n",
    "        behaviors_path,\n",
    "        sep='\\t',\n",
    "        names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "        index_col=False\n",
    "    )\n",
    "    print(\"Loaded behaviors data:\")\n",
    "    print(behaviors_df.head())\n",
    "\n",
    "    # Clean titles and abstracts\n",
    "    news_df['CleanTitle'] = news_df['Title'].apply(clean_text)\n",
    "    news_df['CleanAbstract'] = news_df['Abstract'].apply(clean_text)\n",
    "\n",
    "    # Create a combined text field\n",
    "    news_df['CombinedText'] = news_df['CleanTitle'] + ' ' + news_df['CleanAbstract']\n",
    "\n",
    "    # Initialize and fit tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(news_df['CombinedText'].tolist())\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "    # Save tokenizer for future use\n",
    "    with open('tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    # Encode and pad CombinedText\n",
    "    news_df['EncodedText'] = tokenizer.texts_to_sequences(news_df['CombinedText'])\n",
    "    news_df['PaddedText'] = list(pad_sequences(news_df['EncodedText'], maxlen=max_title_length, padding='post', truncating='post'))\n",
    "\n",
    "    # Create a mapping from NewsID to PaddedText\n",
    "    news_text_dict = dict(zip(news_df['NewsID'], news_df['PaddedText']))\n",
    "\n",
    "    # Function to parse impressions and labels\n",
    "    def parse_impressions(impressions):\n",
    "        impression_list = impressions.split()\n",
    "        news_ids = []\n",
    "        labels = []\n",
    "        for imp in impression_list:\n",
    "            try:\n",
    "                news_id, label = imp.split('-')\n",
    "                news_ids.append(news_id)\n",
    "                labels.append(int(label))\n",
    "            except ValueError:\n",
    "                # Handle cases where split does not result in two items\n",
    "                continue\n",
    "        return news_ids, labels\n",
    "\n",
    "    # Apply parsing to behaviors data\n",
    "    behaviors_df[['ImpressionNewsIDs', 'ImpressionLabels']] = behaviors_df['Impressions'].apply(\n",
    "        lambda x: pd.Series(parse_impressions(x))\n",
    "    )\n",
    "\n",
    "    # Initialize list for train samples\n",
    "    train_samples = []\n",
    "\n",
    "    # Iterate over behaviors to create train samples\n",
    "    for _, row in tqdm(behaviors_df.iterrows(), total=behaviors_df.shape[0]):\n",
    "        user_id = row['UserID']\n",
    "        user_cluster = row['Cluster'] if 'Cluster' in row else None  # Cluster will be assigned later\n",
    "\n",
    "        # Parse user history\n",
    "        history_ids = row['HistoryText'].split() if pd.notna(row['HistoryText']) else []\n",
    "        history_texts = [news_text_dict.get(nid, [0]*max_title_length) for nid in history_ids]\n",
    "\n",
    "        # Limit history length\n",
    "        if len(history_texts) < max_history_length:\n",
    "            padding = [[0]*max_title_length] * (max_history_length - len(history_texts))\n",
    "            history_texts = padding + history_texts\n",
    "        else:\n",
    "            history_texts = history_texts[-max_history_length:]\n",
    "\n",
    "        candidate_news_ids = row['ImpressionNewsIDs']\n",
    "        labels = row['ImpressionLabels']\n",
    "\n",
    "        for candidate_news_id, label in zip(candidate_news_ids, labels):\n",
    "            candidate_text = news_text_dict.get(candidate_news_id, [0]*max_title_length)\n",
    "            train_samples.append({\n",
    "                'UserID': user_id,\n",
    "                'HistoryTitles': history_texts,  # Renamed to 'HistoryTitles'\n",
    "                'CandidateTitleTokens': candidate_text,  # Renamed to match DataGenerator\n",
    "                'Label': label\n",
    "            })\n",
    "\n",
    "    # Create DataFrame from samples\n",
    "    train_df = pd.DataFrame(train_samples)\n",
    "    print(f\"Created train_df with {len(train_df)} samples.\")\n",
    "    print(\"Columns in train_df:\")\n",
    "    print(train_df.columns)\n",
    "    # --- [Clustering Users] ---\n",
    "    # Ensure 'UserID's match between user_category_profiles and behaviors_df\n",
    "    unique_user_ids = behaviors_df['UserID'].unique()\n",
    "    user_category_profiles = user_category_profiles.loc[unique_user_ids]\n",
    "\n",
    "    # Check for any missing 'UserID's\n",
    "    missing_user_ids = set(unique_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"Warning: {len(missing_user_ids)} 'UserID's are missing from user_category_profiles.\")\n",
    "        # Optionally handle missing users\n",
    "        # For this example, we'll remove these users from behaviors_df\n",
    "        behaviors_df = behaviors_df[~behaviors_df['UserID'].isin(missing_user_ids)]\n",
    "    else:\n",
    "        print(\"All 'UserID's are present in user_category_profiles.\")\n",
    "\n",
    "    # Perform clustering on user_category_profiles\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    user_clusters = kmeans.fit_predict(user_category_profiles)\n",
    "    print(f\"Assigned clusters to users. Number of clusters: {num_clusters}\")\n",
    "\n",
    "    # Create a DataFrame for user clusters\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'UserID': user_category_profiles.index,\n",
    "        'Cluster': user_clusters\n",
    "    })\n",
    "\n",
    "    # --- [Assign Clusters Using Map] ---\n",
    "    print(\"Assigning cluster labels to train_df using map...\")\n",
    "    user_cluster_mapping = dict(zip(user_cluster_df['UserID'], user_cluster_df['Cluster']))\n",
    "    train_df['Cluster'] = train_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "    # Verify cluster assignment\n",
    "    missing_clusters = train_df[train_df['Cluster'].isna()]\n",
    "    if not missing_clusters.empty:\n",
    "        print(f\"Warning: {len(missing_clusters)} samples have missing cluster assignments.\")\n",
    "        # Remove samples with missing cluster assignments\n",
    "        train_df = train_df.dropna(subset=['Cluster'])\n",
    "    else:\n",
    "        print(\"All samples have cluster assignments.\")\n",
    "\n",
    "    # Convert 'Cluster' column to integer type\n",
    "    train_df['Cluster'] = train_df['Cluster'].astype(int)\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "    # Assuming train_df has a 'Cluster' column indicating cluster assignments\n",
    "    # Find the minimum size among all clusters\n",
    "    min_cluster_size = train_df['Cluster'].value_counts().min()\n",
    "\n",
    "    # Initialize an empty list to hold balanced data\n",
    "    balanced_data = []\n",
    "\n",
    "    # Iterate over each cluster and sample data to balance\n",
    "    for cluster in train_df['Cluster'].unique():\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        balanced_cluster_data = cluster_data.sample(n=min_cluster_size, random_state=42)\n",
    "        balanced_data.append(balanced_cluster_data)\n",
    "\n",
    "    # Combine balanced data for all clusters\n",
    "    balanced_train_df = pd.concat(balanced_data)\n",
    "\n",
    "    # Update train_df with the balanced data\n",
    "    # --- [Label Balancing for 0/1 Classes] ---\n",
    "    # Count how many 0s and 1s we have\n",
    "    label_counts = balanced_train_df['Label'].value_counts()\n",
    "    min_label_count = label_counts.min()\n",
    "\n",
    "    balanced_labels = []\n",
    "    for label_value in balanced_train_df['Label'].unique():\n",
    "        label_data = balanced_train_df[balanced_train_df['Label'] == label_value]\n",
    "        # Downsample to the min_label_count to balance the label distribution\n",
    "        balanced_label_data = label_data.sample(n=min_label_count, random_state=42)\n",
    "        balanced_labels.append(balanced_label_data)\n",
    "\n",
    "    # Combine the label balanced data\n",
    "    final_balanced_train_df = pd.concat(balanced_labels, ignore_index=True)\n",
    "\n",
    "    # Shuffle the final dataset to mix up the rows\n",
    "    final_balanced_train_df = final_balanced_train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nAfter label balancing (0 vs 1):\")\n",
    "    print(final_balanced_train_df['Label'].value_counts())\n",
    "\n",
    "    # Now final_balanced_train_df is balanced both by cluster and by label\n",
    "    train_df = final_balanced_train_df\n",
    "    #train_df = balanced_train_df.reset_index(drop=True)\n",
    "\n",
    "    # Print summary of the balanced dataset\n",
    "    print(\"Balanced cluster sizes:\")\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        print(f\"Cluster {cluster}: {len(cluster_data)} samples\")\n",
    "    print(\"Balanced dataset:\")\n",
    "    print(train_df['Cluster'].value_counts())\n",
    "    \"\"\"\n",
    "    clustered_data_balanced = {}\n",
    "    min_cluster_size = float('inf')\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "        min_cluster_size = len(cluster_data) if len(cluster_data) < min_cluster_size else min_cluster_size\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        data = train_df[train_df['Cluster'] == cluster]\n",
    "        if len(data) > min_cluster_size:\n",
    "            clustered_data_balanced[cluster] = data.sample(n=min_cluster_size, random_state=42)\n",
    "        else:\n",
    "            clustered_data_balanced[cluster] = data\n",
    "\n",
    "    print(\"Balanced cluster sizes:\")\n",
    "    for cluster, data in clustered_data_balanced.items():\n",
    "        print(f\"Cluster {cluster}: {len(data)} samples\")\n",
    "    \"\"\"\n",
    "    # --- [Sampling] ---\n",
    "    # Optionally perform random sampling\n",
    "    print(f\"Original size: {len(train_df)}\")\n",
    "    train_df_sampled = train_df.sample(frac=fraction, random_state=42)\n",
    "    print(f\"Sampled size: {len(train_df_sampled)}\")\n",
    "\n",
    "    # Optionally, set train_df to sampled\n",
    "    train_df = train_df_sampled\n",
    "    print(\"Columns in sampled train_df:\")\n",
    "    print(train_df.columns)\n",
    "    print(f\"Cluster:{train_df['Cluster']}\")\n",
    "    # --- [Split Data for Each Cluster] ---\n",
    "    print(\"Splitting data into training and validation sets for each cluster...\")\n",
    "    clustered_data = {}\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "\n",
    "        if cluster_data.empty:\n",
    "            print(f\"No data for Cluster {cluster}. Skipping...\")\n",
    "            continue  # Skip to the next cluster\n",
    "\n",
    "        train_data, val_data = train_test_split(cluster_data, test_size=0.2, random_state=42)\n",
    "        clustered_data[cluster] = {\n",
    "            'train': train_data.reset_index(drop=True),\n",
    "            'val': val_data.reset_index(drop=True)\n",
    "        }\n",
    "        print(f\"Cluster {cluster}: {len(train_data)} training samples, {len(val_data)} validation samples.\")\n",
    "\n",
    "    return clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters\n",
    "\n",
    "# --- [DataGenerator Class] ---\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, max_history_length=50, max_title_length=30):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_history_length = max_history_length\n",
    "        self.max_title_length = max_title_length\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        #print(f\"[DataGenerator] Initialized with {len(self.df)} samples and batch_size={self.batch_size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        length = int(np.ceil(len(self.df) / self.batch_size))\n",
    "        #print(f\"[DataGenerator] Number of batches per epoch: {length}\")\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, len(self.df))\n",
    "        batch_indices = self.indices[start:end]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "\n",
    "        # Debugging: Print batch information\n",
    "        #print(f\"[DataGenerator] Generating batch {idx+1}/{self.__len__()} with samples {start} to {end}\")\n",
    "\n",
    "        if len(batch_df) == 0:\n",
    "            print(f\"[DataGenerator] Warning: Batch {idx} is empty.\")\n",
    "            return None, None\n",
    "\n",
    "        # Initialize batches\n",
    "        history_batch = []\n",
    "        candidate_batch = []\n",
    "        labels_batch = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            # Get tokenized history titles\n",
    "            history_titles = row['HistoryTitles']  # List of lists of integers\n",
    "\n",
    "            # Pad each title in history\n",
    "            history_titles_padded = pad_sequences(\n",
    "                history_titles,\n",
    "                maxlen=self.max_title_length,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0\n",
    "            )\n",
    "\n",
    "            # Pad or truncate the history to MAX_HISTORY_LENGTH\n",
    "            if len(history_titles_padded) < self.max_history_length:\n",
    "                padding = np.zeros((self.max_history_length - len(history_titles_padded), self.max_title_length), dtype='int32')\n",
    "                history_titles_padded = np.vstack([padding, history_titles_padded])\n",
    "            else:\n",
    "                history_titles_padded = history_titles_padded[-self.max_history_length:]\n",
    "\n",
    "            # Get candidate title tokens\n",
    "            candidate_title = row['CandidateTitleTokens']  # List of integers\n",
    "            candidate_title_padded = pad_sequences(\n",
    "                [candidate_title],\n",
    "                maxlen=self.max_title_length,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0\n",
    "            )[0]\n",
    "\n",
    "            # Append to batches\n",
    "            history_batch.append(history_titles_padded)\n",
    "            candidate_batch.append(candidate_title_padded)\n",
    "            labels_batch.append(row['Label'])\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        history_batch = np.array(history_batch, dtype='int32')  # Shape: (batch_size, MAX_HISTORY_LENGTH, MAX_TITLE_LENGTH)\n",
    "        candidate_batch = np.array(candidate_batch, dtype='int32')  # Shape: (batch_size, MAX_TITLE_LENGTH)\n",
    "        labels_batch = np.array(labels_batch, dtype='float32')  # Shape: (batch_size,)\n",
    "        inputs = {\n",
    "            'history_input': history_batch,\n",
    "            'candidate_input': candidate_batch\n",
    "        }\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        #print(f\"[DataGenerator] Batch shapes - history_input: {history_batch.shape}, candidate_input: {candidate_batch.shape}, labels: {labels_batch.shape}\")\n",
    "        return inputs, labels_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# --- [Fastformer Model Classes and Functions] ---\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Softmax, Multiply, Embedding, TimeDistributed, LayerNormalization\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class SqueezeLayer(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super(SqueezeLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SqueezeLayer, self).get_config()\n",
    "        config.update({'axis': self.axis})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ExpandDimsLayer(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super(ExpandDimsLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ExpandDimsLayer, self).get_config()\n",
    "        config.update({'axis': self.axis})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class SumPooling(Layer):\n",
    "    def __init__(self, axis=1, **kwargs):\n",
    "        super(SumPooling, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_sum(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SumPooling, self).get_config()\n",
    "        config.update({'axis': self.axis})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Fastformer(Layer):\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        super(Fastformer, self).__init__(**kwargs)\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head * size_per_head\n",
    "\n",
    "        self.WQ = None\n",
    "        self.WK = None\n",
    "        self.WV = None\n",
    "        self.WO = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = Dense(self.output_dim, use_bias=False, name='WQ')\n",
    "        self.WK = Dense(self.output_dim, use_bias=False, name='WK')\n",
    "        self.WV = Dense(self.output_dim, use_bias=False, name='WV')\n",
    "        self.WO = Dense(self.output_dim, use_bias=False, name='WO')\n",
    "        super(Fastformer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 2:\n",
    "            Q_seq, K_seq = inputs\n",
    "            Q_mask = None\n",
    "            K_mask = None\n",
    "        elif len(inputs) == 4:\n",
    "            Q_seq, K_seq, Q_mask, K_mask = inputs\n",
    "\n",
    "        batch_size = tf.shape(Q_seq)[0]\n",
    "        seq_len = tf.shape(Q_seq)[1]\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.WQ(Q_seq)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        K = self.WK(K_seq)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        V = self.WV(K_seq)  # Shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.nb_head, self.size_per_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.nb_head, self.size_per_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.nb_head, self.size_per_head))\n",
    "\n",
    "        # Compute global query and key\n",
    "        global_q = tf.reduce_mean(Q, axis=1, keepdims=True)  # (batch_size, 1, nb_head, size_per_head)\n",
    "        global_k = tf.reduce_mean(K, axis=1, keepdims=True)  # (batch_size, 1, nb_head, size_per_head)\n",
    "\n",
    "        # Compute attention weights\n",
    "        weights = global_q * K + global_k * Q  # (batch_size, seq_len, nb_head, size_per_head)\n",
    "        weights = tf.reduce_sum(weights, axis=-1)  # (batch_size, seq_len, nb_head)\n",
    "        weights = tf.nn.softmax(weights, axis=1)  # Softmax over seq_len\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        weights = tf.expand_dims(weights, axis=-1)  # (batch_size, seq_len, nb_head, 1)\n",
    "        context = weights * V  # (batch_size, seq_len, nb_head, size_per_head)\n",
    "\n",
    "        # Combine heads\n",
    "        context = tf.reshape(context, (batch_size, seq_len, self.output_dim))\n",
    "\n",
    "        # Final projection\n",
    "        output = self.WO(context)  # (batch_size, seq_len, output_dim)\n",
    "\n",
    "        return output  # Output shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Fastformer, self).get_config()\n",
    "        config.update({\n",
    "            'nb_head': self.nb_head,\n",
    "            'size_per_head': self.size_per_head\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class NewsEncoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, dropout_rate=0.2, nb_head=8, size_per_head=32, **kwargs):\n",
    "        super(NewsEncoder, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "\n",
    "        # Define sub-layers\n",
    "        self.embedding_layer = Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            name='embedding_layer'\n",
    "        )\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        self.dense = Dense(1)\n",
    "        self.softmax = Softmax(axis=1)\n",
    "        self.squeeze = SqueezeLayer(axis=-1)\n",
    "        self.expand_dims = ExpandDimsLayer(axis=-1)\n",
    "        self.sum_pooling = SumPooling(axis=1)\n",
    "\n",
    "        self.fastformer_layer = Fastformer(nb_head=self.nb_head, size_per_head=self.size_per_head, name='fastformer_layer')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(NewsEncoder, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Create mask\n",
    "        mask = tf.cast(tf.not_equal(inputs, 0), dtype='float32')  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Embedding\n",
    "        title_emb = self.embedding_layer(inputs)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        title_emb = self.dropout(title_emb)\n",
    "\n",
    "        # Fastformer\n",
    "        hidden_emb = self.fastformer_layer([title_emb, title_emb, mask, mask])  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        hidden_emb = self.dropout(hidden_emb)\n",
    "\n",
    "        # Attention-based Pooling\n",
    "        attention_scores = self.dense(hidden_emb)  # Shape: (batch_size, seq_len, 1)\n",
    "        attention_scores = self.squeeze(attention_scores)  # Shape: (batch_size, seq_len)\n",
    "        attention_weights = self.softmax(attention_scores)  # Shape: (batch_size, seq_len)\n",
    "        attention_weights = self.expand_dims(attention_weights)  # Shape: (batch_size, seq_len, 1)\n",
    "        multiplied = Multiply()([hidden_emb, attention_weights])  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        news_vector = self.sum_pooling(multiplied)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        return news_vector  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NewsEncoder, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'nb_head': self.nb_head,\n",
    "            'size_per_head': self.size_per_head\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class MaskLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaskLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Create mask: cast to float32 any position that is not equal to zero\n",
    "        mask = tf.cast(tf.not_equal(inputs, 0), dtype='float32')\n",
    "        return mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MaskLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class UserEncoder(Layer):\n",
    "    def __init__(self, news_encoder_layer, embedding_dim=256, **kwargs):\n",
    "        super(UserEncoder, self).__init__(**kwargs)\n",
    "        self.news_encoder_layer = news_encoder_layer\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.fastformer = Fastformer(nb_head=8, size_per_head=32, name='user_fastformer')\n",
    "        self.dense = Dense(1)\n",
    "        self.squeeze = SqueezeLayer(axis=-1)\n",
    "        self.softmax = Softmax(axis=1)\n",
    "        self.expand_dims = ExpandDimsLayer(axis=-1)\n",
    "        self.sum_pooling = SumPooling(axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, MAX_HISTORY_LENGTH, MAX_TITLE_LENGTH)\n",
    "        # Encode each news article in the history\n",
    "        news_vectors = TimeDistributed(self.news_encoder_layer)(inputs)  # Shape: (batch_size, MAX_HISTORY_LENGTH, embedding_dim)\n",
    "\n",
    "        # Step 1: Create a boolean mask\n",
    "        mask = tf.not_equal(inputs, 0)  # Shape: (batch_size, MAX_HISTORY_LENGTH, MAX_TITLE_LENGTH), dtype=bool\n",
    "\n",
    "        # Step 2: Reduce along the last axis\n",
    "        mask = tf.reduce_any(mask, axis=-1)  # Shape: (batch_size, MAX_HISTORY_LENGTH), dtype=bool\n",
    "\n",
    "        # Step 3: Cast to float32 if needed\n",
    "        mask = tf.cast(mask, dtype='float32')  # Shape: (batch_size, MAX_HISTORY_LENGTH), dtype=float32\n",
    "\n",
    "        # Fastformer\n",
    "        hidden_emb = self.fastformer([news_vectors, news_vectors, mask, mask])  # Shape: (batch_size, MAX_HISTORY_LENGTH, embedding_dim)\n",
    "        hidden_emb = self.dropout(hidden_emb)\n",
    "        hidden_emb = self.layer_norm(hidden_emb)\n",
    "\n",
    "        # Attention-based Pooling over history\n",
    "        attention_scores = self.dense(hidden_emb)  # Shape: (batch_size, MAX_HISTORY_LENGTH, 1)\n",
    "        attention_scores = self.squeeze(attention_scores)  # Shape: (batch_size, MAX_HISTORY_LENGTH)\n",
    "        attention_weights = self.softmax(attention_scores)  # Shape: (batch_size, MAX_HISTORY_LENGTH)\n",
    "        attention_weights = self.expand_dims(attention_weights)  # Shape: (batch_size, MAX_HISTORY_LENGTH, 1)\n",
    "        multiplied = Multiply()([hidden_emb, attention_weights])  # Shape: (batch_size, MAX_HISTORY_LENGTH, embedding_dim)\n",
    "        user_vector = self.sum_pooling(multiplied)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        return user_vector  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(UserEncoder, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# --- [Build Model Function] ---\n",
    "def build_model(vocab_size, max_title_length=30, max_history_length=50, embedding_dim=256, nb_head=8, size_per_head=32, dropout_rate=0.2):\n",
    "    # Define Inputs\n",
    "    history_input = Input(shape=(max_history_length, max_title_length), dtype='int32', name='history_input')\n",
    "    candidate_input = Input(shape=(max_title_length,), dtype='int32', name='candidate_input')\n",
    "\n",
    "    # Instantiate NewsEncoder Layer\n",
    "    news_encoder_layer = NewsEncoder(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        nb_head=nb_head,\n",
    "        size_per_head=size_per_head,\n",
    "        name='news_encoder'\n",
    "    )\n",
    "\n",
    "    # Encode Candidate News\n",
    "    candidate_vector = news_encoder_layer(candidate_input)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    # Encode User History\n",
    "    user_vector = UserEncoder(news_encoder_layer, embedding_dim=embedding_dim, name='user_encoder')(history_input)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    # Scoring Function: Dot Product between User and Candidate Vectors\n",
    "    score = Dot(axes=-1)([user_vector, candidate_vector])  # Shape: (batch_size, 1)\n",
    "    score = Activation('sigmoid')(score)  # Shape: (batch_size, 1)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs={'history_input': history_input, 'candidate_input': candidate_input}, outputs=score)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='AUC')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def build_and_load_weights(weights_file):\n",
    "    model = build_model(\n",
    "        vocab_size=vocab_size,\n",
    "        max_title_length=max_title_length,\n",
    "        max_history_length=max_history_length,\n",
    "        embedding_dim=256,\n",
    "        nb_head=8,\n",
    "        size_per_head=32,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "\n",
    "    # Manually build the model\n",
    "    input_shapes = {\n",
    "        'history_input': (None, max_history_length, max_title_length),\n",
    "        'candidate_input': (None, max_title_length)\n",
    "    }\n",
    "    # Prepare dummy inputs\n",
    "    import numpy as np\n",
    "\n",
    "    dummy_history_input = np.zeros((1, 50, 30), dtype=np.int32)\n",
    "    dummy_candidate_input = np.zeros((1, 30), dtype=np.int32)\n",
    "\n",
    "    # Build the model by passing dummy data\n",
    "    model.predict({'history_input': dummy_history_input, 'candidate_input': dummy_candidate_input})\n",
    "    #model.build(input_shapes)\n",
    "    model.load_weights(weights_file)\n",
    "    return model\n",
    "# --- [Training Function] ---\n",
    "def train_cluster_models(clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters, batch_size=64, epochs=5):\n",
    "    models = {}\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        m_name = f'fastformer_cluster_{cluster}_full_balanced_1_epoch'\n",
    "        weights_file = f'{m_name}.weights.h5'\n",
    "        model_file = f'{m_name}.h5'\n",
    "        if cluster in []:\n",
    "            print(f\"\\nLoading model for Cluster {cluster} from {weights_file}\")\n",
    "            model = build_and_load_weights(weights_file)\n",
    "            models[cluster] = model\n",
    "            continue\n",
    "        print(f\"\\nTraining model for Cluster {cluster} into {weights_file}\")\n",
    "        # Retrieve training and validation data\n",
    "        train_data = clustered_data[cluster]['train']\n",
    "        val_data = clustered_data[cluster]['val']\n",
    "\n",
    "        print(f\"Cluster {cluster} - Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "\n",
    "        # Create data generators\n",
    "        train_generator = DataGenerator(train_data, batch_size, max_history_length, max_title_length)\n",
    "        val_generator = DataGenerator(val_data, batch_size, max_history_length, max_title_length)\n",
    "\n",
    "        steps_per_epoch = len(train_generator)\n",
    "        validation_steps = len(val_generator)\n",
    "\n",
    "        # Build model\n",
    "        model = build_model(\n",
    "            vocab_size=vocab_size,\n",
    "            max_title_length=max_title_length,\n",
    "            max_history_length=max_history_length,\n",
    "            embedding_dim=256,\n",
    "            nb_head=8,\n",
    "            size_per_head=32,\n",
    "            dropout_rate=0.2\n",
    "        )\n",
    "        print(model.summary())\n",
    "\n",
    "        # Define callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_AUC',\n",
    "            patience=2,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        csv_logger = CSVLogger(f'training_log_cluster_{cluster}.csv', append=True)\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            f'best_model_cluster_{cluster}.keras',\n",
    "            monitor='val_AUC',\n",
    "            mode='max',\n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            train_generator,\n",
    "            epochs=epochs,\n",
    "            #steps_per_epoch=steps_per_epoch,\n",
    "            #validation_data=val_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[early_stopping, csv_logger, model_checkpoint]\n",
    "        )\n",
    "\n",
    "        # Save model weights\n",
    "        model.save_weights(weights_file)\n",
    "        print(f\"Saved model weights for Cluster {cluster} into {weights_file}.\")\n",
    "        model.save(model_file)\n",
    "        print(f\"Saved model for Cluster {cluster} into {model_file}.\")\n",
    "\n",
    "        # Store the model\n",
    "        models[cluster] = model\n",
    "        # Clear memory\n",
    "        del train_data, val_data, train_generator, val_generator, model\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    print(\"Returning models list\")\n",
    "    print(models)\n",
    "    return models\n",
    "\n",
    "# --- [Recommendation Function] ---\n",
    "def recommend_news(user_id, user_cluster_df, models, candidate_texts, history_texts, max_history_length=50, max_title_length=30):\n",
    "    # Determine the user's cluster\n",
    "    cluster = user_cluster_df.get(user_id)\n",
    "    if cluster is None:\n",
    "        print(f\"User {user_id} not found in any cluster.\")\n",
    "        return None\n",
    "\n",
    "    # Retrieve the corresponding model\n",
    "    model = models.get(cluster)\n",
    "    if model is None:\n",
    "        print(f\"No model trained for Cluster {cluster}.\")\n",
    "        return None\n",
    "\n",
    "    # Prepare input data for the user\n",
    "    history_padded = pad_sequences(\n",
    "        [history_texts],\n",
    "        maxlen=max_history_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (1, max_history_length, max_title_length)\n",
    "\n",
    "    candidate_padded = pad_sequences(\n",
    "        [candidate_texts],\n",
    "        maxlen=max_title_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (1, max_title_length)\n",
    "\n",
    "    inputs = {\n",
    "        'history_input': history_padded,\n",
    "        'candidate_input': candidate_padded\n",
    "    }\n",
    "\n",
    "    # Generate prediction\n",
    "    prediction = model.predict(inputs)[0][0]  # Get the prediction score\n",
    "\n",
    "    # Return the prediction score\n",
    "    return prediction\n",
    "# --- [Main Execution] ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to data files\n",
    "    news_file = 'news.tsv'\n",
    "    behaviors_file = 'behaviors.tsv'\n",
    "\n",
    "    # Load behaviors data to get unique UserIDs\n",
    "    behaviors_path = os.path.join(data_dir, behaviors_file)\n",
    "    behaviors_df = pd.read_csv(\n",
    "        behaviors_path,\n",
    "        sep='\\t',\n",
    "        names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    # Extract unique UserIDs\n",
    "    unique_user_ids = behaviors_df['UserID'].unique()\n",
    "    print(f\"Number of unique users in behaviors_df: {len(unique_user_ids)}\")\n",
    "\n",
    "    # Create dummy user_category_profiles with matching UserIDs\n",
    "    #user_category_profiles = pd.DataFrame(\n",
    "    #    np.random.rand(len(unique_user_ids), 10),  # One row per user\n",
    "    #    index=unique_user_ids,\n",
    "    #    columns=[f'feature_{j}' for j in range(10)]\n",
    "    #)\n",
    "\n",
    "    # Prepare clustered data\n",
    "    clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters = prepare_train_df(\n",
    "        data_dir=data_dir,\n",
    "        news_file=news_file,\n",
    "        behaviours_file=behaviors_file,\n",
    "        user_category_profiles=user_category_profiles,\n",
    "        num_clusters=3,\n",
    "        fraction=1,\n",
    "        max_title_length=30,\n",
    "        max_history_length=50\n",
    "    )\n",
    "\n",
    "    # Train cluster-specific models\n",
    "    models = train_cluster_models(\n",
    "        clustered_data=clustered_data,\n",
    "        tokenizer=tokenizer,\n",
    "        vocab_size=vocab_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length,\n",
    "        num_clusters=num_clusters,\n",
    "        batch_size=64,\n",
    "        epochs=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d9e880-2c16-4dea-a3d6-62fa4dc02940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "user_profiles_scaled = scaler.fit_transform(filtered_user_category_profiles)\n",
    "sum_of_squared_distances = []\n",
    "K = range(1, 10)  # Try more clusters\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(user_profiles_scaled)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101119d7-77b1-4eab-9903-8118ebf2de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news data:\n",
      "   NewsID   Category      SubCategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N18955     health          medical   \n",
      "2  N61837       news        newsworld   \n",
      "3  N53526     health           voices   \n",
      "4  N38324     health          medical   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1  Dispose of unwanted prescription drugs during ...   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
      "4  How to Get Rid of Skin Tags, According to a De...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1                                                NaN   \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "3  I felt like I was a fraud, and being an NBA wi...   \n",
      "4  They seem harmless, but there's a very good re...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
      "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1                                                 []  \n",
      "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "3  [{\"Label\": \"National Basketball Association\", ...  \n",
      "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  \n",
      "\n",
      "Loaded validation behaviors data:\n",
      "   ImpressionID  UserID                    Time  \\\n",
      "0             1  U80234  11/15/2019 12:37:50 PM   \n",
      "1             2  U60458   11/15/2019 7:11:50 AM   \n",
      "2             3  U44190   11/15/2019 9:55:12 AM   \n",
      "3             4  U87380   11/15/2019 3:12:46 PM   \n",
      "4             5   U9444   11/15/2019 8:25:46 AM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N55189 N46039 N51741 N53234 N11276 N264 N40716...   \n",
      "1  N58715 N32109 N51180 N33438 N54827 N28488 N611...   \n",
      "2  N56253 N1150 N55189 N16233 N61704 N51706 N5303...   \n",
      "3  N63554 N49153 N28678 N23232 N43369 N58518 N444...   \n",
      "4                 N51692 N18285 N26015 N22679 N55556   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5...  \n",
      "1  N20036-0 N23513-1 N32536-0 N46976-0 N35216-0 N...  \n",
      "2  N36779-0 N62365-0 N58098-0 N5472-0 N13408-0 N5...  \n",
      "3  N6950-0 N60215-0 N6074-0 N11930-0 N6916-0 N248...  \n",
      "4  N5940-1 N23513-0 N49285-0 N23355-0 N19990-0 N3...  \n",
      "Number of entries with missing 'HistoryText': 2214\n",
      "73152\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Define data directory and file paths\n",
    "data_dir = 'small/valid/'  # Adjust if your files are in a different directory\n",
    "news_file = 'news.tsv'\n",
    "valid_behaviors_file = 'behaviors.tsv'  # Validation behaviors file\n",
    "\n",
    "# Load news data (if not already loaded)\n",
    "news_path = os.path.join(data_dir, news_file)\n",
    "news_df = pd.read_csv(\n",
    "    news_path,\n",
    "    sep='\\t',\n",
    "    names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"Loaded news data:\")\n",
    "print(news_df.head())\n",
    "\n",
    "# Load validation behaviors data\n",
    "valid_behaviors_path = os.path.join(data_dir, valid_behaviors_file)\n",
    "valid_behaviors_df = pd.read_csv(\n",
    "    valid_behaviors_path,\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded validation behaviors data:\")\n",
    "print(valid_behaviors_df.head())\n",
    "# Check for missing 'HistoryText' entries\n",
    "missing_history = valid_behaviors_df['HistoryText'].isna().sum()\n",
    "print(f\"Number of entries with missing 'HistoryText': {missing_history}\")\n",
    "print(len(valid_behaviors_df['HistoryText']))\n",
    "valid_behaviors_df['HistoryText'] = valid_behaviors_df['HistoryText'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc610f7-bafb-45ba-ba33-81088dcfceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scaler to user_profiles_scaler.pkl\n",
      "Saved KMeans clustering model to kmeans_user_clusters.pkl\n",
      "Saved user cluster assignments to user_cluster_df.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "# Load the user_category_profiles\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "\n",
    "# --- [Perform Clustering] ---\n",
    "\n",
    "# Optionally, standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the user profiles\n",
    "user_profiles_scaled = scaler.fit_transform(user_category_profiles)\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved scaler to {scaler_path}\")\n",
    "\n",
    "# Initialize the clustering model\n",
    "num_clusters = 3  # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit the clustering model\n",
    "kmeans.fit(user_profiles_scaled)\n",
    "\n",
    "# Save the clustering model for future use\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "with open(clustering_model_path, 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(f\"Saved KMeans clustering model to {clustering_model_path}\")\n",
    "\n",
    "# Assign clusters to users\n",
    "user_clusters = kmeans.predict(user_profiles_scaled)\n",
    "\n",
    "# Add the cluster assignments to the user profiles\n",
    "user_category_profiles['Cluster'] = user_clusters\n",
    "\n",
    "# Save the cluster assignments\n",
    "user_cluster_df = user_category_profiles[['Cluster']]\n",
    "user_cluster_df_path = 'user_cluster_df.pkl'\n",
    "user_cluster_df.to_pickle(user_cluster_df_path)\n",
    "print(f\"Saved user cluster assignments to {user_cluster_df_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8b871-0d46-4cbf-8d82-8fa7c8abbde1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded existing tokenizer.\n",
      "\n",
      "Sample tokenized titles:\n",
      "   NewsID                                              Title  \\\n",
      "0  N55528  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1  N18955  Dispose of unwanted prescription drugs during ...   \n",
      "2  N61837  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "3  N53526  I Was An NBA Wife. Here's How It Affected My M...   \n",
      "4  N38324  How to Get Rid of Skin Tags, According to a De...   \n",
      "\n",
      "                                         TitleTokens  \n",
      "0  [3342, 2487, 1105, 1721, 1648, 1721, 6877, 14535]  \n",
      "1              [13222, 9234, 4983, 2648, 79, 26, 29]  \n",
      "2                     [1006, 1889, 3014, 14536, 701]  \n",
      "3                        [518, 840, 2469, 1779, 164]  \n",
      "4                 [20, 5752, 3762, 11289, 27, 23410]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing behaviors: 100%|█████████████████████████████████████████████████████| 73152/73152 [00:35<00:00, 2068.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created test_df with samples:\n",
      "   UserID                                      HistoryTitles  \\\n",
      "0  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "1  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "2  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "3  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "4  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "\n",
      "                                CandidateTitleTokens  Label  \n",
      "0           [246, 9230, 2452, 3978, 198, 3566, 3179]      0  \n",
      "1  [4195, 2250, 342, 21, 671, 976, 1035, 780, 18,...      0  \n",
      "2                  [2114, 2380, 2971, 20, 5062, 583]      1  \n",
      "3      [10700, 318, 4152, 81, 210, 309, 22938, 5574]      0  \n",
      "4                                    [653, 723, 147]      0  \n",
      "\n",
      "Total test samples: 2740998\n",
      "\n",
      "Sample HistoryTokens:\n",
      "   UserID                                      HistoryTitles  \\\n",
      "0  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "1  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "2  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "3  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "4  U80234  [N55189, N46039, N51741, N53234, N11276, N264,...   \n",
      "\n",
      "                                       HistoryTokens  \n",
      "0  [[3682, 4877, 4438, 12418, 8316], [403, 525, 6...  \n",
      "1  [[3682, 4877, 4438, 12418, 8316], [403, 525, 6...  \n",
      "2  [[3682, 4877, 4438, 12418, 8316], [403, 525, 6...  \n",
      "3  [[3682, 4877, 4438, 12418, 8316], [403, 525, 6...  \n",
      "4  [[3682, 4877, 4438, 12418, 8316], [403, 525, 6...  \n",
      "\n",
      "Loaded user_cluster_df.\n",
      "         Cluster\n",
      "UserID          \n",
      "U87243         2\n",
      "U598644        2\n",
      "U532401        2\n",
      "U593596        2\n",
      "U239687        1\n",
      "Index(['Cluster'], dtype='object')\n",
      "'UserID' is not a column. It might be the index.\n",
      "\n",
      "Assigned clusters to test_df:\n",
      "Cluster\n",
      "2    2642398\n",
      "0      93051\n",
      "1       5549\n",
      "Name: count, dtype: int64\n",
      "Cluster 0: 93051 test samples.\n",
      "Cluster 1: 5549 test samples.\n",
      "Cluster 2: 2642398 test samples.\n",
      "Created test_generator for Cluster 0.\n",
      "Created test_generator for Cluster 1.\n",
      "Created test_generator for Cluster 2.\n",
      "\n",
      "Evaluating Model for Cluster 0\n",
      "   1452/Unknown \u001b[1m22s\u001b[0m 7ms/step - AUC: 0.5953 - loss: 0.4521 - precision: 0.0666 - recall: 0.0511"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:41:54.963923: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-12-07 17:41:55.106324: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-12-07 17:41:55.124907: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-12-07 17:41:55.242299: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 196 bytes spill stores, 168 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1454/1454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 14ms/step - AUC: 0.5954 - loss: 0.4521 - precision: 0.0666 - recall: 0.0511\n",
      "Test Metrics for Cluster 0:\n",
      "  loss: 0.4501\n",
      "  compile_metrics: 0.0658\n",
      "\n",
      "Evaluating Model for Cluster 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:42:03.208322: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-12-07 17:42:03.208366: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2024-12-07 17:42:03.208684: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 2370515797060828571\n",
      "2024-12-07 17:42:03.208714: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16841741726712019017\n",
      "2024-12-07 17:42:03.208734: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14315849865906561681\n",
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     86/Unknown \u001b[1m10s\u001b[0m 10ms/step - AUC: 0.5739 - loss: 0.6367 - precision: 0.0515 - recall: 0.1297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:42:14.356077: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-12-07 17:42:14.506937: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-12-07 17:42:14.516369: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2024-12-07 17:42:14.628957: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_462', 196 bytes spill stores, 168 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 114ms/step - AUC: 0.5738 - loss: 0.6367 - precision: 0.0515 - recall: 0.1300\n",
      "Test Metrics for Cluster 1:\n",
      "  loss: 0.6376\n",
      "  compile_metrics: 0.0504\n",
      "\n",
      "Evaluating Model for Cluster 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:42:22.114351: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34546/Unknown \u001b[1m962s\u001b[0m 28ms/step - AUC: 0.5925 - loss: 0.5957 - precision: 0.0688 - recall: 0.1910"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- [DataGenerator Class] ---\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, max_history_length, max_title_length):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_history_length = max_history_length\n",
    "        self.max_title_length = max_title_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if batch.empty:\n",
    "            raise ValueError(f\"Batch index {idx} is empty.\")\n",
    "\n",
    "        # Extract data\n",
    "        history_tokens = batch['HistoryTokens'].tolist()  # List of lists of lists\n",
    "        candidate_titles = batch['CandidateTitleTokens'].tolist()\n",
    "        labels = batch['Label'].astype(np.float32).values  # Ensure labels are float32\n",
    "\n",
    "        # Pad candidate titles\n",
    "        candidate_titles_padded = pad_sequences(\n",
    "            candidate_titles,\n",
    "            maxlen=self.max_title_length,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "        # Initialize list to store padded histories\n",
    "        history_padded_batch = []\n",
    "\n",
    "        for user_history in history_tokens:\n",
    "            # user_history is a list of lists (titles)\n",
    "            if not user_history:\n",
    "                # If user has no history, create a zero matrix\n",
    "                padded_user_history = np.zeros((self.max_history_length, self.max_title_length), dtype=np.int32)\n",
    "            else:\n",
    "                # Pad each title in the history\n",
    "                padded_titles = pad_sequences(\n",
    "                    user_history,\n",
    "                    maxlen=self.max_title_length,\n",
    "                    padding='post',\n",
    "                    truncating='post',\n",
    "                    value=0\n",
    "                )\n",
    "\n",
    "                # Pad/truncate the history to max_history_length\n",
    "                if len(padded_titles) < self.max_history_length:\n",
    "                    pad_length = self.max_history_length - len(padded_titles)\n",
    "                    # Create padding titles\n",
    "                    pad_titles = np.zeros((pad_length, self.max_title_length), dtype=np.int32)\n",
    "                    # Concatenate padded titles with padding titles\n",
    "                    padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "                else:\n",
    "                    # Truncate to max_history_length\n",
    "                    padded_user_history = padded_titles[:self.max_history_length]\n",
    "\n",
    "            history_padded_batch.append(padded_user_history)\n",
    "\n",
    "        # Convert list to numpy array\n",
    "        history_padded_batch = np.array(history_padded_batch, dtype=np.int32)\n",
    "\n",
    "        # Create a dictionary for inputs\n",
    "        inputs = {\n",
    "            'history_input': history_padded_batch,\n",
    "            'candidate_input': candidate_titles_padded\n",
    "        }\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "# --- [Loading and Preparing Data] ---\n",
    "\n",
    "# Load or initialize your tokenizer\n",
    "tokenizer_path = 'tokenizer.pkl'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"\\nLoaded existing tokenizer.\")\n",
    "else:\n",
    "    # If not, initialize and fit a new tokenizer (not recommended if training was already done)\n",
    "    tokenizer = Tokenizer(num_words=63346, oov_token='<OOV>')  # Adjust num_words as per your setup\n",
    "    # Assuming 'news_df' is already loaded\n",
    "    tokenizer.fit_on_texts(news_df['Title'])\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\"\\nInitialized and fitted a new tokenizer.\")\n",
    "\n",
    "# Convert titles to sequences\n",
    "news_df['TitleTokens'] = tokenizer.texts_to_sequences(news_df['Title'])\n",
    "\n",
    "print(\"\\nSample tokenized titles:\")\n",
    "print(news_df[['NewsID', 'Title', 'TitleTokens']].head())\n",
    "\n",
    "# Create a mapping from NewsID to TitleTokens\n",
    "newsid_to_tokens = dict(zip(news_df['NewsID'], news_df['TitleTokens']))\n",
    "\n",
    "# Function to parse impressions and create labeled data\n",
    "def parse_impressions(row):\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    history_titles = row['HistoryText'].split(' ') if isinstance(row['HistoryText'], str) else []\n",
    "    data = []\n",
    "    for impression in impressions:\n",
    "        if '-' not in impression:\n",
    "            continue  # Skip malformed impressions\n",
    "        news_id, label = impression.split('-')\n",
    "        title_tokens = newsid_to_tokens.get(news_id, [])\n",
    "        data.append({\n",
    "            'UserID': row['UserID'],\n",
    "            'HistoryTitles': history_titles,  # List of NewsIDs\n",
    "            'CandidateTitleTokens': title_tokens,\n",
    "            'Label': int(label)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Apply the function to each row in the validation behaviors with a progress bar\n",
    "test_data = []\n",
    "for idx, row in tqdm(valid_behaviors_df.iterrows(), total=valid_behaviors_df.shape[0], desc=\"Processing behaviors\"):\n",
    "    parsed = parse_impressions(row)\n",
    "    test_data.extend(parsed)\n",
    "\n",
    "# Create a DataFrame from the test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"\\nCreated test_df with samples:\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nTotal test samples: {len(test_df)}\")\n",
    "\n",
    "# Function to convert NewsIDs in history to token sequences\n",
    "def convert_history(news_ids):\n",
    "    return [newsid_to_tokens.get(news_id, []) for news_id in news_ids]\n",
    "\n",
    "# Apply the function to convert history\n",
    "test_df['HistoryTokens'] = test_df['HistoryTitles'].apply(convert_history)\n",
    "\n",
    "print(\"\\nSample HistoryTokens:\")\n",
    "print(test_df[['UserID', 'HistoryTitles', 'HistoryTokens']].head())\n",
    "\n",
    "# --- [Loading Clustering Artifacts] ---\n",
    "\n",
    "# Paths to clustering artifacts\n",
    "user_cluster_path = 'user_cluster_df.pkl'\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "\n",
    "# Check if user_cluster_df.pkl exists\n",
    "if os.path.exists(user_cluster_path):\n",
    "    user_cluster_df = pd.read_pickle(user_cluster_path)\n",
    "    print(\"\\nLoaded user_cluster_df.\")\n",
    "else:\n",
    "    print(\"\\nuser_cluster_df.pkl not found. Recreating cluster assignments.\")\n",
    "    # Load the clustering model\n",
    "    if os.path.exists(clustering_model_path):\n",
    "        with open(clustering_model_path, 'rb') as f:\n",
    "            clustering_model = pickle.load(f)\n",
    "        print(\"\\nLoaded clustering model.\")\n",
    "    else:\n",
    "        print(\"Clustering model not found. Please ensure you have saved the clustering model during training.\")\n",
    "        raise FileNotFoundError(\"Clustering model not found.\")\n",
    "\n",
    "    # Load user_category_profiles\n",
    "    if os.path.exists(user_category_profiles_path):\n",
    "        user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "        print(\"\\nLoaded user_category_profiles.\")\n",
    "    else:\n",
    "        print(\"user_category_profiles.pkl not found. Cannot proceed without user profiles.\")\n",
    "        raise FileNotFoundError(\"User profiles not found.\")\n",
    "\n",
    "    # Load scaler\n",
    "    if os.path.exists(scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"\\nLoaded scaler for user profiles.\")\n",
    "    else:\n",
    "        print(\"Scaler not found. Please ensure you have saved the scaler during training.\")\n",
    "        raise FileNotFoundError(\"Scaler not found.\")\n",
    "\n",
    "    # Ensure test users are included in user_category_profiles\n",
    "    test_user_ids = test_df['UserID'].unique()\n",
    "    missing_user_ids = set(test_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"\\nAdding {len(missing_user_ids)} new users to user_category_profiles.\")\n",
    "        new_user_profiles = pd.DataFrame(0, index=missing_user_ids, columns=user_category_profiles.columns)\n",
    "        user_category_profiles = pd.concat([user_category_profiles, new_user_profiles])\n",
    "        print(\"Added new user profiles for test users.\")\n",
    "\n",
    "    # Scale user profiles\n",
    "    user_profiles_for_clustering = user_category_profiles.loc[test_user_ids]\n",
    "    user_profiles_scaled = scaler.transform(user_profiles_for_clustering)\n",
    "\n",
    "    # Assign clusters\n",
    "    test_user_clusters = clustering_model.predict(user_profiles_scaled)\n",
    "\n",
    "    # Create user_cluster_df with 'UserID' as index\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'Cluster': test_user_clusters\n",
    "    }, index=user_profiles_for_clustering.index)\n",
    "\n",
    "    # Save the cluster assignments\n",
    "    user_cluster_df.to_pickle('user_cluster_df.pkl')\n",
    "    print(\"\\nAssigned clusters to test users and saved to 'user_cluster_df.pkl'.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(user_cluster_df.head())\n",
    "\n",
    "# Check columns\n",
    "print(user_cluster_df.columns)\n",
    "\n",
    "# Check if 'UserID' is a column\n",
    "if 'UserID' in user_cluster_df.columns:\n",
    "    print(\"'UserID' is a column.\")\n",
    "else:\n",
    "    print(\"'UserID' is not a column. It might be the index.\")\n",
    "\n",
    "# Map clusters to test_df\n",
    "user_cluster_mapping = user_cluster_df['Cluster']  # Series with UserID as index\n",
    "\n",
    "test_df['Cluster'] = test_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "# Handle users not found in user_cluster_df\n",
    "missing_clusters = test_df['Cluster'].isna().sum()\n",
    "if missing_clusters > 0:\n",
    "    print(f\"\\nNumber of users without cluster assignment: {missing_clusters}\")\n",
    "    test_df['Cluster'] = test_df['Cluster'].fillna(0)  # Assign to Cluster 0 or any default cluster\n",
    "    print(\"Assigned default cluster 0 to users without cluster assignment.\")\n",
    "\n",
    "print(\"\\nAssigned clusters to test_df:\")\n",
    "print(test_df['Cluster'].value_counts())\n",
    "\n",
    "# --- [Creating Test Data per Cluster] ---\n",
    "\n",
    "# Define the number of clusters (ensure it matches your training)\n",
    "num_clusters = 3  # Adjust as needed\n",
    "\n",
    "# Initialize a dictionary to hold test data per cluster\n",
    "cluster_test_data = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = test_df[test_df['Cluster'] == cluster].reset_index(drop=True)\n",
    "    cluster_test_data[cluster] = cluster_data\n",
    "    print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "\n",
    "# Define maximum lengths (should match training)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Define a function to create TensorFlow Dataset with output_signature\n",
    "def add_output_sig(datagen, max_history_length, max_title_length):\n",
    "    output_signature = (\n",
    "        {\n",
    "            'history_input': tf.TensorSpec(shape=(None, max_history_length, max_title_length), dtype=tf.int32),\n",
    "            'candidate_input': tf.TensorSpec(shape=(None, max_title_length), dtype=tf.int32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    # Define a generator function\n",
    "    def generator():\n",
    "        for i in range(len(datagen)):\n",
    "            inputs, labels = datagen[i]\n",
    "            \n",
    "            # Assert non-empty inputs\n",
    "            assert inputs['history_input'].shape[0] > 0, f\"Batch {i} has empty history_input.\"\n",
    "            assert inputs['candidate_input'].shape[0] > 0, f\"Batch {i} has empty candidate_input.\"\n",
    "            assert labels.shape[0] > 0, f\"Batch {i} has empty labels.\"\n",
    "            \n",
    "            yield inputs, labels\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # Optional optimizations\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Initialize test DataGenerators per cluster\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    datagen = DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    )\n",
    "    test_generators[cluster] = add_output_sig(datagen, max_history_length, max_title_length)\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "\n",
    "# --- [Evaluating Models] ---\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster}\")\n",
    "    \n",
    "    model = models.get(cluster)  # Assuming 'models' is a dict with cluster as key\n",
    "    if model is None:\n",
    "        print(f\"No model found for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        results = model.evaluate(\n",
    "            test_generator,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model for Cluster {cluster}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Metrics for Cluster {cluster}:\")\n",
    "    for metric_name, value in zip(model.metrics_names, results):\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# --- [Generating Classification Reports] ---\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nGenerating Classification Report for Cluster {cluster}\")\n",
    "    \n",
    "    model = models.get(cluster)\n",
    "    if model is None:\n",
    "        print(f\"No model found for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Generate predictions\n",
    "        predictions = model.predict(test_generator, verbose=1)\n",
    "        predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Retrieve true labels\n",
    "        true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(true_labels, predicted_labels, digits=4)\n",
    "        print(f\"Classification Report for Cluster {cluster}:\\n{report}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for Cluster {cluster}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for Cluster {cluster}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f78a452-c58b-4ea2-ae47-acf531ac3163",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m     38\u001b[0m test_generators \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_clusters\u001b[49m):\n\u001b[1;32m     40\u001b[0m     cluster_data \u001b[38;5;241m=\u001b[39m cluster_test_data[cluster]\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cluster_data\u001b[38;5;241m.\u001b[39mempty:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "model_weights = {0: 1.0, 1: 1.0, 2: 1.0}\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cluster_validation_metrics = {}\n",
    "\n",
    "def add_output_sig(datagen, max_history_length, max_title_length):\n",
    "    output_signature = (\n",
    "        {\n",
    "            'history_input': tf.TensorSpec(shape=(None, max_history_length, max_title_length), dtype=tf.int32),\n",
    "            'candidate_input': tf.TensorSpec(shape=(None, max_title_length), dtype=tf.int32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    # Define a generator function\n",
    "    def generator():\n",
    "        for i in range(len(datagen)):\n",
    "            inputs, labels = datagen[i]\n",
    "            \n",
    "            # Assert non-empty inputs\n",
    "            assert inputs['history_input'].shape[0] > 0, f\"Batch {i} has empty history_input.\"\n",
    "            assert inputs['candidate_input'].shape[0] > 0, f\"Batch {i} has empty candidate_input.\"\n",
    "            assert labels.shape[0] > 0, f\"Batch {i} has empty labels.\"\n",
    "            \n",
    "            yield inputs, labels\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # Optional optimizations\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    datagen = DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    )\n",
    "    test_generators[cluster] = add_output_sig(datagen, max_history_length, max_title_length)\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster} on Validation Data\")\n",
    "    \n",
    "    model = models.get(cluster)\n",
    "    if model is None:\n",
    "        print(f\"No model found for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    # SHOULD THERE BE VALIDATION DATASET?????????\n",
    "    test_generator = test_generators.get(cluster)  # Ensure you have validation_generators similar to test_generators\n",
    "    if test_generator is None:\n",
    "        print(f\"No validation data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "    # Compute AUC score\n",
    "    auc = roc_auc_score(true_labels, predictions)\n",
    "    cluster_validation_metrics[cluster] = auc\n",
    "    print(f\"Cluster {cluster} Validation AUC: {auc:.4f}\")\n",
    "# Extract AUC scores\n",
    "auc_scores = np.array(list(cluster_validation_metrics.values()))\n",
    "clusters = list(cluster_validation_metrics.keys())\n",
    "\n",
    "# Normalize AUC scores to sum to 1\n",
    "normalized_weights = auc_scores / np.sum(auc_scores)\n",
    "\n",
    "# Map clusters to normalized weights\n",
    "model_weights = dict(zip(clusters, normalized_weights))\n",
    "\n",
    "print(\"\\nModel Weights based on Validation AUC:\")\n",
    "for cluster, weight in model_weights.items():\n",
    "    print(f\"Cluster {cluster}: Weight = {weight:.4f}\")\n",
    "def recommend_news_weighted_averaging(models, model_weights, candidate_texts, history_texts, max_history_length=50, max_title_length=30):\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    # Prepare candidate input\n",
    "    candidate_padded = pad_sequences(\n",
    "        [candidate_texts],\n",
    "        maxlen=max_title_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (1, max_title_length)\n",
    "\n",
    "    # Prepare history input\n",
    "    if len(history_texts) == 0:\n",
    "        # If user has no history, create a zero matrix\n",
    "        padded_user_history = np.zeros((max_history_length, max_title_length), dtype=np.int32)\n",
    "    else:\n",
    "        # Pad each title in the history to max_title_length\n",
    "        padded_titles = pad_sequences(\n",
    "            history_texts,  # Correct usage\n",
    "            maxlen=max_title_length,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0\n",
    "        )  # Shape: (num_titles_in_history, max_title_length)\n",
    "\n",
    "        # Pad or truncate the history to max_history_length\n",
    "        if len(padded_titles) < max_history_length:\n",
    "            pad_length = max_history_length - len(padded_titles)\n",
    "            # Create padding titles\n",
    "            pad_titles = np.zeros((pad_length, max_title_length), dtype=np.int32)\n",
    "            # Concatenate padded titles with padding titles\n",
    "            padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "        else:\n",
    "            # Truncate to max_history_length\n",
    "            padded_user_history = padded_titles[:max_history_length]\n",
    "\n",
    "    # Add batch dimension\n",
    "    history_padded = np.expand_dims(padded_user_history, axis=0)  # Shape: (1, max_history_length, max_title_length)\n",
    "\n",
    "    inputs = {\n",
    "        'history_input': history_padded,\n",
    "        'candidate_input': candidate_padded\n",
    "    }\n",
    "\n",
    "    # Get weighted predictions\n",
    "    weighted_sum = 0\n",
    "    total_weight = sum(model_weights.values())\n",
    "\n",
    "    for cluster, model in models.items():\n",
    "        prediction = model.predict(inputs)[0][0]\n",
    "        weight = model_weights.get(cluster, 1)\n",
    "        weighted_sum += prediction * weight\n",
    "\n",
    "    final_prediction = weighted_sum / total_weight\n",
    "\n",
    "    return final_prediction\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set your desired batch size\n",
    "batch_size = 1024  # Adjust as needed\n",
    "\n",
    "# Number of samples and batches\n",
    "num_samples = len(test_df)\n",
    "num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "ensemble_true_labels = []\n",
    "ensemble_predictions = []\n",
    "\n",
    "print(\"\\nEvaluating Weighted Averaging Ensemble Model in Batches\")\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing test batches\"):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "    batch_df = test_df.iloc[start_idx:end_idx]\n",
    "\n",
    "    candidate_texts_batch = batch_df['CandidateTitleTokens'].tolist()\n",
    "    history_texts_batch = batch_df['HistoryTokens'].tolist()\n",
    "    labels_batch = batch_df['Label'].tolist()\n",
    "\n",
    "    # Prepare candidate inputs\n",
    "    candidate_padded_batch = pad_sequences(\n",
    "        candidate_texts_batch,\n",
    "        maxlen=max_title_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (batch_size, max_title_length)\n",
    "\n",
    "    # Prepare history inputs\n",
    "    history_padded_batch = []\n",
    "    for history_texts in history_texts_batch:\n",
    "        if len(history_texts) == 0:\n",
    "            # If user has no history, create a zero matrix\n",
    "            padded_user_history = np.zeros((max_history_length, max_title_length), dtype=np.int32)\n",
    "        else:\n",
    "            # Pad each title in the history to max_title_length\n",
    "            padded_titles = pad_sequences(\n",
    "                history_texts,\n",
    "                maxlen=max_title_length,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0\n",
    "            )  # Shape: (num_titles_in_history, max_title_length)\n",
    "\n",
    "            # Pad or truncate the history to max_history_length\n",
    "            if len(padded_titles) < max_history_length:\n",
    "                pad_length = max_history_length - len(padded_titles)\n",
    "                # Create padding titles\n",
    "                pad_titles = np.zeros((pad_length, max_title_length), dtype=np.int32)\n",
    "                # Concatenate padded titles with padding titles\n",
    "                padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "            else:\n",
    "                # Truncate to max_history_length\n",
    "                padded_user_history = padded_titles[:max_history_length]\n",
    "\n",
    "        history_padded_batch.append(padded_user_history)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    history_padded_batch = np.array(history_padded_batch)  # Shape: (batch_size, max_history_length, max_title_length)\n",
    "\n",
    "    inputs = {\n",
    "        'history_input': history_padded_batch,\n",
    "        'candidate_input': candidate_padded_batch\n",
    "    }\n",
    "\n",
    "    # Get weighted predictions\n",
    "    total_weight = sum(model_weights.values())\n",
    "    weighted_sum = np.zeros(len(batch_df))\n",
    "\n",
    "    for cluster, model in models.items():\n",
    "        # Suppress progress bar by setting verbose=0\n",
    "        predictions = model.predict(inputs, verbose=0).flatten()\n",
    "        weight = model_weights.get(cluster, 1)\n",
    "        weighted_sum += predictions * weight\n",
    "\n",
    "    final_predictions = weighted_sum / total_weight\n",
    "\n",
    "    ensemble_true_labels.extend(labels_batch)\n",
    "    ensemble_predictions.extend(final_predictions)\n",
    "\n",
    "# After processing all batches, proceed with evaluation metrics as before\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "ensemble_true_labels = np.array(ensemble_true_labels)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_predicted_labels = (ensemble_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "auc = roc_auc_score(ensemble_true_labels, ensemble_predictions)\n",
    "accuracy = accuracy_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "precision = precision_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "recall = recall_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "f1 = f1_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "\n",
    "print(\"\\nWeighted Averaging Ensemble Model Performance:\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(ensemble_true_labels, ensemble_predicted_labels, digits=4)\n",
    "print(\"\\nClassification Report for Weighted Averaging Ensemble Model:\\n\", report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(ensemble_true_labels, ensemble_predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Weighted Averaging Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Example: Collect individual model metrics\n",
    "cluster_metrics = []\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    # Retrieve true labels and predictions for each cluster\n",
    "    true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "    predictions = models[cluster].predict(test_generators[cluster]).flatten()\n",
    "    predicted_labels = (predictions >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    auc_cluster = roc_auc_score(true_labels, predictions)\n",
    "    accuracy_cluster = accuracy_score(true_labels, predicted_labels)\n",
    "    precision_cluster = precision_score(true_labels, predicted_labels)\n",
    "    recall_cluster = recall_score(true_labels, predicted_labels)\n",
    "    f1_cluster = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    cluster_metrics.append({\n",
    "        'Model': f'Cluster {cluster} Model',\n",
    "        'AUC': auc_cluster,\n",
    "        'Accuracy': accuracy_cluster,\n",
    "        'Precision': precision_cluster,\n",
    "        'Recall': recall_cluster,\n",
    "        'F1 Score': f1_cluster\n",
    "    })\n",
    "\n",
    "# Add ensemble model metrics\n",
    "cluster_metrics.append({\n",
    "    'Model': 'Weighted Averaging Ensemble',\n",
    "    'AUC': auc,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "performance_df = pd.DataFrame(cluster_metrics)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(performance_df)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot AUC scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='AUC', data=performance_df)\n",
    "plt.title('AUC Score Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=performance_df)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7f922-05b9-476d-93fc-1539d349c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute metrics\n",
    "auc = roc_auc_score(ensemble_true_labels, ensemble_predictions)\n",
    "accuracy = accuracy_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "precision = precision_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "recall = recall_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "f1 = f1_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "\n",
    "print(\"\\nWeighted Averaging Ensemble Model Performance:\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(ensemble_true_labels, ensemble_predicted_labels, digits=4)\n",
    "print(\"\\nClassification Report for Weighted Averaging Ensemble Model:\\n\", report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(ensemble_true_labels, ensemble_predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Weighted Averaging Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Example: Collect individual model metrics\n",
    "cluster_metrics = []\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    # Retrieve true labels and predictions for each cluster\n",
    "    true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "    predictions = models[cluster].predict(test_generators[cluster]).flatten()\n",
    "    predicted_labels = (predictions >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    auc_cluster = roc_auc_score(true_labels, predictions)\n",
    "    accuracy_cluster = accuracy_score(true_labels, predicted_labels)\n",
    "    precision_cluster = precision_score(true_labels, predicted_labels)\n",
    "    recall_cluster = recall_score(true_labels, predicted_labels)\n",
    "    f1_cluster = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    cluster_metrics.append({\n",
    "        'Model': f'Cluster {cluster} Model',\n",
    "        'AUC': auc_cluster,\n",
    "        'Accuracy': accuracy_cluster,\n",
    "        'Precision': precision_cluster,\n",
    "        'Recall': recall_cluster,\n",
    "        'F1 Score': f1_cluster\n",
    "    })\n",
    "\n",
    "# Add ensemble model metrics\n",
    "cluster_metrics.append({\n",
    "    'Model': 'Weighted Averaging Ensemble',\n",
    "    'AUC': auc,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "performance_df = pd.DataFrame(cluster_metrics)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(performance_df)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot AUC scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='AUC', data=performance_df)\n",
    "plt.title('AUC Score Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=performance_df)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069ef22-89f4-4b97-840c-611d0cb8da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    model = models[cluster]\n",
    "    # Assuming 'model' is your Keras model\n",
    "    #plot_model(model, to_file=f\"model_architecture_{cluster}.png\", show_shapes=True, show_layer_names=True)\n",
    "    SVG(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6972612-4cee-4fa2-9e77-ca3fa78d2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, max_history_length, max_title_length):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_history_length = max_history_length\n",
    "        self.max_title_length = max_title_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # Extract data\n",
    "        history_tokens = batch['HistoryTokens'].tolist()  # List of lists of lists\n",
    "        candidate_titles = batch['CandidateTitleTokens'].tolist()\n",
    "        labels = batch['Label'].astype(np.float32).values  # Ensure labels are float32\n",
    "\n",
    "        # Pad candidate titles\n",
    "        candidate_titles_padded = pad_sequences(\n",
    "            candidate_titles,\n",
    "            maxlen=self.max_title_length,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "        # Initialize list to store padded histories\n",
    "        history_padded_batch = []\n",
    "\n",
    "        for user_history in history_tokens:\n",
    "            # user_history is a list of lists (titles)\n",
    "            if not user_history:\n",
    "                # If user has no history, create a zero matrix\n",
    "                padded_user_history = np.zeros((self.max_history_length, self.max_title_length), dtype=np.int32)\n",
    "            else:\n",
    "                # Pad each title in the history\n",
    "                padded_titles = pad_sequences(\n",
    "                    user_history,\n",
    "                    maxlen=self.max_title_length,\n",
    "                    padding='post',\n",
    "                    truncating='post',\n",
    "                    value=0\n",
    "                )\n",
    "\n",
    "                # Pad/truncate the history to max_history_length\n",
    "                if len(padded_titles) < self.max_history_length:\n",
    "                    pad_length = self.max_history_length - len(padded_titles)\n",
    "                    # Create padding titles\n",
    "                    pad_titles = np.zeros((pad_length, self.max_title_length), dtype=np.int32)\n",
    "                    # Concatenate padded titles with padding titles\n",
    "                    padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "                else:\n",
    "                    # Truncate to max_history_length\n",
    "                    padded_user_history = padded_titles[:self.max_history_length]\n",
    "\n",
    "            history_padded_batch.append(padded_user_history)\n",
    "\n",
    "        # Convert list to numpy array\n",
    "        history_padded_batch = np.array(history_padded_batch, dtype=np.int32)\n",
    "\n",
    "        # Create a dictionary for inputs\n",
    "        inputs = {\n",
    "            'history_input': history_padded_batch,\n",
    "            'candidate_input': candidate_titles_padded\n",
    "        }\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "# --- [Loading and Preparing Data] ---\n",
    "\n",
    "# Load or initialize your tokenizer\n",
    "tokenizer_path = 'tokenizer.pkl'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"\\nLoaded existing tokenizer.\")\n",
    "else:\n",
    "    # If not, initialize and fit a new tokenizer (not recommended if training was already done)\n",
    "    tokenizer = Tokenizer(num_words=63346, oov_token='<OOV>')  # Adjust num_words as per your setup\n",
    "    tokenizer.fit_on_texts(news_df['Title'])\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\"\\nInitialized and fitted a new tokenizer.\")\n",
    "\n",
    "# Convert titles to sequences\n",
    "news_df['TitleTokens'] = tokenizer.texts_to_sequences(news_df['Title'])\n",
    "\n",
    "print(\"\\nSample tokenized titles:\")\n",
    "print(news_df[['NewsID', 'Title', 'TitleTokens']].head())\n",
    "\n",
    "# Create a mapping from NewsID to TitleTokens\n",
    "newsid_to_tokens = dict(zip(news_df['NewsID'], news_df['TitleTokens']))\n",
    "\n",
    "# Function to parse impressions and create labeled data\n",
    "def parse_impressions(row):\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    history_titles = row['HistoryText'].split(' ') if isinstance(row['HistoryText'], str) else []\n",
    "    data = []\n",
    "    for impression in impressions:\n",
    "        if '-' not in impression:\n",
    "            continue  # Skip malformed impressions\n",
    "        news_id, label = impression.split('-')\n",
    "        title_tokens = newsid_to_tokens.get(news_id, [])\n",
    "        data.append({\n",
    "            'UserID': row['UserID'],\n",
    "            'HistoryTitles': history_titles,  # List of NewsIDs\n",
    "            'CandidateTitleTokens': title_tokens,\n",
    "            'Label': int(label)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Apply the function to each row in the validation behaviors with a progress bar\n",
    "test_data = []\n",
    "for idx, row in tqdm(valid_behaviors_df.iterrows(), total=valid_behaviors_df.shape[0], desc=\"Processing behaviors\"):\n",
    "    parsed = parse_impressions(row)\n",
    "    test_data.extend(parsed)\n",
    "\n",
    "# Create a DataFrame from the test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"\\nCreated test_df with samples:\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nTotal test samples: {len(test_df)}\")\n",
    "\n",
    "# Function to convert NewsIDs in history to token sequences\n",
    "def convert_history(news_ids):\n",
    "    return [newsid_to_tokens.get(news_id, []) for news_id in news_ids]\n",
    "\n",
    "# Apply the function to convert history\n",
    "test_df['HistoryTokens'] = test_df['HistoryTitles'].apply(convert_history)\n",
    "\n",
    "print(\"\\nSample HistoryTokens:\")\n",
    "print(test_df[['UserID', 'HistoryTitles', 'HistoryTokens']].head())\n",
    "\n",
    "# --- [Loading Clustering Artifacts] ---\n",
    "\n",
    "# Load user_cluster_df or recreate it\n",
    "user_cluster_path = 'user_cluster_df.pkl'\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "\n",
    "# Check if user_cluster_df.pkl exists\n",
    "if os.path.exists(user_cluster_path):\n",
    "    user_cluster_df = pd.read_pickle(user_cluster_path)\n",
    "    print(\"\\nLoaded user_cluster_df.\")\n",
    "else:\n",
    "    print(\"\\nuser_cluster_df.pkl not found. Recreating cluster assignments.\")\n",
    "    \n",
    "    # Load the clustering model\n",
    "    if os.path.exists(clustering_model_path):\n",
    "        with open(clustering_model_path, 'rb') as f:\n",
    "            clustering_model = pickle.load(f)\n",
    "        print(\"\\nLoaded clustering model.\")\n",
    "    else:\n",
    "        print(\"Clustering model not found. Please ensure you have saved the clustering model during training.\")\n",
    "        # Handle the error or exit\n",
    "        raise FileNotFoundError(\"Clustering model not found.\")\n",
    "    \n",
    "    # Load user_category_profiles\n",
    "    if os.path.exists(user_category_profiles_path):\n",
    "        user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "        print(\"\\nLoaded user_category_profiles.\")\n",
    "    else:\n",
    "        print(\"user_category_profiles.pkl not found. Cannot proceed without user profiles.\")\n",
    "        raise FileNotFoundError(\"User profiles not found.\")\n",
    "    \n",
    "    # Ensure test users are included in user_category_profiles\n",
    "    test_user_ids = test_df['UserID'].unique()\n",
    "    missing_user_ids = set(test_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"\\nAdding {len(missing_user_ids)} new users to user_category_profiles.\")\n",
    "        new_user_profiles = pd.DataFrame(0, index=missing_user_ids, columns=user_category_profiles.columns)\n",
    "        user_category_profiles = user_category_profiles.append(new_user_profiles)\n",
    "    \n",
    "    # Load scaler\n",
    "    if os.path.exists(scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"\\nLoaded scaler for user profiles.\")\n",
    "    else:\n",
    "        print(\"Scaler not found. Please ensure you have saved the scaler during training.\")\n",
    "        raise FileNotFoundError(\"Scaler not found.\")\n",
    "    \n",
    "    # Scale user profiles\n",
    "    user_profiles_for_clustering = user_category_profiles.loc[test_user_ids]\n",
    "    user_profiles_scaled = scaler.transform(user_profiles_for_clustering)\n",
    "    \n",
    "    # Assign clusters\n",
    "    test_user_clusters = clustering_model.predict(user_profiles_scaled)\n",
    "    \n",
    "    # Create user_cluster_df\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'UserID': user_profiles_for_clustering.index,\n",
    "        'Cluster': test_user_clusters\n",
    "    })\n",
    "    user_cluster_df.to_pickle('user_cluster_df.pkl')\n",
    "    print(\"\\nAssigned clusters to test users and saved to 'user_cluster_df.pkl'.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(user_cluster_df.head())\n",
    "\n",
    "# Check columns\n",
    "print(user_cluster_df.columns)\n",
    "\n",
    "# Check if 'UserID' is a column\n",
    "if 'UserID' in user_cluster_df.columns:\n",
    "    print(\"'UserID' is a column.\")\n",
    "else:\n",
    "    print(\"'UserID' is not a column. It might be the index.\")\n",
    "user_cluster_mapping = user_cluster_df['Cluster']\n",
    "\n",
    "# Perform the mapping\n",
    "test_df['Cluster'] = test_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "# Handle users not found in user_cluster_df\n",
    "missing_clusters = test_df['Cluster'].isna().sum()\n",
    "if missing_clusters > 0:\n",
    "    print(f\"\\nNumber of users without cluster assignment: {missing_clusters}\")\n",
    "    test_df['Cluster'].fillna(0, inplace=True)  # Assign to Cluster 0 or any default cluster\n",
    "    print(\"Assigned default cluster 0 to users without cluster assignment.\")\n",
    "\n",
    "print(\"\\nAssigned clusters to test_df:\")\n",
    "print(test_df['Cluster'].value_counts())\n",
    "\n",
    "# --- [Creating Test Data per Cluster] ---\n",
    "\n",
    "# Initialize a dictionary to hold test data per cluster\n",
    "cluster_test_data = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = test_df[test_df['Cluster'] == cluster].reset_index(drop=True)\n",
    "    cluster_test_data[cluster] = cluster_data\n",
    "    print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "\n",
    "# Define maximum lengths (should match training)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "batch_size = 64  # Adjust as needed\n",
    "def add_output_sig(datagen):\n",
    "    output_signature = (\n",
    "        {\n",
    "            'history_input': tf.TensorSpec(shape=(None, max_history_length, max_title_length), dtype=tf.int32),\n",
    "            'candidate_input': tf.TensorSpec(shape=(None, max_title_length), dtype=tf.int32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: datagen,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    dataset = dataset.repeat(1)\n",
    "\n",
    "    # Optional caching and prefetching\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "# Initialize test DataGenerators per cluster\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    test_generators[cluster] = add_output_sig(DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    ))\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "\n",
    "# --- [Evaluating Models] ---\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = model.evaluate(\n",
    "        test_generator,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Metrics for Cluster {cluster}:\")\n",
    "    for metric_name, value in zip(model.metrics_names, results):\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# --- [Generating Classification Reports] ---\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(models)\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nGenerating Classification Report for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Retrieve true labels\n",
    "    true_labels = cluster_test_data[cluster]['Label'].values\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(true_labels, predicted_labels, digits=4)\n",
    "    print(f\"Classification Report for Cluster {cluster}:\\n{report}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for Cluster {cluster}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792ea8c-f030-47cc-af4c-479eeb4fe53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- [Loading and Preparing Data] ---\n",
    "\n",
    "# Load or initialize your tokenizer\n",
    "tokenizer_path = 'tokenizer.pkl'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"\\nLoaded existing tokenizer.\")\n",
    "else:\n",
    "    # If not, initialize and fit a new tokenizer (not recommended if training was already done)\n",
    "    tokenizer = Tokenizer(num_words=63346, oov_token='<OOV>')  # Adjust num_words as per your setup\n",
    "    tokenizer.fit_on_texts(news_df['Title'])\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\"\\nInitialized and fitted a new tokenizer.\")\n",
    "\n",
    "# Convert titles to sequences\n",
    "news_df['TitleTokens'] = tokenizer.texts_to_sequences(news_df['Title'])\n",
    "\n",
    "print(\"\\nSample tokenized titles:\")\n",
    "print(news_df[['NewsID', 'Title', 'TitleTokens']].head())\n",
    "\n",
    "# Create a mapping from NewsID to TitleTokens\n",
    "newsid_to_tokens = dict(zip(news_df['NewsID'], news_df['TitleTokens']))\n",
    "\n",
    "# Function to parse impressions and create labeled data\n",
    "def parse_impressions(row):\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    history_titles = row['HistoryText'].split(' ') if isinstance(row['HistoryText'], str) else []\n",
    "    data = []\n",
    "    for impression in impressions:\n",
    "        if '-' not in impression:\n",
    "            continue  # Skip malformed impressions\n",
    "        news_id, label = impression.split('-')\n",
    "        title_tokens = newsid_to_tokens.get(news_id, [])\n",
    "        data.append({\n",
    "            'UserID': row['UserID'],\n",
    "            'HistoryTitles': history_titles,  # List of NewsIDs\n",
    "            'CandidateTitleTokens': title_tokens,\n",
    "            'Label': int(label)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Apply the function to each row in the validation behaviors with a progress bar\n",
    "test_data = []\n",
    "for idx, row in tqdm(valid_behaviors_df.iterrows(), total=valid_behaviors_df.shape[0], desc=\"Processing behaviors\"):\n",
    "    parsed = parse_impressions(row)\n",
    "    test_data.extend(parsed)\n",
    "\n",
    "# Create a DataFrame from the test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"\\nCreated test_df with samples:\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nTotal test samples: {len(test_df)}\")\n",
    "\n",
    "# Function to convert NewsIDs in history to token sequences\n",
    "def convert_history(news_ids):\n",
    "    return [newsid_to_tokens.get(news_id, []) for news_id in news_ids]\n",
    "\n",
    "# Apply the function to convert history\n",
    "test_df['HistoryTokens'] = test_df['HistoryTitles'].apply(convert_history)\n",
    "\n",
    "print(\"\\nSample HistoryTokens:\")\n",
    "print(test_df[['UserID', 'HistoryTitles', 'HistoryTokens']].head())\n",
    "\n",
    "# --- [Loading Clustering Artifacts] ---\n",
    "\n",
    "# Paths to clustering artifacts\n",
    "user_cluster_path = 'user_cluster_df.pkl'\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "\n",
    "# Check if user_cluster_df.pkl exists\n",
    "if os.path.exists(user_cluster_path):\n",
    "    user_cluster_df = pd.read_pickle(user_cluster_path)\n",
    "    print(\"\\nLoaded user_cluster_df.\")\n",
    "else:\n",
    "    print(\"\\nuser_cluster_df.pkl not found. Recreating cluster assignments.\")\n",
    "    # Load the clustering model\n",
    "    if os.path.exists(clustering_model_path):\n",
    "        with open(clustering_model_path, 'rb') as f:\n",
    "            clustering_model = pickle.load(f)\n",
    "        print(\"\\nLoaded clustering model.\")\n",
    "    else:\n",
    "        print(\"Clustering model not found. Please ensure you have saved the clustering model during training.\")\n",
    "        raise FileNotFoundError(\"Clustering model not found.\")\n",
    "    \n",
    "    # Load user_category_profiles\n",
    "    if os.path.exists(user_category_profiles_path):\n",
    "        user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "        print(\"\\nLoaded user_category_profiles.\")\n",
    "    else:\n",
    "        print(\"user_category_profiles.pkl not found. Cannot proceed without user profiles.\")\n",
    "        raise FileNotFoundError(\"User profiles not found.\")\n",
    "    \n",
    "    # Load scaler\n",
    "    if os.path.exists(scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"\\nLoaded scaler for user profiles.\")\n",
    "    else:\n",
    "        print(\"Scaler not found. Please ensure you have saved the scaler during training.\")\n",
    "        raise FileNotFoundError(\"Scaler not found.\")\n",
    "    \n",
    "    # Ensure test users are included in user_category_profiles\n",
    "    test_user_ids = test_df['UserID'].unique()\n",
    "    missing_user_ids = set(test_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"\\nAdding {len(missing_user_ids)} new users to user_category_profiles.\")\n",
    "        new_user_profiles = pd.DataFrame(0, index=missing_user_ids, columns=user_category_profiles.columns)\n",
    "        user_category_profiles = pd.concat([user_category_profiles, new_user_profiles])\n",
    "        print(\"Added new user profiles for test users.\")\n",
    "    \n",
    "    # Scale user profiles\n",
    "    user_profiles_for_clustering = user_category_profiles.loc[test_user_ids]\n",
    "    user_profiles_scaled = scaler.transform(user_profiles_for_clustering)\n",
    "    \n",
    "    # Assign clusters\n",
    "    test_user_clusters = clustering_model.predict(user_profiles_scaled)\n",
    "    \n",
    "    # Create user_cluster_df with 'UserID' as index\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'Cluster': test_user_clusters\n",
    "    }, index=user_profiles_for_clustering.index)\n",
    "    \n",
    "    # Save the cluster assignments\n",
    "    user_cluster_df.to_pickle('user_cluster_df.pkl')\n",
    "    print(\"\\nAssigned clusters to test users and saved to 'user_cluster_df.pkl'.\")\n",
    "\n",
    "# Map clusters to test_df\n",
    "# Since 'UserID' is the index, access 'Cluster' directly\n",
    "user_cluster_mapping = user_cluster_df['Cluster']  # Series with UserID as index\n",
    "\n",
    "# Perform the mapping\n",
    "test_df['Cluster'] = test_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "# Handle users not found in user_cluster_df\n",
    "missing_clusters = test_df['Cluster'].isna().sum()\n",
    "if missing_clusters > 0:\n",
    "    print(f\"\\nNumber of users without cluster assignment: {missing_clusters}\")\n",
    "    # Replace the following line to avoid FutureWarning\n",
    "    test_df['Cluster'] = test_df['Cluster'].fillna(0)  # Assign to Cluster 0 or any default cluster\n",
    "    print(\"Assigned default cluster 0 to users without cluster assignment.\")\n",
    "\n",
    "print(\"\\nAssigned clusters to test_df:\")\n",
    "print(test_df['Cluster'].value_counts())\n",
    "\n",
    "# --- [Creating Test Data per Cluster] ---\n",
    "\n",
    "# Define the number of clusters (ensure it matches your training)\n",
    "num_clusters = 3  # Adjust as needed\n",
    "\n",
    "# Initialize a dictionary to hold test data per cluster\n",
    "cluster_test_data = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = test_df[test_df['Cluster'] == cluster].reset_index(drop=True)\n",
    "    cluster_test_data[cluster] = cluster_data\n",
    "    print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "\n",
    "# Define maximum lengths (should match training)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Define your DataGenerator class (ensure it's correctly implemented)\n",
    "# Example:\n",
    "# class DataGenerator(Sequence):\n",
    "#     ...\n",
    "\n",
    "# Initialize test DataGenerators per cluster\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    test_generators[cluster] = add_output_sig(DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    ))\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "\n",
    "# --- [Evaluating Models] ---\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        results = model.evaluate(\n",
    "            test_generator,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model for Cluster {cluster}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Metrics for Cluster {cluster}:\")\n",
    "    for metric_name, value in zip(model.metrics_names, results):\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# --- [Generating Classification Reports] ---\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nGenerating Classification Report for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Generate predictions\n",
    "        predictions = model.predict(test_generator, verbose=1)\n",
    "        predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Retrieve true labels\n",
    "        true_labels = cluster_test_data[cluster]['Label'].values\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(true_labels, predicted_labels, digits=4)\n",
    "        print(f\"Classification Report for Cluster {cluster}:\\n{report}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for Cluster {cluster}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for Cluster {cluster}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6cc0c-7d92-49cd-aa02-3ed21961a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have access to training and validation logs\n",
    "for cluster in range(num_clusters):\n",
    "    # Load training and validation logs\n",
    "    train_log_path = f'training_log_cluster_{cluster}.csv'\n",
    "    try:\n",
    "        train_log_df = pd.read_csv(train_log_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Training log for Cluster {cluster} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Get the last epoch metrics\n",
    "    train_auc = train_log_df['AUC'].iloc[-1]\n",
    "    val_auc = train_log_df['val_AUC'].iloc[-1]\n",
    "    \n",
    "    # Retrieve test AUC\n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        test_auc = None\n",
    "    else:\n",
    "        results = model.evaluate(test_generator, verbose=0)\n",
    "        test_auc = results[model.metrics_names.index('AUC')]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster} Metrics:\")\n",
    "    print(f\"  Training AUC: {train_auc:.4f}\")\n",
    "    print(f\"  Validation AUC: {val_auc:.4f}\")\n",
    "    if test_auc is not None:\n",
    "        print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    else:\n",
    "        print(\"  Test AUC: Not Available\")\n",
    "import seaborn as sns\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nVisualizing Prediction Distribution for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        continue\n",
    "    \n",
    "    predictions = model.predict(test_generator, verbose=1).flatten()\n",
    "    true_labels = test_test_data[cluster]['Label'].values\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(predictions[true_labels == 1], shade=True, label='Positive Label')\n",
    "    sns.kdeplot(predictions[true_labels == 0], shade=True, label='Negative Label')\n",
    "    plt.title(f'Prediction Score Distribution for Cluster {cluster}')\n",
    "    plt.xlabel('Predicted Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# Example: Modify your model to output attention weights (if not already done)\n",
    "# Assuming your model is built to return attention weights\n",
    "\n",
    "# Create a separate model for attention extraction\n",
    "attention_model = Model(\n",
    "    inputs=model.input,\n",
    "    outputs=[model.get_layer('user_encoder').output[1], model.get_layer('news_encoder').output[1]]  # Adjust layer names as needed\n",
    ")\n",
    "\n",
    "# Select a sample from the test set\n",
    "sample = test_df.iloc[0]\n",
    "history_tokens = convert_history(sample['HistoryTitles'])\n",
    "candidate_tokens = sample['CandidateTitleTokens']\n",
    "\n",
    "# Prepare input\n",
    "history_padded = pad_sequences(\n",
    "    [history_tokens],\n",
    "    maxlen=max_history_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=0\n",
    ")\n",
    "candidate_padded = pad_sequences(\n",
    "    [candidate_tokens],\n",
    "    maxlen=max_title_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=0\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    'history_input': np.array(history_padded),\n",
    "    'candidate_input': np.array(candidate_padded)\n",
    "}\n",
    "\n",
    "# Get attention weights\n",
    "user_attention, candidate_attention = attention_model.predict(inputs)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap(user_attention, annot=False, cmap='viridis', cbar=False)\n",
    "plt.title('User History Attention Weights')\n",
    "plt.xlabel('History Sequence')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap(candidate_attention, annot=False, cmap='viridis', cbar=False)\n",
    "plt.title('Candidate News Attention Weights')\n",
    "plt.xlabel('Candidate Sequence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff986ae-34a5-495a-8d1c-a3e479417709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_batches(df, batch_size):\n",
    "    while True:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the data\n",
    "        for start in range(0, len(df), batch_size):\n",
    "            end = min(start + batch_size, len(df))\n",
    "            batch_df = df.iloc[start:end]\n",
    "            # Prepare inputs\n",
    "            history_batch = []\n",
    "            candidate_batch = []\n",
    "            labels_batch = []\n",
    "            for idx, row in batch_df.iterrows():\n",
    "                history_titles = row['HistoryTitles']  # Use the tokenized titles\n",
    "                # Pad each title in history\n",
    "                history_titles_padded = pad_sequences(history_titles, maxlen=max_title_length, padding='post', truncating='post', value=0)\n",
    "                # Pad or truncate the history to max_history_length\n",
    "                if len(history_titles_padded) < max_history_length:\n",
    "                    padding = np.zeros((max_history_length - len(history_titles_padded), max_title_length), dtype='int32')\n",
    "                    history_titles_padded = np.vstack([padding, history_titles_padded])\n",
    "                else:\n",
    "                    history_titles_padded = history_titles_padded[-max_history_length:]\n",
    "                candidate_title = row['CandidateTitleTokens']\n",
    "                candidate_title_padded = pad_sequences([candidate_title], maxlen=max_title_length, padding='post', truncating='post', value=0)[0]\n",
    "                history_batch.append(history_titles_padded)\n",
    "                candidate_batch.append(candidate_title_padded)\n",
    "                labels_batch.append(row['Label'])\n",
    "            yield [np.array(history_batch), np.array(candidate_batch)], np.array(labels_batch)\n",
    "# Assuming you have a test set prepared similarly\n",
    "test_generator = generate_batches(test_df, batch_size)\n",
    "test_steps = len(test_df) // batch_size\n",
    "# Evaluate using model.evaluate()\n",
    "evaluation = model.evaluate(test_generator, steps=test_steps)\n",
    "\n",
    "# Or compute metrics manually\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "for [history_batch, candidate_batch], labels_batch in test_generator:\n",
    "    preds = model.predict([history_batch, candidate_batch])\n",
    "    all_labels.extend(labels_batch)\n",
    "    all_preds.extend(preds)\n",
    "    if len(all_labels) >= len(test_df):\n",
    "        break\n",
    "all_labels = all_labels[:len(test_df)]\n",
    "all_preds = all_preds[:len(test_df)]\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(all_labels, all_preds)\n",
    "print(f'Test AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f5bbb-a88a-45bf-94fd-448e72a2bd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a547fc-f992-4730-b96d-b37710856427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f47309-0bae-42a2-8d2d-7609b8ec11f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ac5cc-6510-47f9-9f43-11cbb8d11083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
