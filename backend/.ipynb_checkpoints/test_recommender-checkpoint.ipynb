{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b15c815",
   "metadata": {},
   "source": [
    "# Testing Recommender Functions\n",
    "\n",
    "This notebook installs the required dependencies and tests the recommender functions defined in your modules (e.g. `utils.py` and `recommender.py`). It loads the models via the `get_models()` function and then tests various ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "#!pip install --upgrade pip\n",
    "#!pip install numpy scikit-learn tensorflow keras fastapi torch transformers\n",
    "\n",
    "# If your project has a requirements.txt file, you can also use:\n",
    "# !pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c8c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 11:40:08.110876: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-05 11:40:08.164196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741174808.188531     151 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741174808.195738     151 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 11:40:08.244768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "from utils import get_models  # Ensure these are in your PYTHONPATH\n",
    "from recommender import ensemble_bagging, ensemble_boosting, train_stacking_meta_model, ensemble_stacking, hybrid_ensemble, tokenize_input\n",
    "\n",
    "# For demonstration, we assume get_models() returns a dictionary of models for clusters 0, 1, 2, etc.\n",
    "print(\"Modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb7c730-e9d6-4a9f-b541-9c779858a1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.preprocessing.text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load the pre-saved tokenizer (assumes you already created and saved it)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtokenizer.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     tokenizer = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load MIND test data (adjust file paths as necessary)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Assume news.tsv contains columns: NewsID, Category, SubCategory, Title, Abstract, URL, TitleEntities, AbstractEntities\u001b[39;00m\n\u001b[32m     18\u001b[39m news_df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mnews.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m, \n\u001b[32m     19\u001b[39m                       names=[\u001b[33m'\u001b[39m\u001b[33mNewsID\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCategory\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSubCategory\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTitle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAbstract\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mURL\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTitleEntities\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAbstractEntities\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'keras.preprocessing.text'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Set maximum lengths (should match your model settings)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "\n",
    "# Load the pre-saved tokenizer (assumes you already created and saved it)\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Load MIND test data (adjust file paths as necessary)\n",
    "# Assume news.tsv contains columns: NewsID, Category, SubCategory, Title, Abstract, URL, TitleEntities, AbstractEntities\n",
    "news_df = pd.read_csv(\"news.tsv\", sep='\\t', \n",
    "                      names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'])\n",
    "# Assume behaviors_test.tsv contains: ImpressionID, UserID, Time, HistoryText, Impressions\n",
    "behaviors_df = pd.read_csv(\"behaviors_test.tsv\", sep='\\t', \n",
    "                           names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'])\n",
    "\n",
    "# Create a dictionary mapping NewsID to Title (or CombinedText if available)\n",
    "news_dict = dict(zip(news_df['NewsID'], news_df['Title']))\n",
    "\n",
    "# Select one sample from the test behaviors\n",
    "sample = behaviors_df.iloc[0]\n",
    "\n",
    "# Process history: split the HistoryText (a space-separated string of NewsIDs)\n",
    "history_text = sample['HistoryText']\n",
    "history_ids = history_text.split() if pd.notna(history_text) else []\n",
    "\n",
    "# Retrieve the title for each news ID in the history (default to empty string if missing)\n",
    "history_titles = [news_dict.get(nid, \"\") for nid in history_ids]\n",
    "\n",
    "# Convert history titles to sequences using the tokenizer\n",
    "history_sequences = tokenizer.texts_to_sequences(history_titles)\n",
    "# Pad each sequence to max_title_length\n",
    "history_padded = pad_sequences(history_sequences, maxlen=max_title_length, \n",
    "                               padding='post', truncating='post', value=0)\n",
    "\n",
    "# Ensure the history has exactly max_history_length rows:\n",
    "if history_padded.shape[0] < max_history_length:\n",
    "    # Pre-pad with zeros if there are fewer history items\n",
    "    pad_rows = np.zeros((max_history_length - history_padded.shape[0], max_title_length), dtype=int)\n",
    "    history_padded = np.vstack([pad_rows, history_padded])\n",
    "else:\n",
    "    # If too many, take the last max_history_length items\n",
    "    history_padded = history_padded[-max_history_length:]\n",
    "\n",
    "# Process candidate: the \"Impressions\" column is a space-separated list like \"newsID-label newsID-label ...\"\n",
    "impressions = sample['Impressions']\n",
    "first_candidate = impressions.split()[0]  # take the first candidate\n",
    "candidate_news_id = first_candidate.split('-')[0]\n",
    "candidate_title = news_dict.get(candidate_news_id, \"\")\n",
    "candidate_sequence = tokenizer.texts_to_sequences([candidate_title])\n",
    "candidate_padded = pad_sequences(candidate_sequence, maxlen=max_title_length, \n",
    "                                 padding='post', truncating='post', value=0)[0]\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "history_tensor = tf.convert_to_tensor([history_padded], dtype=tf.int32)  # shape: (1, max_history_length, max_title_length)\n",
    "candidate_tensor = tf.convert_to_tensor([candidate_padded], dtype=tf.int32)  # shape: (1, max_title_length)\n",
    "\n",
    "print(\"History tensor shape:\", history_tensor.shape)\n",
    "print(\"Candidate tensor shape:\", candidate_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f8ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Number of columns in user_category_profiles: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n",
      "Cluster 0: 44551 training samples, 11138 validation samples.\n",
      "Cluster 1: 48725 training samples, 12182 validation samples.\n",
      "Cluster 2: 53212 training samples, 13304 validation samples.\n",
      "\n",
      "Loading model for Cluster 0 from fastformer_cluster_0_full_balanced_1_epoch.keras\n",
      ".cache\n",
      ".ipynb_checkpoints\n",
      "backend copy 2.py\n",
      "backend copy.py\n",
      "backend-flask-unused.py\n",
      "backend.py\n",
      "data\n",
      "dataset\n",
      "Dockerfile\n",
      "downloads\n",
      "fastapi copy.py\n",
      "fastapi2.py\n",
      "fastformer.json\n",
      "fastformer_clusters.ipynb\n",
      "fastformer_cluster_0_full_balanced_1_epoch.h5\n",
      "fastformer_cluster_0_full_balanced_1_epoch.hdf5\n",
      "fastformer_cluster_0_full_balanced_1_epoch.json\n",
      "fastformer_cluster_0_full_balanced_1_epoch.keras\n",
      "fastformer_cluster_0_full_balanced_1_epoch.weights.h5\n",
      "fastformer_cluster_1_full_balanced_1_epoch.keras\n",
      "fastformer_cluster_2_full_balanced_1_epoch.keras\n",
      "fastformer_model.py\n",
      "gdrive.py\n",
      "models\n",
      "models.py\n",
      "recommender.py\n",
      "requirements.txt\n",
      "test_recommender.ipynb\n",
      "test_recommender.py\n",
      "tokenizer.pkl\n",
      "upload_to_hf.py\n",
      "user_category_profiles.pkl\n",
      "utils.py\n",
      "__pycache__\n",
      "2.18.0\n",
      "3.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 11:42:11.709549: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/usr/local/lib/python3.11/site-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'user_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 46 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model for Cluster 1 from fastformer_cluster_1_full_balanced_1_epoch.keras\n",
      ".cache\n",
      ".ipynb_checkpoints\n",
      "backend copy 2.py\n",
      "backend copy.py\n",
      "backend-flask-unused.py\n",
      "backend.py\n",
      "data\n",
      "dataset\n",
      "Dockerfile\n",
      "downloads\n",
      "fastapi copy.py\n",
      "fastapi2.py\n",
      "fastformer.json\n",
      "fastformer_clusters.ipynb\n",
      "fastformer_cluster_0_full_balanced_1_epoch.h5\n",
      "fastformer_cluster_0_full_balanced_1_epoch.hdf5\n",
      "fastformer_cluster_0_full_balanced_1_epoch.json\n",
      "fastformer_cluster_0_full_balanced_1_epoch.keras\n",
      "fastformer_cluster_0_full_balanced_1_epoch.weights.h5\n",
      "fastformer_cluster_1_full_balanced_1_epoch.keras\n",
      "fastformer_cluster_2_full_balanced_1_epoch.keras\n",
      "fastformer_model.py\n",
      "gdrive.py\n",
      "models\n",
      "models.py\n",
      "recommender.py\n",
      "requirements.txt\n",
      "test_recommender.ipynb\n",
      "test_recommender.py\n",
      "tokenizer.pkl\n",
      "upload_to_hf.py\n",
      "user_category_profiles.pkl\n",
      "utils.py\n",
      "__pycache__\n",
      "2.18.0\n",
      "3.8.0\n",
      "\n",
      "Loading model for Cluster 2 from fastformer_cluster_2_full_balanced_1_epoch.keras\n",
      ".cache\n",
      ".ipynb_checkpoints\n",
      "backend copy 2.py\n",
      "backend copy.py\n",
      "backend-flask-unused.py\n",
      "backend.py\n",
      "data\n",
      "dataset\n",
      "Dockerfile\n",
      "downloads\n",
      "fastapi copy.py\n",
      "fastapi2.py\n",
      "fastformer.json\n",
      "fastformer_clusters.ipynb\n",
      "fastformer_cluster_0_full_balanced_1_epoch.h5\n",
      "fastformer_cluster_0_full_balanced_1_epoch.hdf5\n",
      "fastformer_cluster_0_full_balanced_1_epoch.json\n",
      "fastformer_cluster_0_full_balanced_1_epoch.keras\n",
      "fastformer_cluster_0_full_balanced_1_epoch.weights.h5\n",
      "fastformer_cluster_1_full_balanced_1_epoch.keras\n",
      "fastformer_cluster_2_full_balanced_1_epoch.keras\n",
      "fastformer_model.py\n",
      "gdrive.py\n",
      "models\n",
      "models.py\n",
      "recommender.py\n",
      "requirements.txt\n",
      "test_recommender.ipynb\n",
      "test_recommender.py\n",
      "tokenizer.pkl\n",
      "upload_to_hf.py\n",
      "user_category_profiles.pkl\n",
      "utils.py\n",
      "__pycache__\n",
      "2.18.0\n",
      "3.8.0\n",
      "Returning models list\n",
      "{0: <Functional name=functional, built=True>, 1: <Functional name=functional_1, built=True>, 2: <Functional name=functional_2, built=True>}\n",
      "Models loaded: dict_keys([0, 1, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(1, 2) dtype=int64>]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m test_input = \u001b[33m\"\u001b[39m\u001b[33mThis is a dummy test input for the recommender functions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Test ensemble bagging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m bagging_pred = \u001b[43mensemble_bagging\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnsemble Bagging Prediction:\u001b[39m\u001b[33m\"\u001b[39m, bagging_pred)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Test ensemble boosting with dummy error values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/recommender.py:27\u001b[39m, in \u001b[36mensemble_bagging\u001b[39m\u001b[34m(input_text, models)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mensemble_bagging\u001b[39m(input_text: \u001b[38;5;28mstr\u001b[39m, models: \u001b[38;5;28mdict\u001b[39m) -> np.ndarray:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     y1 = \u001b[43mfastformer_model1_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     y2 = fastformer_model2_predict(input_text, models[\u001b[32m1\u001b[39m])\n\u001b[32m     29\u001b[39m     y3 = fastformer_model3_predict(input_text, models[\u001b[32m2\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/recommender.py:13\u001b[39m, in \u001b[36mfastformer_model1_predict\u001b[39m\u001b[34m(input_text, model)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfastformer_model1_predict\u001b[39m(input_text: \u001b[38;5;28mstr\u001b[39m, model):\n\u001b[32m     12\u001b[39m     input_arr = tokenize_input(input_text)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m preds[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/keras/src/layers/input_spec.py:160\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    158\u001b[39m inputs = tree.flatten(inputs)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) != \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLayer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input(s),\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input tensors. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Layer \"functional\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(1, 2) dtype=int64>]"
     ]
    }
   ],
   "source": [
    "# Load ensemble models using the get_models function\n",
    "print(\"Loading models...\")\n",
    "models_dict = get_models()\n",
    "print(\"Models loaded:\", models_dict.keys())\n",
    "\n",
    "# Define a test input text for recommendation\n",
    "test_input = \"This is a dummy test input for the recommender functions.\"\n",
    "\n",
    "# Test ensemble bagging\n",
    "bagging_pred = ensemble_bagging(test_input, models_dict)\n",
    "print(\"Ensemble Bagging Prediction:\", bagging_pred)\n",
    "\n",
    "# Test ensemble boosting with dummy error values\n",
    "dummy_errors = np.array([0.2, 0.15, 0.25])\n",
    "boosting_pred = ensemble_boosting(test_input, models_dict, dummy_errors)\n",
    "print(\"Ensemble Boosting Prediction:\", boosting_pred)\n",
    "\n",
    "# Test ensemble stacking with dummy training data\n",
    "X_train_dummy = np.array([\n",
    "    [0.80, 0.75, 0.85],\n",
    "    [0.55, 0.60, 0.50],\n",
    "    [0.30, 0.35, 0.25],\n",
    "    [0.20, 0.25, 0.15]\n",
    "])\n",
    "y_train_dummy = np.array([1, 0, 1, 0])\n",
    "meta_model = train_stacking_meta_model(X_train_dummy, y_train_dummy)\n",
    "stacking_pred = ensemble_stacking(test_input, models_dict, meta_model)\n",
    "print(\"Ensemble Stacking Prediction:\", stacking_pred)\n",
    "\n",
    "# Test hybrid ensemble\n",
    "hybrid_pred = hybrid_ensemble(test_input, models_dict, dummy_errors, meta_model)\n",
    "print(\"Hybrid Ensemble Prediction:\", hybrid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2e6a8",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "You can now develop and test your recommendation functions independently of the FastAPI backend. \n",
    "\n",
    "For further debugging, you might want to add additional print statements or assertions within your recommender functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
