2025-04-03 13:40:30,304 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
2025-04-03 15:01:36,827 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
2025-04-03 15:12:36,448 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
2025-04-03 15:14:03,400 - INFO - Starting user U87243 in cluster 2 (index 0)
2025-04-03 15:15:18,247 - INFO - lens: newsdf:101527candidate_pool:14638,candidate_pool:14638
2025-04-03 15:16:56,321 - INFO - Starting user U598644 in cluster 2 (index 1)
2025-04-03 15:17:51,706 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 15:18:18,405 - INFO - Starting user U532401 in cluster 2 (index 2)
2025-04-03 15:18:18,491 - INFO - history_titles:[] empty for user: U532401!!!
2025-04-03 15:19:36,546 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:20:03,322 - INFO - Starting user U593596 in cluster 2 (index 3)
2025-04-03 15:20:58,627 - INFO - lens: newsdf:101527candidate_pool:14636,candidate_pool:14636
2025-04-03 15:21:34,892 - INFO - Starting user U521853 in cluster 2 (index 4)
2025-04-03 15:22:33,899 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 15:23:03,098 - INFO - Starting user U687515 in cluster 2 (index 5)
2025-04-03 15:23:03,174 - INFO - history_titles:[] empty for user: U687515!!!
2025-04-03 15:23:58,103 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:24:02,916 - INFO - Starting user U192112 in cluster 2 (index 6)
2025-04-03 15:25:01,699 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 15:25:34,244 - INFO - Starting user U530668 in cluster 2 (index 7)
2025-04-03 15:26:29,447 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:26:34,074 - INFO - Starting user U290933 in cluster 2 (index 8)
2025-04-03 15:27:29,720 - INFO - lens: newsdf:101527candidate_pool:14638,candidate_pool:14638
2025-04-03 15:27:35,100 - INFO - Starting user U108656 in cluster 2 (index 9)
2025-04-03 15:28:37,658 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 15:28:43,300 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U87243', 'user_index_in_cluster': 0, 'num_candidates': 14638, 'num_history_articles': 16, 'original_history_len': 16, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U598644', 'user_index_in_cluster': 1, 'num_candidates': 14640, 'num_history_articles': 24, 'original_history_len': 24, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U532401', 'user_index_in_cluster': 2, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U593596', 'user_index_in_cluster': 3, 'num_candidates': 14636, 'num_history_articles': 13, 'original_history_len': 13, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.001953125, 'ndcg_1000': 0.030532264901341654, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.14285714285714285, 'ap_2000': 0.001953125, 'ndcg_2000': 0.030532264901341654, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U521853', 'user_index_in_cluster': 4, 'num_candidates': 14641, 'num_history_articles': 14, 'original_history_len': 14, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.03225806451612903, 'ndcg_50': 0.0938557452045513, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.03225806451612903, 'ndcg_100': 0.0938557452045513, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.03225806451612903, 'ndcg_200': 0.0938557452045513, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.03225806451612903, 'ndcg_500': 0.0938557452045513, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.03225806451612903, 'ndcg_1000': 0.0938557452045513, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.03225806451612903, 'ndcg_2000': 0.0938557452045513, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U687515', 'user_index_in_cluster': 5, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U192112', 'user_index_in_cluster': 6, 'num_candidates': 14642, 'num_history_articles': 13, 'original_history_len': 13, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U530668', 'user_index_in_cluster': 7, 'num_candidates': 14643, 'num_history_articles': 11, 'original_history_len': 11, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U290933', 'user_index_in_cluster': 8, 'num_candidates': 14638, 'num_history_articles': 47, 'original_history_len': 47, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U108656', 'user_index_in_cluster': 9, 'num_candidates': 14640, 'num_history_articles': 13, 'original_history_len': 13, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 15:28:43,323 - INFO - Starting user U178651 in cluster 2 (index 10)
2025-04-03 15:29:38,593 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 15:29:43,770 - INFO - Starting user U86679 in cluster 2 (index 11)
2025-04-03 15:30:43,792 - INFO - lens: newsdf:101527candidate_pool:14636,candidate_pool:14636
2025-04-03 15:30:49,006 - INFO - Starting user U95671 in cluster 2 (index 12)
2025-04-03 15:31:43,893 - INFO - lens: newsdf:101527candidate_pool:14632,candidate_pool:14632
2025-04-03 15:32:31,258 - INFO - Starting user U656379 in cluster 2 (index 13)
2025-04-03 15:33:27,511 - INFO - lens: newsdf:101527candidate_pool:14633,candidate_pool:14633
2025-04-03 15:34:02,251 - INFO - Starting user U52262 in cluster 2 (index 14)
2025-04-03 15:35:02,360 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 15:35:10,455 - INFO - Starting user U629430 in cluster 2 (index 15)
2025-04-03 15:36:05,689 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 15:36:10,507 - INFO - Starting user U419371 in cluster 2 (index 16)
2025-04-03 15:37:10,026 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 15:37:15,143 - INFO - Starting user U594274 in cluster 2 (index 17)
2025-04-03 15:38:10,487 - INFO - lens: newsdf:101527candidate_pool:14631,candidate_pool:14631
2025-04-03 15:38:41,398 - INFO - Starting user U6325 in cluster 2 (index 18)
2025-04-03 15:38:41,571 - INFO - history_titles:[] empty for user: U6325!!!
2025-04-03 15:39:41,031 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:39:46,331 - INFO - Starting user U597276 in cluster 2 (index 19)
2025-04-03 15:40:42,344 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 15:40:47,231 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U178651', 'user_index_in_cluster': 10, 'num_candidates': 14641, 'num_history_articles': 12, 'original_history_len': 12, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U86679', 'user_index_in_cluster': 11, 'num_candidates': 14636, 'num_history_articles': 15, 'original_history_len': 15, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U95671', 'user_index_in_cluster': 12, 'num_candidates': 14632, 'num_history_articles': 41, 'original_history_len': 41, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U656379', 'user_index_in_cluster': 13, 'num_candidates': 14633, 'num_history_articles': 48, 'original_history_len': 48, 'num_future_clicks': 10, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U52262', 'user_index_in_cluster': 14, 'num_candidates': 14642, 'num_history_articles': 5, 'original_history_len': 5, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U629430', 'user_index_in_cluster': 15, 'num_candidates': 14642, 'num_history_articles': 10, 'original_history_len': 10, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U419371', 'user_index_in_cluster': 16, 'num_candidates': 14640, 'num_history_articles': 31, 'original_history_len': 31, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U594274', 'user_index_in_cluster': 17, 'num_candidates': 14631, 'num_history_articles': 27, 'original_history_len': 27, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U6325', 'user_index_in_cluster': 18, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0021231422505307855, 'ndcg_500': 0.052830978731771845, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0021231422505307855, 'ndcg_1000': 0.052830978731771845, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0021231422505307855, 'ndcg_2000': 0.052830978731771845, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U597276', 'user_index_in_cluster': 19, 'num_candidates': 14640, 'num_history_articles': 15, 'original_history_len': 15, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 15:40:47,235 - INFO - Starting user U200033 in cluster 2 (index 20)
2025-04-03 15:40:47,506 - INFO - history_titles:[] empty for user: U200033!!!
2025-04-03 15:41:42,570 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:41:47,390 - INFO - Starting user U260964 in cluster 2 (index 21)
2025-04-03 15:41:47,462 - INFO - history_titles:[] empty for user: U260964!!!
2025-04-03 15:42:46,903 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:42:51,758 - INFO - Starting user U90382 in cluster 2 (index 22)
2025-04-03 15:43:46,261 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 15:43:51,239 - INFO - Starting user U640108 in cluster 2 (index 23)
2025-04-03 15:44:50,945 - INFO - lens: newsdf:101527candidate_pool:14633,candidate_pool:14633
2025-04-03 15:44:57,181 - INFO - Starting user U552980 in cluster 2 (index 24)
2025-04-03 15:45:52,267 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 15:45:57,618 - INFO - Starting user U428315 in cluster 2 (index 25)
2025-04-03 15:45:57,686 - INFO - history_titles:[] empty for user: U428315!!!
2025-04-03 15:46:56,998 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:47:01,884 - INFO - Starting user U449564 in cluster 2 (index 26)
2025-04-03 15:47:01,952 - INFO - history_titles:[] empty for user: U449564!!!
2025-04-03 15:47:56,719 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:48:01,464 - INFO - Starting user U163476 in cluster 2 (index 27)
2025-04-03 15:48:56,744 - INFO - lens: newsdf:101527candidate_pool:14638,candidate_pool:14638
2025-04-03 15:49:03,508 - INFO - Starting user U641845 in cluster 2 (index 28)
2025-04-03 15:50:02,460 - INFO - lens: newsdf:101527candidate_pool:14638,candidate_pool:14638
2025-04-03 15:50:07,223 - INFO - Starting user U3280 in cluster 2 (index 29)
2025-04-03 15:51:02,470 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:51:07,167 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U200033', 'user_index_in_cluster': 20, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0021231422505307855, 'ndcg_500': 0.052830978731771845, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0021231422505307855, 'ndcg_1000': 0.052830978731771845, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0021231422505307855, 'ndcg_2000': 0.052830978731771845, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U260964', 'user_index_in_cluster': 21, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U90382', 'user_index_in_cluster': 22, 'num_candidates': 14642, 'num_history_articles': 22, 'original_history_len': 22, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U640108', 'user_index_in_cluster': 23, 'num_candidates': 14633, 'num_history_articles': 19, 'original_history_len': 19, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U552980', 'user_index_in_cluster': 24, 'num_candidates': 14641, 'num_history_articles': 8, 'original_history_len': 8, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U428315', 'user_index_in_cluster': 25, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U449564', 'user_index_in_cluster': 26, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U163476', 'user_index_in_cluster': 27, 'num_candidates': 14638, 'num_history_articles': 9, 'original_history_len': 9, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U641845', 'user_index_in_cluster': 28, 'num_candidates': 14638, 'num_history_articles': 24, 'original_history_len': 24, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U3280', 'user_index_in_cluster': 29, 'num_candidates': 14643, 'num_history_articles': 7, 'original_history_len': 7, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 15:51:07,169 - INFO - Starting user U401352 in cluster 2 (index 30)
2025-04-03 15:52:06,370 - INFO - lens: newsdf:101527candidate_pool:14631,candidate_pool:14631
2025-04-03 15:52:11,215 - INFO - Starting user U115754 in cluster 2 (index 31)
2025-04-03 15:53:06,553 - INFO - lens: newsdf:101527candidate_pool:14629,candidate_pool:14629
2025-04-03 15:53:43,662 - INFO - Starting user U569010 in cluster 2 (index 32)
2025-04-03 15:53:43,754 - INFO - history_titles:[] empty for user: U569010!!!
2025-04-03 15:54:38,735 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:54:44,594 - INFO - Starting user U159295 in cluster 2 (index 33)
2025-04-03 15:55:43,885 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 15:55:48,935 - INFO - Starting user U171052 in cluster 2 (index 34)
2025-04-03 15:55:49,015 - INFO - history_titles:[] empty for user: U171052!!!
2025-04-03 15:56:44,275 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:56:49,063 - INFO - Starting user U73915 in cluster 2 (index 35)
2025-04-03 15:57:49,055 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 15:57:53,824 - INFO - Starting user U515908 in cluster 2 (index 36)
2025-04-03 15:58:48,783 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 15:58:53,627 - INFO - Starting user U520221 in cluster 2 (index 37)
2025-04-03 15:59:49,234 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 15:59:54,254 - INFO - Starting user U24161 in cluster 2 (index 38)
2025-04-03 16:00:53,729 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 16:00:58,827 - INFO - Starting user U208520 in cluster 2 (index 39)
2025-04-03 16:01:54,185 - INFO - lens: newsdf:101527candidate_pool:14638,candidate_pool:14638
2025-04-03 16:01:58,947 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U401352', 'user_index_in_cluster': 30, 'num_candidates': 14631, 'num_history_articles': 38, 'original_history_len': 38, 'num_future_clicks': 15, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.06666666666666667, 'ap_2000': 0.0005310674455655868, 'ndcg_2000': 0.015681594999025068, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U115754', 'user_index_in_cluster': 31, 'num_candidates': 14629, 'num_history_articles': 43, 'original_history_len': 43, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.000881057268722467, 'ndcg_2000': 0.03846204692547578, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U569010', 'user_index_in_cluster': 32, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.0021231422505307855, 'ndcg_500': 0.04394863643880725, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0021231422505307855, 'ndcg_1000': 0.04394863643880725, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0021231422505307855, 'ndcg_2000': 0.04394863643880725, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U159295', 'user_index_in_cluster': 33, 'num_candidates': 14640, 'num_history_articles': 15, 'original_history_len': 15, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U171052', 'user_index_in_cluster': 34, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0006934812760055479, 'ndcg_2000': 0.03719727697530082, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U73915', 'user_index_in_cluster': 35, 'num_candidates': 14640, 'num_history_articles': 24, 'original_history_len': 24, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U515908', 'user_index_in_cluster': 36, 'num_candidates': 14643, 'num_history_articles': 1, 'original_history_len': 1, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U520221', 'user_index_in_cluster': 37, 'num_candidates': 14642, 'num_history_articles': 4, 'original_history_len': 4, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U24161', 'user_index_in_cluster': 38, 'num_candidates': 14641, 'num_history_articles': 6, 'original_history_len': 6, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U208520', 'user_index_in_cluster': 39, 'num_candidates': 14638, 'num_history_articles': 21, 'original_history_len': 21, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 16:01:58,950 - INFO - Starting user U79744 in cluster 2 (index 40)
2025-04-03 16:01:59,019 - INFO - history_titles:[] empty for user: U79744!!!
2025-04-03 16:02:58,561 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:03:03,365 - INFO - Starting user U595088 in cluster 2 (index 41)
2025-04-03 16:03:58,774 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:04:03,922 - INFO - Starting user U219005 in cluster 2 (index 42)
2025-04-03 16:04:04,254 - INFO - history_titles:[] empty for user: U219005!!!
2025-04-03 16:04:58,989 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:05:03,679 - INFO - Starting user U146290 in cluster 2 (index 43)
2025-04-03 16:06:04,858 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:06:09,660 - INFO - Starting user U691255 in cluster 2 (index 44)
2025-04-03 16:07:04,909 - INFO - lens: newsdf:101527candidate_pool:14640,candidate_pool:14640
2025-04-03 16:07:09,636 - INFO - Starting user U624340 in cluster 2 (index 45)
2025-04-03 16:08:10,721 - INFO - lens: newsdf:101527candidate_pool:14637,candidate_pool:14637
2025-04-03 16:08:48,358 - INFO - Starting user U185733 in cluster 2 (index 46)
2025-04-03 16:09:45,012 - INFO - lens: newsdf:101527candidate_pool:14639,candidate_pool:14639
2025-04-03 16:10:24,909 - INFO - Starting user U621293 in cluster 2 (index 47)
2025-04-03 16:11:20,428 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:11:25,234 - INFO - Starting user U5224 in cluster 2 (index 48)
2025-04-03 16:12:24,780 - INFO - lens: newsdf:101527candidate_pool:14632,candidate_pool:14632
2025-04-03 16:12:30,488 - INFO - Starting user U71675 in cluster 2 (index 49)
2025-04-03 16:12:30,556 - INFO - history_titles:[] empty for user: U71675!!!
2025-04-03 16:13:25,316 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:13:31,976 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U79744', 'user_index_in_cluster': 40, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U595088', 'user_index_in_cluster': 41, 'num_candidates': 14643, 'num_history_articles': 5, 'original_history_len': 5, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U219005', 'user_index_in_cluster': 42, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U146290', 'user_index_in_cluster': 43, 'num_candidates': 14642, 'num_history_articles': 8, 'original_history_len': 8, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.010752688172043012, 'ndcg_100': 0.09354472212372622, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.010752688172043012, 'ndcg_200': 0.09354472212372622, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.010752688172043012, 'ndcg_500': 0.09354472212372622, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.010752688172043012, 'ndcg_1000': 0.09354472212372622, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.010752688172043012, 'ndcg_2000': 0.09354472212372622, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U691255', 'user_index_in_cluster': 44, 'num_candidates': 14640, 'num_history_articles': 9, 'original_history_len': 9, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U624340', 'user_index_in_cluster': 45, 'num_candidates': 14637, 'num_history_articles': 39, 'original_history_len': 39, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.018867924528301886, 'ndcg_100': 0.08154437872961684, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.018867924528301886, 'ndcg_200': 0.08154437872961684, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.018867924528301886, 'ndcg_500': 0.08154437872961684, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.018867924528301886, 'ndcg_1000': 0.08154437872961684, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.018867924528301886, 'ndcg_2000': 0.08154437872961684, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U185733', 'user_index_in_cluster': 46, 'num_candidates': 14639, 'num_history_articles': 15, 'original_history_len': 15, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U621293', 'user_index_in_cluster': 47, 'num_candidates': 14642, 'num_history_articles': 6, 'original_history_len': 6, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U5224', 'user_index_in_cluster': 48, 'num_candidates': 14632, 'num_history_articles': 41, 'original_history_len': 41, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U71675', 'user_index_in_cluster': 49, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 16:13:31,979 - INFO - Starting user U184159 in cluster 2 (index 50)
2025-04-03 16:14:32,683 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:14:37,509 - INFO - Starting user U604790 in cluster 2 (index 51)
2025-04-03 16:15:33,373 - INFO - lens: newsdf:101527candidate_pool:14629,candidate_pool:14629
2025-04-03 16:15:38,552 - INFO - Starting user U330859 in cluster 2 (index 52)
2025-04-03 16:16:34,945 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:16:39,851 - INFO - Starting user U49579 in cluster 2 (index 53)
2025-04-03 16:17:39,349 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:17:44,214 - INFO - Starting user U317352 in cluster 2 (index 54)
2025-04-03 16:17:44,293 - INFO - history_titles:[] empty for user: U317352!!!
2025-04-03 16:18:39,316 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:18:44,191 - INFO - Starting user U202591 in cluster 2 (index 55)
2025-04-03 16:19:43,075 - INFO - lens: newsdf:101527candidate_pool:14637,candidate_pool:14637
2025-04-03 16:19:47,796 - INFO - Starting user U562005 in cluster 2 (index 56)
2025-04-03 16:20:44,340 - INFO - lens: newsdf:101527candidate_pool:14639,candidate_pool:14639
2025-04-03 16:20:49,418 - INFO - Starting user U665033 in cluster 2 (index 57)
2025-04-03 16:20:49,503 - INFO - history_titles:[] empty for user: U665033!!!
2025-04-03 16:21:49,653 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:21:54,521 - INFO - Starting user U683198 in cluster 2 (index 58)
2025-04-03 16:22:50,054 - INFO - lens: newsdf:101527candidate_pool:14633,candidate_pool:14633
2025-04-03 16:22:55,564 - INFO - Starting user U158238 in cluster 2 (index 59)
2025-04-03 16:23:50,484 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:23:55,286 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U184159', 'user_index_in_cluster': 50, 'num_candidates': 14642, 'num_history_articles': 3, 'original_history_len': 3, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U604790', 'user_index_in_cluster': 51, 'num_candidates': 14629, 'num_history_articles': 43, 'original_history_len': 43, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U330859', 'user_index_in_cluster': 52, 'num_candidates': 14643, 'num_history_articles': 3, 'original_history_len': 3, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U49579', 'user_index_in_cluster': 53, 'num_candidates': 14643, 'num_history_articles': 9, 'original_history_len': 9, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.005076142131979695, 'ndcg_200': 0.05116814817184784, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.005076142131979695, 'ndcg_500': 0.05116814817184784, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.005076142131979695, 'ndcg_1000': 0.05116814817184784, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.005076142131979695, 'ndcg_2000': 0.05116814817184784, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U317352', 'user_index_in_cluster': 54, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.125, 'ap_500': 0.0021231422505307855, 'ndcg_500': 0.02847606296482097, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.125, 'ap_1000': 0.0021231422505307855, 'ndcg_1000': 0.02847606296482097, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.25, 'ap_2000': 0.001565349462796879, 'ndcg_2000': 0.05156394044380494, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U202591', 'user_index_in_cluster': 55, 'num_candidates': 14637, 'num_history_articles': 30, 'original_history_len': 30, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.0006071645415907711, 'ndcg_2000': 0.031737256215062674, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U562005', 'user_index_in_cluster': 56, 'num_candidates': 14639, 'num_history_articles': 10, 'original_history_len': 10, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U665033', 'user_index_in_cluster': 57, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U683198', 'user_index_in_cluster': 58, 'num_candidates': 14633, 'num_history_articles': 17, 'original_history_len': 17, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0005743825387708214, 'ndcg_2000': 0.05694938433460745, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U158238', 'user_index_in_cluster': 59, 'num_candidates': 14642, 'num_history_articles': 3, 'original_history_len': 3, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0005959475566150178, 'ndcg_2000': 0.04380302394284659, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 16:23:55,288 - INFO - Starting user U28155 in cluster 2 (index 60)
2025-04-03 16:23:55,357 - INFO - history_titles:[] empty for user: U28155!!!
2025-04-03 16:24:54,603 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:24:59,434 - INFO - Starting user U165938 in cluster 2 (index 61)
2025-04-03 16:25:57,386 - INFO - lens: newsdf:101527candidate_pool:14638,candidate_pool:14638
2025-04-03 16:26:02,866 - INFO - Starting user U285096 in cluster 2 (index 62)
2025-04-03 16:26:02,969 - INFO - history_titles:[] empty for user: U285096!!!
2025-04-03 16:27:05,735 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:27:10,737 - INFO - Starting user U383569 in cluster 2 (index 63)
2025-04-03 16:27:10,832 - INFO - history_titles:[] empty for user: U383569!!!
2025-04-03 16:28:11,160 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:28:16,132 - INFO - Starting user U649058 in cluster 2 (index 64)
2025-04-03 16:28:16,209 - INFO - history_titles:[] empty for user: U649058!!!
2025-04-03 16:29:18,121 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:29:23,179 - INFO - Starting user U415551 in cluster 2 (index 65)
2025-04-03 16:30:26,449 - INFO - lens: newsdf:101527candidate_pool:14630,candidate_pool:14630
2025-04-03 16:31:15,669 - INFO - Starting user U614277 in cluster 2 (index 66)
2025-04-03 16:32:17,665 - INFO - lens: newsdf:101527candidate_pool:14634,candidate_pool:14634
2025-04-03 16:32:54,818 - INFO - Starting user U343264 in cluster 2 (index 67)
2025-04-03 16:32:55,035 - INFO - history_titles:[] empty for user: U343264!!!
2025-04-03 16:34:17,113 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:34:24,134 - INFO - Starting user U26169 in cluster 2 (index 68)
2025-04-03 16:34:24,327 - INFO - history_titles:[] empty for user: U26169!!!
2025-04-03 16:35:27,850 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:35:32,792 - INFO - Starting user U369158 in cluster 2 (index 69)
2025-04-03 16:36:33,234 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:36:40,077 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U28155', 'user_index_in_cluster': 60, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U165938', 'user_index_in_cluster': 61, 'num_candidates': 14638, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0005078720162519045, 'ndcg_2000': 0.035670757169469, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U285096', 'user_index_in_cluster': 62, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U383569', 'user_index_in_cluster': 63, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U649058', 'user_index_in_cluster': 64, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.0021231422505307855, 'ndcg_500': 0.03406670872815872, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.0021231422505307855, 'ndcg_1000': 0.03406670872815872, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.16666666666666666, 'ap_2000': 0.0021231422505307855, 'ndcg_2000': 0.03406670872815872, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U415551', 'user_index_in_cluster': 65, 'num_candidates': 14630, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 10, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.1, 'ap_50': 0.045454545454545456, 'ndcg_50': 0.048654526772509234, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.1, 'ap_100': 0.045454545454545456, 'ndcg_100': 0.048654526772509234, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.1, 'ap_200': 0.045454545454545456, 'ndcg_200': 0.048654526772509234, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.1, 'ap_500': 0.045454545454545456, 'ndcg_500': 0.048654526772509234, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.2, 'ap_1000': 0.023916333369365474, 'ndcg_1000': 0.07130312677895301, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.2, 'ap_2000': 0.023916333369365474, 'ndcg_2000': 0.07130312677895301, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U614277', 'user_index_in_cluster': 66, 'num_candidates': 14634, 'num_history_articles': 29, 'original_history_len': 29, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U343264', 'user_index_in_cluster': 67, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U26169', 'user_index_in_cluster': 68, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U369158', 'user_index_in_cluster': 69, 'num_candidates': 14642, 'num_history_articles': 6, 'original_history_len': 6, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 16:36:40,083 - INFO - Starting user U415460 in cluster 2 (index 70)
2025-04-03 16:36:40,158 - INFO - history_titles:[] empty for user: U415460!!!
2025-04-03 16:37:45,461 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:37:50,348 - INFO - Starting user U559720 in cluster 2 (index 71)
2025-04-03 16:38:51,205 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:38:56,160 - INFO - Starting user U558663 in cluster 2 (index 72)
2025-04-03 16:38:56,239 - INFO - history_titles:[] empty for user: U558663!!!
2025-04-03 16:40:01,762 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:40:06,672 - INFO - Starting user U201390 in cluster 2 (index 73)
2025-04-03 16:41:05,989 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:41:11,258 - INFO - Starting user U158842 in cluster 2 (index 74)
2025-04-03 16:41:11,361 - INFO - history_titles:[] empty for user: U158842!!!
2025-04-03 16:42:11,234 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:42:16,178 - INFO - Starting user U503349 in cluster 2 (index 75)
2025-04-03 16:43:19,185 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 16:43:27,828 - INFO - Starting user U250016 in cluster 2 (index 76)
2025-04-03 16:44:28,814 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 16:44:33,759 - INFO - Starting user U43685 in cluster 2 (index 77)
2025-04-03 16:45:38,360 - INFO - lens: newsdf:101527candidate_pool:14635,candidate_pool:14635
2025-04-03 16:46:17,981 - INFO - Starting user U655647 in cluster 2 (index 78)
2025-04-03 16:47:18,892 - INFO - lens: newsdf:101527candidate_pool:14636,candidate_pool:14636
2025-04-03 16:47:25,617 - INFO - Starting user U292984 in cluster 2 (index 79)
2025-04-03 16:47:25,704 - INFO - history_titles:[] empty for user: U292984!!!
2025-04-03 16:48:25,780 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:48:30,881 - INFO - Writing partial rows: [{'cluster_id': 2, 'user_id': 'U415460', 'user_index_in_cluster': 70, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U559720', 'user_index_in_cluster': 71, 'num_candidates': 14642, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 14, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.07142857142857142, 'ap_100': 0.012658227848101266, 'ndcg_100': 0.02818923206968657, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.07142857142857142, 'ap_200': 0.012658227848101266, 'ndcg_200': 0.02818923206968657, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.07142857142857142, 'ap_500': 0.012658227848101266, 'ndcg_500': 0.02818923206968657, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.07142857142857142, 'ap_1000': 0.012658227848101266, 'ndcg_1000': 0.02818923206968657, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.14285714285714285, 'ap_2000': 0.006953723543038765, 'ndcg_2000': 0.04492941768865435, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U558663', 'user_index_in_cluster': 72, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 10, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U201390', 'user_index_in_cluster': 73, 'num_candidates': 14643, 'num_history_articles': 2, 'original_history_len': 2, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U158842', 'user_index_in_cluster': 74, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U503349', 'user_index_in_cluster': 75, 'num_candidates': 14641, 'num_history_articles': 11, 'original_history_len': 11, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U250016', 'user_index_in_cluster': 76, 'num_candidates': 14641, 'num_history_articles': 3, 'original_history_len': 3, 'num_future_clicks': 15, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.06666666666666667, 'ap_2000': 0.0006150061500615006, 'ndcg_2000': 0.015992617331930414, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U43685', 'user_index_in_cluster': 77, 'num_candidates': 14635, 'num_history_articles': 26, 'original_history_len': 26, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U655647', 'user_index_in_cluster': 78, 'num_candidates': 14636, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 2, 'user_id': 'U292984', 'user_index_in_cluster': 79, 'num_candidates': 14643, 'num_history_articles': 0, 'original_history_len': 0, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.125, 'ap_200': 0.008130081300813009, 'ndcg_200': 0.03637267219360183, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.25, 'ap_500': 0.0061881829009372905, 'ndcg_500': 0.06484873515842281, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.25, 'ap_1000': 0.0061881829009372905, 'ndcg_1000': 0.06484873515842281, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.25, 'ap_2000': 0.0061881829009372905, 'ndcg_2000': 0.06484873515842281, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 16:48:30,885 - INFO - Starting user U626729 in cluster 2 (index 80)
2025-04-03 16:49:45,204 - INFO - lens: newsdf:101527candidate_pool:14639,candidate_pool:14639
2025-04-03 16:49:51,159 - INFO - Starting user U353674 in cluster 2 (index 81)
2025-04-03 16:50:51,636 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 16:50:56,947 - INFO - Starting user U110904 in cluster 2 (index 82)
2025-04-03 16:51:59,089 - INFO - lens: newsdf:101527candidate_pool:14641,candidate_pool:14641
2025-04-03 16:52:03,915 - INFO - Starting user U344648 in cluster 2 (index 83)
2025-04-03 16:52:03,986 - INFO - history_titles:[] empty for user: U344648!!!
2025-04-03 16:53:02,383 - INFO - lens: newsdf:101527candidate_pool:14643,candidate_pool:14643
2025-04-03 16:53:07,314 - INFO - Starting user U642982 in cluster 2 (index 84)
2025-04-03 16:54:08,882 - INFO - lens: newsdf:101527candidate_pool:14632,candidate_pool:14632
2025-04-03 16:54:16,322 - INFO - Starting user U479071 in cluster 2 (index 85)
2025-04-03 16:55:13,990 - INFO - lens: newsdf:101527candidate_pool:14642,candidate_pool:14642
2025-04-03 16:55:19,377 - INFO - Starting user U36940 in cluster 2 (index 86)
2025-04-03 16:56:19,162 - INFO - lens: newsdf:101527candidate_pool:14637,candidate_pool:14637
2025-04-03 17:28:00,351 - INFO - Starting user U332817 in cluster 0 (index 0)
2025-04-03 17:28:13,008 - INFO - Starting user U239687 in cluster 1 (index 0)
2025-04-03 17:28:23,040 - INFO - Starting user U87243 in cluster 2 (index 0)
2025-04-03 17:28:29,820 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:28:40,660 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:28:47,568 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:29:58,746 - ERROR - Failed on user U87243 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:29:58,746 - INFO - Starting user U598644 in cluster 2 (index 1)
2025-04-03 17:30:19,694 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:30:44,680 - ERROR - Failed on user U598644 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:30:44,680 - INFO - Starting user U532401 in cluster 2 (index 2)
2025-04-03 17:31:14,503 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:31:31,585 - ERROR - Failed on user U532401 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:31:31,585 - INFO - Starting user U593596 in cluster 2 (index 3)
2025-04-03 17:31:50,154 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:32:13,275 - ERROR - Failed on user U593596 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:32:13,276 - INFO - Starting user U521853 in cluster 2 (index 4)
2025-04-03 17:32:33,066 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:32:52,053 - ERROR - Failed on user U521853 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:32:52,053 - INFO - Starting user U687515 in cluster 2 (index 5)
2025-04-03 17:32:52,497 - INFO - Starting user U552512 in cluster 0 (index 1)
2025-04-03 17:32:56,899 - INFO - Starting user U394751 in cluster 1 (index 1)
2025-04-03 17:33:10,589 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:33:25,858 - ERROR - Failed on user U687515 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:33:25,859 - INFO - Starting user U192112 in cluster 2 (index 6)
2025-04-03 17:33:29,259 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:33:31,928 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:33:43,152 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:33:59,238 - ERROR - Failed on user U192112 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:33:59,238 - INFO - Starting user U530668 in cluster 2 (index 7)
2025-04-03 17:34:17,684 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:34:37,217 - ERROR - Failed on user U530668 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:34:37,218 - INFO - Starting user U290933 in cluster 2 (index 8)
2025-04-03 17:35:07,915 - INFO - Starting user U157928 in cluster 0 (index 2)
2025-04-03 17:35:09,437 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:35:20,543 - INFO - Starting user U318219 in cluster 1 (index 2)
2025-04-03 17:35:27,535 - ERROR - Failed on user U290933 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:35:27,536 - INFO - Starting user U108656 in cluster 2 (index 9)
2025-04-03 17:35:28,461 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:35:39,741 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:35:45,216 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:35:47,789 - INFO - Starting user U450596 in cluster 0 (index 3)
2025-04-03 17:35:57,972 - INFO - Starting user U572245 in cluster 1 (index 3)
2025-04-03 17:36:03,852 - ERROR - Failed on user U108656 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:36:03,852 - INFO - Starting user U178651 in cluster 2 (index 10)
2025-04-03 17:36:05,777 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:36:15,882 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:36:16,045 - INFO - Starting user U555104 in cluster 0 (index 4)
2025-04-03 17:36:21,619 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:36:34,968 - ERROR - Failed on user U178651 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:36:34,968 - INFO - Starting user U86679 in cluster 2 (index 11)
2025-04-03 17:36:40,084 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:36:55,286 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:37:07,270 - ERROR - Failed on user U86679 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:37:07,271 - INFO - Starting user U95671 in cluster 2 (index 12)
2025-04-03 17:37:09,460 - INFO - Starting user U255539 in cluster 1 (index 4)
2025-04-03 17:37:25,337 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:37:36,656 - INFO - Starting user U80394 in cluster 0 (index 5)
2025-04-03 17:37:37,714 - ERROR - Failed on user U95671 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:37:37,714 - INFO - Starting user U656379 in cluster 2 (index 13)
2025-04-03 17:37:37,833 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:37:55,660 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:37:56,116 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:38:17,782 - ERROR - Failed on user U656379 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:38:17,782 - INFO - Starting user U52262 in cluster 2 (index 14)
2025-04-03 17:38:22,876 - INFO - Starting user U531786 in cluster 0 (index 6)
2025-04-03 17:38:35,209 - INFO - Starting user U197392 in cluster 1 (index 5)
2025-04-03 17:38:37,488 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:38:41,573 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 17:38:49,696 - ERROR - Failed on user U52262 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:38:49,696 - INFO - Starting user U629430 in cluster 2 (index 15)
2025-04-03 17:38:53,964 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:39:08,412 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:39:27,734 - ERROR - Failed on user U629430 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:39:27,734 - INFO - Starting user U419371 in cluster 2 (index 16)
2025-04-03 17:39:45,389 - INFO - Starting user U188079 in cluster 0 (index 7)
2025-04-03 17:39:45,426 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:40:01,690 - ERROR - Failed on user U419371 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:40:01,690 - INFO - Starting user U594274 in cluster 2 (index 17)
2025-04-03 17:40:02,433 - INFO - Starting user U347805 in cluster 1 (index 6)
2025-04-03 17:40:11,439 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:40:20,068 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:40:20,718 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:40:32,479 - ERROR - Failed on user U594274 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:40:32,479 - INFO - Starting user U6325 in cluster 2 (index 18)
2025-04-03 17:40:48,872 - INFO - Starting user U50451 in cluster 0 (index 8)
2025-04-03 17:40:50,166 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:40:52,839 - INFO - Starting user U313837 in cluster 1 (index 7)
2025-04-03 17:41:04,234 - ERROR - Failed on user U6325 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:41:04,234 - INFO - Starting user U597276 in cluster 2 (index 19)
2025-04-03 17:41:06,909 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 17:41:22,566 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:41:22,824 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:41:34,708 - ERROR - Failed on user U597276 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:41:34,708 - INFO - Starting user U200033 in cluster 2 (index 20)
2025-04-03 17:41:52,897 - INFO - Starting user U378487 in cluster 1 (index 8)
2025-04-03 17:41:54,360 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:42:06,697 - ERROR - Failed on user U200033 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:42:06,698 - INFO - Starting user U260964 in cluster 2 (index 21)
2025-04-03 17:42:10,619 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:42:12,540 - INFO - Starting user U707428 in cluster 0 (index 9)
2025-04-03 17:42:19,733 - INFO - Starting user U130490 in cluster 1 (index 9)
2025-04-03 17:42:24,333 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:42:31,192 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:42:36,627 - ERROR - Failed on user U260964 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:42:36,628 - INFO - Starting user U90382 in cluster 2 (index 22)
2025-04-03 17:42:37,585 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:42:54,663 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:43:05,348 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U332817', 'user_index_in_cluster': 0, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 78, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0012224938875305623, 'ndcg_1000': 0.04849063037573001, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0013653947567366237, 'ndcg_2000': 0.09372688136052981, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U552512', 'user_index_in_cluster': 1, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 130, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U157928', 'user_index_in_cluster': 2, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 123, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U450596', 'user_index_in_cluster': 3, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 147, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U555104', 'user_index_in_cluster': 4, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 58, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U80394', 'user_index_in_cluster': 5, 'num_candidates': 4527, 'num_history_articles': 46, 'original_history_len': 46, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U531786', 'user_index_in_cluster': 6, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 110, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U188079', 'user_index_in_cluster': 7, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 93, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U50451', 'user_index_in_cluster': 8, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 71, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U707428', 'user_index_in_cluster': 9, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 17:43:05,426 - INFO - Starting user U368341 in cluster 0 (index 10)
2025-04-03 17:43:10,850 - ERROR - Failed on user U90382 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:43:10,851 - INFO - Starting user U640108 in cluster 2 (index 23)
2025-04-03 17:43:12,407 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U239687', 'user_index_in_cluster': 0, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 336, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.0021604386435425233, 'ndcg_1000': 0.08286037006329458, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0021604386435425233, 'ndcg_2000': 0.08286037006329458, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U394751', 'user_index_in_cluster': 1, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 164, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.001392757660167131, 'ndcg_1000': 0.02896530419245689, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.14285714285714285, 'ap_2000': 0.001392757660167131, 'ndcg_2000': 0.02896530419245689, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U318219', 'user_index_in_cluster': 2, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 235, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U572245', 'user_index_in_cluster': 3, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 433, 'num_future_clicks': 14, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.07142857142857142, 'ap_1000': 0.0014144271570014145, 'ndcg_1000': 0.01882316465889193, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.14285714285714285, 'ap_2000': 0.0015590023348891228, 'ndcg_2000': 0.03629742641843487, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U255539', 'user_index_in_cluster': 4, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 270, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.16666666666666666, 'ap_10': 0.125, 'ndcg_10': 0.09546043308946733, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.16666666666666666, 'ap_20': 0.125, 'ndcg_20': 0.09546043308946733, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.16666666666666666, 'ap_50': 0.125, 'ndcg_50': 0.09546043308946733, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.16666666666666666, 'ap_100': 0.125, 'ndcg_100': 0.09546043308946733, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.16666666666666666, 'ap_200': 0.125, 'ndcg_200': 0.09546043308946733, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.125, 'ndcg_500': 0.09546043308946733, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.125, 'ndcg_1000': 0.09546043308946733, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.5, 'ap_2000': 0.04299727779324486, 'ndcg_2000': 0.15445748411131122, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U197392', 'user_index_in_cluster': 5, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U347805', 'user_index_in_cluster': 6, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 238, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U313837', 'user_index_in_cluster': 7, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 278, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 1.0, 'ap_100': 0.014084507042253521, 'ndcg_100': 0.16207652439312228, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.014084507042253521, 'ndcg_200': 0.16207652439312228, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.014084507042253521, 'ndcg_500': 0.16207652439312228, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.014084507042253521, 'ndcg_1000': 0.16207652439312228, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.014084507042253521, 'ndcg_2000': 0.16207652439312228, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U378487', 'user_index_in_cluster': 8, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 160, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0005966587112171838, 'ndcg_2000': 0.04381005563194822, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U130490', 'user_index_in_cluster': 9, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 274, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 17:43:12,432 - INFO - Starting user U182803 in cluster 1 (index 10)
2025-04-03 17:43:28,591 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:43:30,072 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:43:31,658 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:43:42,580 - ERROR - Failed on user U640108 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:43:42,580 - INFO - Starting user U552980 in cluster 2 (index 24)
2025-04-03 17:44:01,307 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:44:07,699 - INFO - Starting user U27745 in cluster 0 (index 11)
2025-04-03 17:44:12,706 - INFO - Starting user U645755 in cluster 1 (index 11)
2025-04-03 17:44:18,565 - ERROR - Failed on user U552980 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:44:18,565 - INFO - Starting user U428315 in cluster 2 (index 25)
2025-04-03 17:44:25,889 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:44:31,114 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:44:36,524 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:44:53,101 - ERROR - Failed on user U428315 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:44:53,101 - INFO - Starting user U449564 in cluster 2 (index 26)
2025-04-03 17:45:03,565 - INFO - Starting user U244721 in cluster 1 (index 12)
2025-04-03 17:45:07,612 - INFO - Starting user U158594 in cluster 0 (index 12)
2025-04-03 17:45:12,250 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:45:21,371 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:45:25,478 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:45:25,650 - ERROR - Failed on user U449564 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:45:25,650 - INFO - Starting user U163476 in cluster 2 (index 27)
2025-04-03 17:45:39,403 - INFO - Starting user U208686 in cluster 1 (index 13)
2025-04-03 17:45:44,385 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:45:56,648 - ERROR - Failed on user U163476 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:45:56,649 - INFO - Starting user U641845 in cluster 2 (index 28)
2025-04-03 17:45:57,817 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:46:14,538 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:46:31,424 - INFO - Starting user U119080 in cluster 1 (index 14)
2025-04-03 17:46:31,629 - INFO - Starting user U232750 in cluster 0 (index 13)
2025-04-03 17:46:33,653 - ERROR - Failed on user U641845 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:46:33,653 - INFO - Starting user U3280 in cluster 2 (index 29)
2025-04-03 17:46:48,924 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:46:51,088 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:46:53,488 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:47:08,557 - ERROR - Failed on user U3280 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:47:08,557 - INFO - Starting user U401352 in cluster 2 (index 30)
2025-04-03 17:47:26,619 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:47:28,640 - INFO - Starting user U42193 in cluster 0 (index 14)
2025-04-03 17:47:31,917 - INFO - Starting user U261440 in cluster 1 (index 15)
2025-04-03 17:47:43,546 - ERROR - Failed on user U401352 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:47:43,547 - INFO - Starting user U115754 in cluster 2 (index 31)
2025-04-03 17:47:46,617 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 17:47:49,329 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:48:01,628 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:48:18,469 - ERROR - Failed on user U115754 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:48:18,469 - INFO - Starting user U569010 in cluster 2 (index 32)
2025-04-03 17:48:22,953 - INFO - Starting user U56431 in cluster 0 (index 15)
2025-04-03 17:48:26,767 - INFO - Starting user U252465 in cluster 1 (index 16)
2025-04-03 17:48:36,656 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:48:41,494 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:48:44,418 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 17:48:49,398 - ERROR - Failed on user U569010 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:48:49,398 - INFO - Starting user U159295 in cluster 2 (index 33)
2025-04-03 17:49:07,482 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:49:26,964 - ERROR - Failed on user U159295 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:49:26,964 - INFO - Starting user U171052 in cluster 2 (index 34)
2025-04-03 17:49:45,149 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:49:50,813 - INFO - Starting user U109846 in cluster 0 (index 16)
2025-04-03 17:50:02,543 - ERROR - Failed on user U171052 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:50:02,544 - INFO - Starting user U73915 in cluster 2 (index 35)
2025-04-03 17:50:08,383 - INFO - Starting user U388093 in cluster 1 (index 17)
2025-04-03 17:50:10,160 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:50:20,439 - INFO - Starting user U564802 in cluster 0 (index 17)
2025-04-03 17:50:20,488 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:50:29,039 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 17:50:32,239 - ERROR - Failed on user U73915 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:50:32,240 - INFO - Starting user U515908 in cluster 2 (index 36)
2025-04-03 17:50:41,319 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 17:50:50,490 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:51:09,555 - ERROR - Failed on user U515908 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:51:09,556 - INFO - Starting user U520221 in cluster 2 (index 37)
2025-04-03 17:51:10,190 - INFO - Starting user U30649 in cluster 0 (index 18)
2025-04-03 17:51:28,456 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:51:28,929 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:51:40,416 - INFO - Starting user U434834 in cluster 1 (index 18)
2025-04-03 17:51:45,666 - ERROR - Failed on user U520221 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:51:45,667 - INFO - Starting user U24161 in cluster 2 (index 38)
2025-04-03 17:51:46,635 - INFO - Starting user U329564 in cluster 0 (index 19)
2025-04-03 17:51:58,419 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:52:03,454 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:52:05,205 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:52:17,421 - ERROR - Failed on user U24161 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:52:17,422 - INFO - Starting user U208520 in cluster 2 (index 39)
2025-04-03 17:52:32,909 - INFO - Starting user U703466 in cluster 1 (index 19)
2025-04-03 17:52:35,151 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:52:36,423 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U368341', 'user_index_in_cluster': 10, 'num_candidates': 4525, 'num_history_articles': 45, 'original_history_len': 45, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 1.0, 'ap_50': 0.02564102564102564, 'ndcg_50': 0.18790182470910757, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 1.0, 'ap_100': 0.02564102564102564, 'ndcg_100': 0.18790182470910757, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.02564102564102564, 'ndcg_200': 0.18790182470910757, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.02564102564102564, 'ndcg_500': 0.18790182470910757, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.02564102564102564, 'ndcg_1000': 0.18790182470910757, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.02564102564102564, 'ndcg_2000': 0.18790182470910757, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U27745', 'user_index_in_cluster': 11, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U158594', 'user_index_in_cluster': 12, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U232750', 'user_index_in_cluster': 13, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 96, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0022988505747126436, 'ndcg_500': 0.05352062737740124, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0022988505747126436, 'ndcg_1000': 0.05352062737740124, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0022988505747126436, 'ndcg_2000': 0.05352062737740124, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U42193', 'user_index_in_cluster': 14, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 54, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U56431', 'user_index_in_cluster': 15, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 78, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U109846', 'user_index_in_cluster': 16, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 73, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.003404125333940812, 'ndcg_500': 0.07723160896839697, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.003404125333940812, 'ndcg_1000': 0.07723160896839697, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.002972651769743945, 'ndcg_2000': 0.1096105371884383, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U564802', 'user_index_in_cluster': 17, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 71, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U30649', 'user_index_in_cluster': 18, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 61, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U329564', 'user_index_in_cluster': 19, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 105, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 17:52:36,434 - INFO - Starting user U265386 in cluster 0 (index 20)
2025-04-03 17:52:48,120 - ERROR - Failed on user U208520 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:52:48,120 - INFO - Starting user U79744 in cluster 2 (index 40)
2025-04-03 17:52:50,869 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:52:55,653 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:53:06,281 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:53:23,607 - ERROR - Failed on user U79744 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:53:23,608 - INFO - Starting user U595088 in cluster 2 (index 41)
2025-04-03 17:53:25,285 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U182803', 'user_index_in_cluster': 10, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 101, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U645755', 'user_index_in_cluster': 11, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 545, 'num_future_clicks': 33, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.030303030303030304, 'ap_20': 0.09090909090909091, 'ndcg_20': 0.039621067055815605, 'num_recommendations_20': 20, 'precision_50': 0.04, 'recall_50': 0.06060606060606061, 'ap_50': 0.07993730407523511, 'ndcg_50': 0.04947919896527423, 'num_recommendations_50': 50, 'precision_100': 0.02, 'recall_100': 0.06060606060606061, 'ap_100': 0.07993730407523511, 'ndcg_100': 0.04947919896527423, 'num_recommendations_100': 100, 'precision_200': 0.01, 'recall_200': 0.06060606060606061, 'ap_200': 0.07993730407523511, 'ndcg_200': 0.04947919896527423, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.09090909090909091, 'ap_500': 0.05671619358440331, 'ndcg_500': 0.06198683133025634, 'num_recommendations_500': 500, 'precision_1000': 0.008, 'recall_1000': 0.24242424242424243, 'ap_1000': 0.02622587132068411, 'ndcg_1000': 0.11569398813110927, 'num_recommendations_1000': 1000, 'precision_2000': 0.006, 'recall_2000': 0.36363636363636365, 'ap_2000': 0.020090312054146953, 'ndcg_2000': 0.15518310619964987, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U244721', 'user_index_in_cluster': 12, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 111, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U208686', 'user_index_in_cluster': 13, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 146, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0009755825864165137, 'ndcg_2000': 0.0888233648003072, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U119080', 'user_index_in_cluster': 14, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 153, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U261440', 'user_index_in_cluster': 15, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 170, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.14285714285714285, 'ap_50': 0.02564102564102564, 'ndcg_50': 0.051649764523943376, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.14285714285714285, 'ap_100': 0.02564102564102564, 'ndcg_100': 0.051649764523943376, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.14285714285714285, 'ap_200': 0.02564102564102564, 'ndcg_200': 0.051649764523943376, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.14285714285714285, 'ap_500': 0.02564102564102564, 'ndcg_500': 0.051649764523943376, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.02564102564102564, 'ndcg_1000': 0.051649764523943376, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.42857142857142855, 'ap_2000': 0.009650248051089712, 'ndcg_2000': 0.10389835401565135, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U252465', 'user_index_in_cluster': 16, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 140, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U388093', 'user_index_in_cluster': 17, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 112, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U434834', 'user_index_in_cluster': 18, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U703466', 'user_index_in_cluster': 19, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 214, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.003367003367003367, 'ndcg_500': 0.04126453964646842, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.003367003367003367, 'ndcg_1000': 0.04126453964646842, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.0026286812676226665, 'ndcg_2000': 0.07501690550280808, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 17:53:25,298 - INFO - Starting user U121891 in cluster 1 (index 20)
2025-04-03 17:53:30,749 - INFO - Starting user U431712 in cluster 0 (index 21)
2025-04-03 17:53:41,743 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:53:47,396 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 17:53:48,736 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:53:54,937 - ERROR - Failed on user U595088 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:53:54,937 - INFO - Starting user U219005 in cluster 2 (index 42)
2025-04-03 17:54:12,671 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:54:23,422 - INFO - Starting user U258580 in cluster 1 (index 21)
2025-04-03 17:54:26,458 - INFO - Starting user U325897 in cluster 0 (index 22)
2025-04-03 17:54:28,496 - ERROR - Failed on user U219005 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:54:28,496 - INFO - Starting user U146290 in cluster 2 (index 43)
2025-04-03 17:54:41,061 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:54:44,269 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:54:45,864 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:55:02,468 - ERROR - Failed on user U146290 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:55:02,468 - INFO - Starting user U691255 in cluster 2 (index 44)
2025-04-03 17:55:20,491 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:55:22,246 - INFO - Starting user U524323 in cluster 1 (index 22)
2025-04-03 17:55:26,890 - INFO - Starting user U126505 in cluster 0 (index 23)
2025-04-03 17:55:38,372 - ERROR - Failed on user U691255 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:55:38,373 - INFO - Starting user U624340 in cluster 2 (index 45)
2025-04-03 17:55:40,048 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 17:55:50,332 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:55:56,561 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:56:15,397 - ERROR - Failed on user U624340 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:56:15,397 - INFO - Starting user U185733 in cluster 2 (index 46)
2025-04-03 17:56:15,956 - INFO - Starting user U73954 in cluster 0 (index 24)
2025-04-03 17:56:33,115 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:56:34,135 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 17:56:45,385 - INFO - Starting user U380008 in cluster 1 (index 23)
2025-04-03 17:56:45,431 - ERROR - Failed on user U185733 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:56:45,431 - INFO - Starting user U621293 in cluster 2 (index 47)
2025-04-03 17:56:52,823 - INFO - Starting user U236341 in cluster 0 (index 25)
2025-04-03 17:57:03,531 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:57:04,249 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:57:10,806 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 17:57:17,308 - ERROR - Failed on user U621293 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:57:17,308 - INFO - Starting user U5224 in cluster 2 (index 48)
2025-04-03 17:57:35,256 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 17:57:38,494 - INFO - Starting user U36229 in cluster 1 (index 24)
2025-04-03 17:57:42,472 - INFO - Starting user U441895 in cluster 0 (index 26)
2025-04-03 17:57:54,011 - ERROR - Failed on user U5224 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:57:54,011 - INFO - Starting user U71675 in cluster 2 (index 49)
2025-04-03 17:57:56,390 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:58:01,520 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:58:11,881 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:58:28,342 - ERROR - Failed on user U71675 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:58:28,343 - INFO - Starting user U184159 in cluster 2 (index 50)
2025-04-03 17:58:30,568 - INFO - Starting user U567260 in cluster 1 (index 25)
2025-04-03 17:58:35,695 - INFO - Starting user U179667 in cluster 0 (index 27)
2025-04-03 17:58:46,456 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 17:58:48,879 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 17:58:53,732 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 17:58:59,778 - ERROR - Failed on user U184159 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:58:59,779 - INFO - Starting user U604790 in cluster 2 (index 51)
2025-04-03 17:59:18,283 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 17:59:33,034 - INFO - Starting user U228133 in cluster 0 (index 28)
2025-04-03 17:59:33,738 - ERROR - Failed on user U604790 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 17:59:33,738 - INFO - Starting user U330859 in cluster 2 (index 52)
2025-04-03 17:59:51,013 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:59:51,044 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 17:59:52,984 - INFO - Starting user U380256 in cluster 1 (index 26)
2025-04-03 18:00:03,163 - INFO - Starting user U42778 in cluster 0 (index 29)
2025-04-03 18:00:03,947 - ERROR - Failed on user U330859 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:00:03,947 - INFO - Starting user U49579 in cluster 2 (index 53)
2025-04-03 18:00:12,655 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:00:22,559 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:00:23,788 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 18:00:34,573 - ERROR - Failed on user U49579 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:00:34,573 - INFO - Starting user U317352 in cluster 2 (index 54)
2025-04-03 18:00:47,771 - INFO - Starting user U403281 in cluster 1 (index 27)
2025-04-03 18:00:48,508 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U265386', 'user_index_in_cluster': 20, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 59, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U431712', 'user_index_in_cluster': 21, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 126, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U325897', 'user_index_in_cluster': 22, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 51, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U126505', 'user_index_in_cluster': 23, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 78, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.0017702596380802518, 'ndcg_1000': 0.07030314267937744, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.0017422866389899822, 'ndcg_2000': 0.10171366656564076, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U73954', 'user_index_in_cluster': 24, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 81, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U236341', 'user_index_in_cluster': 25, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 132, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006788866259334691, 'ndcg_2000': 0.058253382790867676, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U441895', 'user_index_in_cluster': 26, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U179667', 'user_index_in_cluster': 27, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 95, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U228133', 'user_index_in_cluster': 28, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 58, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U42778', 'user_index_in_cluster': 29, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 73, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:00:48,514 - INFO - Starting user U221935 in cluster 0 (index 30)
2025-04-03 18:00:52,608 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:01:05,323 - ERROR - Failed on user U317352 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:01:05,323 - INFO - Starting user U202591 in cluster 2 (index 55)
2025-04-03 18:01:05,740 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:01:06,522 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:01:23,019 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:01:24,269 - INFO - Starting user U435786 in cluster 1 (index 28)
2025-04-03 18:01:35,491 - ERROR - Failed on user U202591 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:01:35,491 - INFO - Starting user U562005 in cluster 2 (index 56)
2025-04-03 18:01:41,745 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:01:49,884 - INFO - Starting user U465803 in cluster 1 (index 29)
2025-04-03 18:01:54,083 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:02:06,601 - ERROR - Failed on user U562005 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:02:06,602 - INFO - Starting user U665033 in cluster 2 (index 57)
2025-04-03 18:02:09,819 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:02:10,335 - INFO - Starting user U399329 in cluster 0 (index 31)
2025-04-03 18:02:19,240 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U121891', 'user_index_in_cluster': 20, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 136, 'num_future_clicks': 14, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.21428571428571427, 'ap_500': 0.004681769749231836, 'ndcg_500': 0.061343150055749945, 'num_recommendations_500': 500, 'precision_1000': 0.005, 'recall_1000': 0.35714285714285715, 'ap_1000': 0.0059261484393261345, 'ndcg_1000': 0.10023542266561951, 'num_recommendations_1000': 1000, 'precision_2000': 0.0035, 'recall_2000': 0.5, 'ap_2000': 0.005504374449568784, 'ndcg_2000': 0.13415561861602188, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U258580', 'user_index_in_cluster': 21, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 212, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U524323', 'user_index_in_cluster': 22, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 191, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U380008', 'user_index_in_cluster': 23, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 157, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U36229', 'user_index_in_cluster': 24, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 195, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U567260', 'user_index_in_cluster': 25, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 104, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0009541984732824427, 'ndcg_2000': 0.046765134784746115, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U380256', 'user_index_in_cluster': 26, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U403281', 'user_index_in_cluster': 27, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 146, 'num_future_clicks': 9, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.2222222222222222, 'ap_1000': 0.0018667854021339402, 'ndcg_1000': 0.048946660940067444, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.2222222222222222, 'ap_2000': 0.0018667854021339402, 'ndcg_2000': 0.048946660940067444, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U435786', 'user_index_in_cluster': 28, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 100, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U465803', 'user_index_in_cluster': 29, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 215, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:02:19,245 - INFO - Starting user U451936 in cluster 1 (index 30)
2025-04-03 18:02:24,219 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:02:30,125 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:02:36,872 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:02:37,758 - ERROR - Failed on user U665033 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:02:37,759 - INFO - Starting user U683198 in cluster 2 (index 58)
2025-04-03 18:02:55,611 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:03:02,755 - INFO - Starting user U669817 in cluster 0 (index 32)
2025-04-03 18:03:09,488 - INFO - Starting user U246941 in cluster 1 (index 31)
2025-04-03 18:03:14,099 - ERROR - Failed on user U683198 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:03:14,099 - INFO - Starting user U158238 in cluster 2 (index 59)
2025-04-03 18:03:23,938 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:03:27,435 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:03:32,825 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:03:48,643 - ERROR - Failed on user U158238 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:03:48,644 - INFO - Starting user U28155 in cluster 2 (index 60)
2025-04-03 18:03:59,552 - INFO - Starting user U474980 in cluster 0 (index 33)
2025-04-03 18:04:03,305 - INFO - Starting user U503286 in cluster 1 (index 32)
2025-04-03 18:04:06,442 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:04:17,671 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:04:18,521 - ERROR - Failed on user U28155 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:04:18,521 - INFO - Starting user U165938 in cluster 2 (index 61)
2025-04-03 18:04:22,970 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:04:36,727 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:04:52,324 - INFO - Starting user U242727 in cluster 0 (index 34)
2025-04-03 18:04:54,115 - ERROR - Failed on user U165938 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:04:54,116 - INFO - Starting user U285096 in cluster 2 (index 62)
2025-04-03 18:04:54,225 - INFO - history_titles:[] empty for user: U285096!!!
2025-04-03 18:04:57,096 - INFO - Starting user U121505 in cluster 1 (index 33)
2025-04-03 18:05:10,344 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:05:12,182 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:05:14,730 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:05:29,536 - ERROR - Failed on user U285096 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:05:29,537 - INFO - Starting user U383569 in cluster 2 (index 63)
2025-04-03 18:05:45,511 - INFO - Starting user U29060 in cluster 0 (index 35)
2025-04-03 18:05:47,839 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:05:49,988 - INFO - Starting user U141206 in cluster 1 (index 34)
2025-04-03 18:06:01,310 - ERROR - Failed on user U383569 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:06:01,310 - INFO - Starting user U649058 in cluster 2 (index 64)
2025-04-03 18:06:06,436 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:06:07,660 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:06:18,762 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:06:30,953 - ERROR - Failed on user U649058 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:06:30,954 - INFO - Starting user U415551 in cluster 2 (index 65)
2025-04-03 18:06:34,059 - INFO - Starting user U301043 in cluster 1 (index 35)
2025-04-03 18:06:49,840 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:06:53,805 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:07:02,275 - ERROR - Failed on user U415551 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:07:02,275 - INFO - Starting user U614277 in cluster 2 (index 66)
2025-04-03 18:07:05,719 - INFO - Starting user U381081 in cluster 0 (index 36)
2025-04-03 18:07:11,035 - INFO - Starting user U308427 in cluster 1 (index 36)
2025-04-03 18:07:20,297 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:07:24,943 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:07:28,937 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:07:33,918 - ERROR - Failed on user U614277 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:07:33,918 - INFO - Starting user U343264 in cluster 2 (index 67)
2025-04-03 18:07:52,194 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:07:58,566 - INFO - Starting user U201019 in cluster 0 (index 37)
2025-04-03 18:08:03,821 - INFO - Starting user U500489 in cluster 1 (index 37)
2025-04-03 18:08:09,698 - ERROR - Failed on user U343264 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:08:09,698 - INFO - Starting user U26169 in cluster 2 (index 68)
2025-04-03 18:08:16,637 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:08:21,622 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:08:28,395 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:08:44,934 - ERROR - Failed on user U26169 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:08:44,934 - INFO - Starting user U369158 in cluster 2 (index 69)
2025-04-03 18:08:55,824 - INFO - Starting user U371426 in cluster 1 (index 38)
2025-04-03 18:08:59,450 - INFO - Starting user U566582 in cluster 0 (index 38)
2025-04-03 18:09:02,536 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:09:13,465 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:09:15,397 - ERROR - Failed on user U369158 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:09:15,397 - INFO - Starting user U415460 in cluster 2 (index 70)
2025-04-03 18:09:16,926 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:09:32,922 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:09:48,262 - INFO - Starting user U324761 in cluster 1 (index 39)
2025-04-03 18:09:48,545 - ERROR - Failed on user U415460 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:09:48,545 - INFO - Starting user U559720 in cluster 2 (index 71)
2025-04-03 18:09:55,507 - INFO - Starting user U437566 in cluster 0 (index 39)
2025-04-03 18:10:06,122 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 18:10:07,125 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:10:13,408 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:10:23,835 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U221935', 'user_index_in_cluster': 30, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 66, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U399329', 'user_index_in_cluster': 31, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U669817', 'user_index_in_cluster': 32, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 63, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U474980', 'user_index_in_cluster': 33, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 92, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U242727', 'user_index_in_cluster': 34, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 53, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U29060', 'user_index_in_cluster': 35, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 67, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U381081', 'user_index_in_cluster': 36, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 112, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U201019', 'user_index_in_cluster': 37, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 168, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U566582', 'user_index_in_cluster': 38, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 148, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U437566', 'user_index_in_cluster': 39, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 73, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.025, 'ndcg_50': 0.11444540197406203, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.025, 'ndcg_100': 0.11444540197406203, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.025, 'ndcg_200': 0.11444540197406203, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.025, 'ndcg_500': 0.11444540197406203, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.025, 'ndcg_1000': 0.11444540197406203, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.025, 'ndcg_2000': 0.11444540197406203, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:10:23,844 - INFO - Starting user U462883 in cluster 0 (index 40)
2025-04-03 18:10:25,040 - ERROR - Failed on user U559720 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:10:25,040 - INFO - Starting user U558663 in cluster 2 (index 72)
2025-04-03 18:10:41,766 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:10:42,368 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:10:59,001 - ERROR - Failed on user U558663 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:10:59,001 - INFO - Starting user U201390 in cluster 2 (index 73)
2025-04-03 18:11:07,112 - INFO - Starting user U131488 in cluster 0 (index 41)
2025-04-03 18:11:16,596 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:11:19,127 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U451936', 'user_index_in_cluster': 30, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 170, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U246941', 'user_index_in_cluster': 31, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 136, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U503286', 'user_index_in_cluster': 32, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 275, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U121505', 'user_index_in_cluster': 33, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 281, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.25, 'ap_10': 0.125, 'ndcg_10': 0.123151194370365, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.25, 'ap_20': 0.125, 'ndcg_20': 0.123151194370365, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.25, 'ap_50': 0.125, 'ndcg_50': 0.123151194370365, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.25, 'ap_100': 0.125, 'ndcg_100': 0.123151194370365, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.125, 'ndcg_200': 0.123151194370365, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.125, 'ndcg_500': 0.123151194370365, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.125, 'ndcg_1000': 0.123151194370365, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.125, 'ndcg_2000': 0.123151194370365, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U141206', 'user_index_in_cluster': 34, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 148, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U301043', 'user_index_in_cluster': 35, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 117, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U308427', 'user_index_in_cluster': 36, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 242, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.002277904328018223, 'ndcg_500': 0.05344032602376989, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.002277904328018223, 'ndcg_1000': 0.05344032602376989, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0018100931036064269, 'ndcg_2000': 0.09795520488269871, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U500489', 'user_index_in_cluster': 37, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 179, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U371426', 'user_index_in_cluster': 38, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 186, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U324761', 'user_index_in_cluster': 39, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 225, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.125, 'ap_50': 0.022727272727272728, 'ndcg_50': 0.04605780568615433, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.125, 'ap_100': 0.022727272727272728, 'ndcg_100': 0.04605780568615433, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.125, 'ap_200': 0.022727272727272728, 'ndcg_200': 0.04605780568615433, 'num_recommendations_200': 200, 'precision_500': 0.008, 'recall_500': 0.5, 'ap_500': 0.011142203921197599, 'ndcg_500': 0.13394898103890607, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.5, 'ap_1000': 0.011142203921197599, 'ndcg_1000': 0.13394898103890607, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.625, 'ap_2000': 0.009655602899569356, 'ndcg_2000': 0.15827583408027066, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:11:19,134 - INFO - Starting user U338240 in cluster 1 (index 40)
2025-04-03 18:11:24,797 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:11:30,801 - ERROR - Failed on user U201390 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:11:30,802 - INFO - Starting user U158842 in cluster 2 (index 74)
2025-04-03 18:11:35,843 - INFO - Starting user U71074 in cluster 0 (index 42)
2025-04-03 18:11:37,020 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:11:45,112 - INFO - Starting user U679236 in cluster 1 (index 41)
2025-04-03 18:11:49,606 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:11:55,439 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:12:01,762 - ERROR - Failed on user U158842 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:12:01,762 - INFO - Starting user U503349 in cluster 2 (index 75)
2025-04-03 18:12:03,341 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:12:19,826 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:12:21,288 - INFO - Starting user U511586 in cluster 0 (index 43)
2025-04-03 18:12:28,549 - INFO - Starting user U343298 in cluster 1 (index 42)
2025-04-03 18:12:36,414 - ERROR - Failed on user U503349 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:12:36,414 - INFO - Starting user U250016 in cluster 2 (index 76)
2025-04-03 18:12:39,103 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 18:12:49,499 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:12:54,626 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:13:07,800 - INFO - Starting user U543803 in cluster 0 (index 44)
2025-04-03 18:13:12,488 - INFO - Starting user U25769 in cluster 1 (index 43)
2025-04-03 18:13:13,154 - ERROR - Failed on user U250016 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:13:13,155 - INFO - Starting user U43685 in cluster 2 (index 77)
2025-04-03 18:13:25,467 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:13:30,048 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:13:31,773 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:13:47,073 - ERROR - Failed on user U43685 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:13:47,074 - INFO - Starting user U655647 in cluster 2 (index 78)
2025-04-03 18:13:58,685 - INFO - Starting user U247213 in cluster 0 (index 45)
2025-04-03 18:14:02,428 - INFO - Starting user U563194 in cluster 1 (index 44)
2025-04-03 18:14:05,013 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:14:17,123 - ERROR - Failed on user U655647 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:14:17,124 - INFO - Starting user U292984 in cluster 2 (index 79)
2025-04-03 18:14:19,831 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:14:19,899 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:14:35,376 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:14:48,325 - INFO - Starting user U444821 in cluster 1 (index 45)
2025-04-03 18:14:50,537 - ERROR - Failed on user U292984 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:14:50,537 - INFO - Starting user U626729 in cluster 2 (index 80)
2025-04-03 18:14:53,610 - INFO - Starting user U263970 in cluster 0 (index 46)
2025-04-03 18:15:06,578 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:15:08,597 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:15:11,674 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:15:25,391 - ERROR - Failed on user U626729 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:15:25,391 - INFO - Starting user U353674 in cluster 2 (index 81)
2025-04-03 18:15:43,565 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:15:43,898 - INFO - Starting user U346893 in cluster 0 (index 47)
2025-04-03 18:15:47,322 - INFO - Starting user U536528 in cluster 1 (index 46)
2025-04-03 18:15:59,290 - ERROR - Failed on user U353674 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:15:59,291 - INFO - Starting user U110904 in cluster 2 (index 82)
2025-04-03 18:16:01,635 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:16:05,062 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:16:17,089 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:16:33,201 - ERROR - Failed on user U110904 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:16:33,202 - INFO - Starting user U344648 in cluster 2 (index 83)
2025-04-03 18:16:35,082 - INFO - Starting user U516706 in cluster 0 (index 48)
2025-04-03 18:16:39,046 - INFO - Starting user U675541 in cluster 1 (index 47)
2025-04-03 18:16:51,073 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:16:53,371 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:16:56,294 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:17:04,415 - ERROR - Failed on user U344648 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:17:04,415 - INFO - Starting user U642982 in cluster 2 (index 84)
2025-04-03 18:17:22,734 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:17:27,005 - INFO - Starting user U497767 in cluster 0 (index 49)
2025-04-03 18:17:32,509 - INFO - Starting user U96240 in cluster 1 (index 48)
2025-04-03 18:17:38,409 - ERROR - Failed on user U642982 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:17:38,409 - INFO - Starting user U479071 in cluster 2 (index 85)
2025-04-03 18:17:44,775 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:17:50,840 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:17:55,753 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:18:12,259 - ERROR - Failed on user U479071 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:18:12,259 - INFO - Starting user U36940 in cluster 2 (index 86)
2025-04-03 18:18:23,534 - INFO - Starting user U167952 in cluster 1 (index 49)
2025-04-03 18:18:27,272 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U462883', 'user_index_in_cluster': 40, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 109, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2857142857142857, 'ap_500': 0.004686951176677204, 'ndcg_500': 0.0669780970716375, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.42857142857142855, 'ap_1000': 0.004549135542286227, 'ndcg_1000': 0.09604283882509085, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.5714285714285714, 'ap_2000': 0.004255021977119392, 'ndcg_2000': 0.122956921654643, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U131488', 'user_index_in_cluster': 41, 'num_candidates': 4522, 'num_history_articles': 35, 'original_history_len': 35, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U71074', 'user_index_in_cluster': 42, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 66, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0038022813688212928, 'ndcg_500': 0.05833611817867791, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0038022813688212928, 'ndcg_1000': 0.05833611817867791, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0025874824826261577, 'ndcg_2000': 0.10298776160868013, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U511586', 'user_index_in_cluster': 43, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U543803', 'user_index_in_cluster': 44, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 65, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U247213', 'user_index_in_cluster': 45, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 109, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0012121212121212121, 'ndcg_1000': 0.048429187230677385, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0012121212121212121, 'ndcg_2000': 0.048429187230677385, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U263970', 'user_index_in_cluster': 46, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 120, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U346893', 'user_index_in_cluster': 47, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 75, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U516706', 'user_index_in_cluster': 48, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0009541984732824427, 'ndcg_2000': 0.09965321714259505, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U497767', 'user_index_in_cluster': 49, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 110, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:18:27,277 - INFO - Starting user U416703 in cluster 0 (index 50)
2025-04-03 18:18:31,158 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:18:41,321 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:18:43,790 - ERROR - Failed on user U36940 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:18:43,790 - INFO - Starting user U355171 in cluster 2 (index 87)
2025-04-03 18:18:44,902 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:19:01,860 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:19:14,936 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U338240', 'user_index_in_cluster': 40, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 213, 'num_future_clicks': 9, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.1111111111111111, 'ap_500': 0.0025380710659898475, 'ndcg_500': 0.027249418417765705, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0028997653107502284, 'ndcg_1000': 0.07592738053269138, 'num_recommendations_1000': 1000, 'precision_2000': 0.003, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0035221836869400204, 'ndcg_2000': 0.14493324677742506, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U679236', 'user_index_in_cluster': 41, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 120, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0005002501250625312, 'ndcg_2000': 0.042794816480125335, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U343298', 'user_index_in_cluster': 42, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.02631578947368421, 'ndcg_50': 0.11600766930798434, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.02631578947368421, 'ndcg_100': 0.11600766930798434, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.02631578947368421, 'ndcg_200': 0.11600766930798434, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.02631578947368421, 'ndcg_500': 0.11600766930798434, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 1.0, 'ap_1000': 0.01497277676950998, 'ndcg_1000': 0.18332342343204347, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.01497277676950998, 'ndcg_2000': 0.18332342343204347, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U25769', 'user_index_in_cluster': 43, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 206, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U563194', 'user_index_in_cluster': 44, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 171, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.024390243902439025, 'ndcg_50': 0.08702728145052864, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.024390243902439025, 'ndcg_100': 0.08702728145052864, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.024390243902439025, 'ndcg_200': 0.08702728145052864, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.024390243902439025, 'ndcg_500': 0.08702728145052864, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.013797686053783615, 'ndcg_1000': 0.13755411807566553, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.013797686053783615, 'ndcg_2000': 0.13755411807566553, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U444821', 'user_index_in_cluster': 45, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 234, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0019763241645000835, 'ndcg_1000': 0.06347788020897274, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0019763241645000835, 'ndcg_2000': 0.06347788020897274, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U536528', 'user_index_in_cluster': 46, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 279, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U675541', 'user_index_in_cluster': 47, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 237, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U96240', 'user_index_in_cluster': 48, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 282, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.004207968730742822, 'ndcg_500': 0.08091528635945372, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.6, 'ap_1000': 0.004010131764270315, 'ndcg_1000': 0.11588492580374418, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.004010131764270315, 'ndcg_2000': 0.11588492580374418, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U167952', 'user_index_in_cluster': 49, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 232, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.003289473684210526, 'ndcg_500': 0.047303510991381165, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.003289473684210526, 'ndcg_1000': 0.047303510991381165, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.75, 'ap_2000': 0.002258414344877087, 'ndcg_2000': 0.12203335871334188, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:19:14,941 - INFO - Starting user U348187 in cluster 1 (index 50)
2025-04-03 18:19:16,863 - ERROR - Failed on user U355171 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:19:16,864 - INFO - Starting user U654088 in cluster 2 (index 88)
2025-04-03 18:19:19,057 - INFO - Starting user U254813 in cluster 0 (index 51)
2025-04-03 18:19:32,774 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:19:34,856 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:19:37,643 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:19:50,370 - ERROR - Failed on user U654088 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:19:50,370 - INFO - Starting user U581465 in cluster 2 (index 89)
2025-04-03 18:20:05,491 - INFO - Starting user U7611 in cluster 1 (index 51)
2025-04-03 18:20:09,013 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:20:09,414 - INFO - Starting user U535755 in cluster 0 (index 52)
2025-04-03 18:20:20,960 - ERROR - Failed on user U581465 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:20:20,960 - INFO - Starting user U331512 in cluster 2 (index 90)
2025-04-03 18:20:23,708 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:20:27,131 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:20:39,534 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:20:56,172 - ERROR - Failed on user U331512 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:20:56,173 - INFO - Starting user U187226 in cluster 2 (index 91)
2025-04-03 18:20:58,363 - INFO - Starting user U169517 in cluster 1 (index 52)
2025-04-03 18:21:02,638 - INFO - Starting user U248665 in cluster 0 (index 53)
2025-04-03 18:21:13,770 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:21:15,889 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:21:20,000 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 18:21:26,672 - ERROR - Failed on user U187226 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:21:26,672 - INFO - Starting user U286542 in cluster 2 (index 92)
2025-04-03 18:21:44,578 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:21:47,946 - INFO - Starting user U686458 in cluster 1 (index 53)
2025-04-03 18:21:52,128 - INFO - Starting user U407018 in cluster 0 (index 54)
2025-04-03 18:22:03,688 - ERROR - Failed on user U286542 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:22:03,688 - INFO - Starting user U417378 in cluster 2 (index 93)
2025-04-03 18:22:05,790 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:22:10,552 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:22:21,927 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:22:37,851 - ERROR - Failed on user U417378 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:22:37,852 - INFO - Starting user U413939 in cluster 2 (index 94)
2025-04-03 18:22:45,222 - INFO - Starting user U285771 in cluster 0 (index 55)
2025-04-03 18:22:48,905 - INFO - Starting user U525537 in cluster 1 (index 54)
2025-04-03 18:22:56,161 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:23:04,134 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:23:08,441 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:23:09,672 - ERROR - Failed on user U413939 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:23:09,672 - INFO - Starting user U142571 in cluster 2 (index 95)
2025-04-03 18:23:29,219 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:23:36,887 - INFO - Starting user U391876 in cluster 0 (index 56)
2025-04-03 18:23:43,686 - INFO - Starting user U151031 in cluster 1 (index 55)
2025-04-03 18:23:48,528 - ERROR - Failed on user U142571 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:23:48,528 - INFO - Starting user U704615 in cluster 2 (index 96)
2025-04-03 18:23:54,735 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:24:01,455 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:24:06,161 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:24:22,103 - ERROR - Failed on user U704615 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:24:22,104 - INFO - Starting user U611828 in cluster 2 (index 97)
2025-04-03 18:24:28,917 - INFO - Starting user U685530 in cluster 0 (index 57)
2025-04-03 18:24:32,749 - INFO - Starting user U610608 in cluster 1 (index 56)
2025-04-03 18:24:40,142 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:24:49,480 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:24:50,404 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:24:53,283 - ERROR - Failed on user U611828 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:24:53,284 - INFO - Starting user U100015 in cluster 2 (index 98)
2025-04-03 18:25:11,218 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:25:23,619 - INFO - Starting user U544680 in cluster 0 (index 58)
2025-04-03 18:25:29,066 - INFO - Starting user U579765 in cluster 1 (index 57)
2025-04-03 18:25:29,573 - ERROR - Failed on user U100015 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:25:29,574 - INFO - Starting user U519106 in cluster 2 (index 99)
2025-04-03 18:25:41,403 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:25:47,244 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:25:47,384 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:26:03,252 - ERROR - Failed on user U519106 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:26:03,253 - INFO - Starting user U620890 in cluster 2 (index 100)
2025-04-03 18:26:03,329 - INFO - history_titles:[] empty for user: U620890!!!
2025-04-03 18:26:14,029 - INFO - Starting user U427058 in cluster 0 (index 59)
2025-04-03 18:26:17,589 - INFO - Starting user U591567 in cluster 1 (index 58)
2025-04-03 18:26:20,963 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:26:31,859 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:26:33,876 - ERROR - Failed on user U620890 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:26:33,877 - INFO - Starting user U357664 in cluster 2 (index 101)
2025-04-03 18:26:35,363 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:26:51,888 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:27:07,926 - ERROR - Failed on user U357664 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:27:07,927 - INFO - Starting user U182348 in cluster 2 (index 102)
2025-04-03 18:27:15,332 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U416703', 'user_index_in_cluster': 50, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 95, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U254813', 'user_index_in_cluster': 51, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 91, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.00228310502283105, 'ndcg_500': 0.04447215993108565, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.00228310502283105, 'ndcg_1000': 0.04447215993108565, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.001983303353166367, 'ndcg_2000': 0.0826865193778048, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U535755', 'user_index_in_cluster': 52, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 66, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U248665', 'user_index_in_cluster': 53, 'num_candidates': 4520, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U407018', 'user_index_in_cluster': 54, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U285771', 'user_index_in_cluster': 55, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006997900629811056, 'ndcg_2000': 0.09540346641045024, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U391876', 'user_index_in_cluster': 56, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U685530', 'user_index_in_cluster': 57, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 109, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0010141987829614604, 'ndcg_1000': 0.100533771245839, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0010141987829614604, 'ndcg_2000': 0.100533771245839, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U544680', 'user_index_in_cluster': 58, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 107, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.002232142857142857, 'ndcg_500': 0.04430814096701581, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.002232142857142857, 'ndcg_1000': 0.04430814096701581, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.002232142857142857, 'ndcg_2000': 0.04430814096701581, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U427058', 'user_index_in_cluster': 59, 'num_candidates': 4519, 'num_history_articles': 40, 'original_history_len': 40, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:27:15,339 - INFO - Starting user U261602 in cluster 0 (index 60)
2025-04-03 18:27:21,186 - INFO - Starting user U89621 in cluster 1 (index 59)
2025-04-03 18:27:26,257 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:27:34,017 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:27:39,247 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:27:39,836 - ERROR - Failed on user U182348 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:27:39,836 - INFO - Starting user U159920 in cluster 2 (index 103)
2025-04-03 18:27:57,883 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:28:08,240 - INFO - Starting user U131785 in cluster 0 (index 61)
2025-04-03 18:28:12,081 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U348187', 'user_index_in_cluster': 50, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 290, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.010101010101010102, 'ndcg_100': 0.051048697561455816, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.010101010101010102, 'ndcg_200': 0.051048697561455816, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.010101010101010102, 'ndcg_500': 0.051048697561455816, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.006232538147431765, 'ndcg_1000': 0.0859194150096549, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.006232538147431765, 'ndcg_2000': 0.0859194150096549, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U7611', 'user_index_in_cluster': 51, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 141, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.125, 'ap_500': 0.00228310502283105, 'ndcg_500': 0.028815274579507586, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.375, 'ap_1000': 0.0032488006532707327, 'ndcg_1000': 0.08277468965125953, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.625, 'ap_2000': 0.0034158919683179466, 'ndcg_2000': 0.13212816147113524, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U169517', 'user_index_in_cluster': 52, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 172, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0011890606420927466, 'ndcg_1000': 0.034901368445297, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.001625041979791563, 'ndcg_2000': 0.10018023200570558, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U686458', 'user_index_in_cluster': 53, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 193, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.002257336343115124, 'ndcg_500': 0.05336098874040606, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.002257336343115124, 'ndcg_1000': 0.05336098874040606, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.002257336343115124, 'ndcg_2000': 0.05336098874040606, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U525537', 'user_index_in_cluster': 54, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 167, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U151031', 'user_index_in_cluster': 55, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 186, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U610608', 'user_index_in_cluster': 56, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 142, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002232142857142857, 'ndcg_500': 0.06959221469214835, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002232142857142857, 'ndcg_1000': 0.06959221469214835, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0018853021978021977, 'ndcg_2000': 0.1288598004602348, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U579765', 'user_index_in_cluster': 57, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 192, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U591567', 'user_index_in_cluster': 58, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 117, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.002145922746781116, 'ndcg_500': 0.11277416999668409, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.002145922746781116, 'ndcg_1000': 0.11277416999668409, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.002145922746781116, 'ndcg_2000': 0.11277416999668409, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U89621', 'user_index_in_cluster': 59, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 144, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:28:12,087 - INFO - Starting user U681891 in cluster 1 (index 60)
2025-04-03 18:28:13,965 - ERROR - Failed on user U159920 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:28:13,966 - INFO - Starting user U674301 in cluster 2 (index 104)
2025-04-03 18:28:26,075 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:28:30,142 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:28:31,526 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:28:47,782 - ERROR - Failed on user U674301 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:28:47,782 - INFO - Starting user U469857 in cluster 2 (index 105)
2025-04-03 18:28:58,676 - INFO - Starting user U234111 in cluster 0 (index 62)
2025-04-03 18:29:02,462 - INFO - Starting user U618431 in cluster 1 (index 61)
2025-04-03 18:29:06,301 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:29:16,455 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:29:19,230 - ERROR - Failed on user U469857 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:29:19,230 - INFO - Starting user U601773 in cluster 2 (index 106)
2025-04-03 18:29:19,787 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:29:37,178 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:29:49,489 - INFO - Starting user U139797 in cluster 0 (index 63)
2025-04-03 18:29:53,330 - INFO - Starting user U651430 in cluster 1 (index 62)
2025-04-03 18:29:55,179 - ERROR - Failed on user U601773 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:29:55,179 - INFO - Starting user U203922 in cluster 2 (index 107)
2025-04-03 18:30:07,785 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:30:10,909 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:30:12,558 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:30:29,429 - ERROR - Failed on user U203922 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:30:29,429 - INFO - Starting user U443540 in cluster 2 (index 108)
2025-04-03 18:30:40,166 - INFO - Starting user U148239 in cluster 0 (index 64)
2025-04-03 18:30:43,856 - INFO - Starting user U164409 in cluster 1 (index 63)
2025-04-03 18:30:47,770 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:30:58,002 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:31:00,618 - ERROR - Failed on user U443540 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:31:00,618 - INFO - Starting user U63207 in cluster 2 (index 109)
2025-04-03 18:31:01,960 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:31:18,259 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:31:31,769 - INFO - Starting user U174436 in cluster 0 (index 65)
2025-04-03 18:31:33,598 - ERROR - Failed on user U63207 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:31:33,598 - INFO - Starting user U607368 in cluster 2 (index 110)
2025-04-03 18:31:36,032 - INFO - Starting user U403278 in cluster 1 (index 64)
2025-04-03 18:31:49,164 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:31:50,965 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:31:53,357 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:32:07,392 - ERROR - Failed on user U607368 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:32:07,392 - INFO - Starting user U310458 in cluster 2 (index 111)
2025-04-03 18:32:21,171 - INFO - Starting user U264552 in cluster 0 (index 66)
2025-04-03 18:32:24,871 - INFO - Starting user U702015 in cluster 1 (index 65)
2025-04-03 18:32:25,516 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:32:37,262 - ERROR - Failed on user U310458 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:32:37,263 - INFO - Starting user U417199 in cluster 2 (index 112)
2025-04-03 18:32:39,036 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:32:42,789 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:32:55,490 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:33:10,976 - ERROR - Failed on user U417199 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:33:10,977 - INFO - Starting user U318822 in cluster 2 (index 113)
2025-04-03 18:33:13,187 - INFO - Starting user U11495 in cluster 0 (index 67)
2025-04-03 18:33:17,591 - INFO - Starting user U342460 in cluster 1 (index 66)
2025-04-03 18:33:28,679 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:33:31,961 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:33:35,522 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:33:42,071 - ERROR - Failed on user U318822 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:33:42,071 - INFO - Starting user U454102 in cluster 2 (index 114)
2025-04-03 18:34:00,125 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:34:03,751 - INFO - Starting user U302323 in cluster 0 (index 68)
2025-04-03 18:34:09,960 - INFO - Starting user U98650 in cluster 1 (index 67)
2025-04-03 18:34:15,366 - ERROR - Failed on user U454102 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:34:15,367 - INFO - Starting user U653934 in cluster 2 (index 115)
2025-04-03 18:34:21,648 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:34:28,806 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:34:33,178 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:34:49,445 - ERROR - Failed on user U653934 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:34:49,445 - INFO - Starting user U204203 in cluster 2 (index 116)
2025-04-03 18:34:49,541 - INFO - history_titles:[] empty for user: U204203!!!
2025-04-03 18:35:03,380 - INFO - Starting user U436656 in cluster 0 (index 69)
2025-04-03 18:35:07,213 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:35:07,874 - INFO - Starting user U232234 in cluster 1 (index 68)
2025-04-03 18:35:19,689 - ERROR - Failed on user U204203 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:35:19,689 - INFO - Starting user U201138 in cluster 2 (index 117)
2025-04-03 18:35:21,052 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:35:25,930 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:35:38,447 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:35:46,177 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U261602', 'user_index_in_cluster': 60, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 84, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U131785', 'user_index_in_cluster': 61, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 92, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U234111', 'user_index_in_cluster': 62, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0007423904974016332, 'ndcg_2000': 0.058975714251951916, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U139797', 'user_index_in_cluster': 63, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 74, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U148239', 'user_index_in_cluster': 64, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U174436', 'user_index_in_cluster': 65, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 78, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U264552', 'user_index_in_cluster': 66, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 90, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U11495', 'user_index_in_cluster': 67, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U302323', 'user_index_in_cluster': 68, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U436656', 'user_index_in_cluster': 69, 'num_candidates': 4525, 'num_history_articles': 48, 'original_history_len': 48, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:35:46,179 - INFO - Starting user U258175 in cluster 0 (index 70)
2025-04-03 18:35:51,931 - INFO - Starting user U391045 in cluster 1 (index 69)
2025-04-03 18:35:57,026 - ERROR - Failed on user U201138 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:35:57,026 - INFO - Starting user U92337 in cluster 2 (index 118)
2025-04-03 18:36:04,272 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 18:36:09,524 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:36:14,524 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:36:31,909 - ERROR - Failed on user U92337 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:36:31,910 - INFO - Starting user U521638 in cluster 2 (index 119)
2025-04-03 18:36:38,776 - INFO - Starting user U140433 in cluster 0 (index 71)
2025-04-03 18:36:42,431 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U681891', 'user_index_in_cluster': 60, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 404, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U618431', 'user_index_in_cluster': 61, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 167, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U651430', 'user_index_in_cluster': 62, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 192, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0028089887640449437, 'ndcg_500': 0.07230696712347667, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0028089887640449437, 'ndcg_1000': 0.07230696712347667, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0020751851935583536, 'ndcg_2000': 0.13046359637574564, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U164409', 'user_index_in_cluster': 63, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 113, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U403278', 'user_index_in_cluster': 64, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0006531678641410843, 'ndcg_2000': 0.044350234398201356, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U702015', 'user_index_in_cluster': 65, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0005099439061703213, 'ndcg_2000': 0.09142347824252825, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U342460', 'user_index_in_cluster': 66, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 174, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U98650', 'user_index_in_cluster': 67, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 304, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U232234', 'user_index_in_cluster': 68, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 236, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.14285714285714285, 'ap_50': 0.025, 'ndcg_50': 0.05130633033097092, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.14285714285714285, 'ap_100': 0.025, 'ndcg_100': 0.05130633033097092, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.14285714285714285, 'ap_200': 0.025, 'ndcg_200': 0.05130633033097092, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.14285714285714285, 'ap_500': 0.025, 'ndcg_500': 0.05130633033097092, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.2857142857142857, 'ap_1000': 0.013932664756446993, 'ndcg_1000': 0.0803963937039098, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.7142857142857143, 'ap_2000': 0.007520835146481523, 'ndcg_2000': 0.1609382312689033, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U391045', 'user_index_in_cluster': 69, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 146, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:36:42,433 - INFO - Starting user U179692 in cluster 1 (index 70)
2025-04-03 18:36:49,721 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:36:56,905 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:37:00,728 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:37:02,796 - ERROR - Failed on user U521638 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:37:02,796 - INFO - Starting user U509707 in cluster 2 (index 120)
2025-04-03 18:37:21,584 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:37:31,337 - INFO - Starting user U268052 in cluster 0 (index 72)
2025-04-03 18:37:35,184 - INFO - Starting user U213585 in cluster 1 (index 71)
2025-04-03 18:37:37,202 - ERROR - Failed on user U509707 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:37:37,202 - INFO - Starting user U327484 in cluster 2 (index 121)
2025-04-03 18:37:49,150 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:37:52,815 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:37:54,831 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:38:10,617 - ERROR - Failed on user U327484 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:38:10,617 - INFO - Starting user U225469 in cluster 2 (index 122)
2025-04-03 18:38:22,007 - INFO - Starting user U637225 in cluster 0 (index 73)
2025-04-03 18:38:25,601 - INFO - Starting user U653354 in cluster 1 (index 72)
2025-04-03 18:38:28,873 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:38:41,076 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:38:42,008 - ERROR - Failed on user U225469 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:38:42,008 - INFO - Starting user U468570 in cluster 2 (index 123)
2025-04-03 18:38:43,224 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:39:01,370 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:39:15,809 - INFO - Starting user U294797 in cluster 0 (index 74)
2025-04-03 18:39:17,552 - ERROR - Failed on user U468570 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:39:17,553 - INFO - Starting user U366024 in cluster 2 (index 124)
2025-04-03 18:39:19,736 - INFO - Starting user U592605 in cluster 1 (index 73)
2025-04-03 18:39:33,756 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:39:35,359 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:39:40,431 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:39:51,690 - ERROR - Failed on user U366024 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:39:51,690 - INFO - Starting user U688601 in cluster 2 (index 125)
2025-04-03 18:40:06,289 - INFO - Starting user U654413 in cluster 0 (index 75)
2025-04-03 18:40:10,015 - INFO - Starting user U668078 in cluster 1 (index 74)
2025-04-03 18:40:10,103 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:40:21,842 - ERROR - Failed on user U688601 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:40:21,842 - INFO - Starting user U249089 in cluster 2 (index 126)
2025-04-03 18:40:24,412 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:40:27,355 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:40:40,150 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:40:56,241 - ERROR - Failed on user U249089 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:40:56,241 - INFO - Starting user U425844 in cluster 2 (index 127)
2025-04-03 18:40:58,498 - INFO - Starting user U336529 in cluster 0 (index 76)
2025-04-03 18:41:02,755 - INFO - Starting user U302232 in cluster 1 (index 75)
2025-04-03 18:41:14,010 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:41:17,161 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:41:20,581 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:41:27,759 - ERROR - Failed on user U425844 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:41:27,759 - INFO - Starting user U606840 in cluster 2 (index 128)
2025-04-03 18:41:45,666 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:41:49,722 - INFO - Starting user U278714 in cluster 0 (index 77)
2025-04-03 18:41:55,487 - INFO - Starting user U181912 in cluster 1 (index 76)
2025-04-03 18:42:01,248 - ERROR - Failed on user U606840 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:42:01,249 - INFO - Starting user U118212 in cluster 2 (index 129)
2025-04-03 18:42:07,563 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:42:13,875 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:42:19,596 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:42:38,703 - ERROR - Failed on user U118212 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:42:38,703 - INFO - Starting user U83962 in cluster 2 (index 130)
2025-04-03 18:42:41,324 - INFO - Starting user U535738 in cluster 0 (index 78)
2025-04-03 18:42:45,300 - INFO - Starting user U154168 in cluster 1 (index 77)
2025-04-03 18:42:56,331 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:42:59,272 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:43:02,927 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:43:09,610 - ERROR - Failed on user U83962 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:43:09,611 - INFO - Starting user U89128 in cluster 2 (index 131)
2025-04-03 18:43:09,707 - INFO - history_titles:[] empty for user: U89128!!!
2025-04-03 18:43:27,372 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:43:31,232 - INFO - Starting user U460737 in cluster 0 (index 79)
2025-04-03 18:43:37,319 - INFO - Starting user U542261 in cluster 1 (index 78)
2025-04-03 18:43:42,814 - ERROR - Failed on user U89128 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:43:42,814 - INFO - Starting user U639282 in cluster 2 (index 132)
2025-04-03 18:43:49,677 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:43:55,233 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:44:01,114 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:44:17,573 - ERROR - Failed on user U639282 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:44:17,574 - INFO - Starting user U348890 in cluster 2 (index 133)
2025-04-03 18:44:24,497 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U258175', 'user_index_in_cluster': 70, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 61, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U140433', 'user_index_in_cluster': 71, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 81, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U268052', 'user_index_in_cluster': 72, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 80, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U637225', 'user_index_in_cluster': 73, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 161, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U294797', 'user_index_in_cluster': 74, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 102, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0022935779816513763, 'ndcg_500': 0.06990228910351387, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0022935779816513763, 'ndcg_1000': 0.06990228910351387, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0022935779816513763, 'ndcg_2000': 0.06990228910351387, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U654413', 'user_index_in_cluster': 75, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 75, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U336529', 'user_index_in_cluster': 76, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 93, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.009259259259259259, 'ndcg_200': 0.05767869656903417, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.009259259259259259, 'ndcg_500': 0.05767869656903417, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.009259259259259259, 'ndcg_1000': 0.05767869656903417, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.009259259259259259, 'ndcg_2000': 0.05767869656903417, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U278714', 'user_index_in_cluster': 77, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 71, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U535738', 'user_index_in_cluster': 78, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 129, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U460737', 'user_index_in_cluster': 79, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 98, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:44:24,504 - INFO - Starting user U396158 in cluster 0 (index 80)
2025-04-03 18:44:27,943 - INFO - Starting user U626915 in cluster 1 (index 79)
2025-04-03 18:44:35,452 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:44:42,931 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:44:46,724 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:44:48,496 - ERROR - Failed on user U348890 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:44:48,496 - INFO - Starting user U640287 in cluster 2 (index 134)
2025-04-03 18:45:06,487 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:45:16,327 - INFO - Starting user U174024 in cluster 0 (index 81)
2025-04-03 18:45:20,315 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U179692', 'user_index_in_cluster': 70, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 230, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U213585', 'user_index_in_cluster': 71, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 188, 'num_future_clicks': 3, 'precision_5': 0.2, 'recall_5': 0.3333333333333333, 'ap_5': 0.3333333333333333, 'ndcg_5': 0.23463936301137822, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.3333333333333333, 'ap_10': 0.3333333333333333, 'ndcg_10': 0.23463936301137822, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.3333333333333333, 'ap_20': 0.3333333333333333, 'ndcg_20': 0.23463936301137822, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.3333333333333333, 'ndcg_50': 0.23463936301137822, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.3333333333333333, 'ndcg_100': 0.23463936301137822, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.3333333333333333, 'ndcg_200': 0.23463936301137822, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.3333333333333333, 'ndcg_500': 0.23463936301137822, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.3333333333333333, 'ndcg_1000': 0.23463936301137822, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.3333333333333333, 'ndcg_2000': 0.23463936301137822, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U653354', 'user_index_in_cluster': 72, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 256, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U592605', 'user_index_in_cluster': 73, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 213, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U668078', 'user_index_in_cluster': 74, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 155, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.5, 'ap_500': 0.003180294827626614, 'ndcg_500': 0.0881873976887382, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.003180294827626614, 'ndcg_1000': 0.0881873976887382, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.003180294827626614, 'ndcg_2000': 0.0881873976887382, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U302232', 'user_index_in_cluster': 75, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 194, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U181912', 'user_index_in_cluster': 76, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 199, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0007194244604316547, 'ndcg_2000': 0.09576795007802541, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U154168', 'user_index_in_cluster': 77, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 220, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.024390243902439025, 'ndcg_50': 0.08702728145052864, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.024390243902439025, 'ndcg_100': 0.08702728145052864, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.024390243902439025, 'ndcg_200': 0.08702728145052864, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.024390243902439025, 'ndcg_500': 0.08702728145052864, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.024390243902439025, 'ndcg_1000': 0.08702728145052864, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.012705586473935184, 'ndcg_2000': 0.1299361469208941, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U542261', 'user_index_in_cluster': 78, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 137, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0007423904974016332, 'ndcg_2000': 0.058975714251951916, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U626915', 'user_index_in_cluster': 79, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 137, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:45:20,321 - INFO - Starting user U51702 in cluster 1 (index 80)
2025-04-03 18:45:22,383 - ERROR - Failed on user U640287 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:45:22,384 - INFO - Starting user U444660 in cluster 2 (index 135)
2025-04-03 18:45:34,152 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:45:38,029 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:45:40,262 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:45:55,676 - ERROR - Failed on user U444660 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:45:55,677 - INFO - Starting user U462807 in cluster 2 (index 136)
2025-04-03 18:46:06,764 - INFO - Starting user U628595 in cluster 0 (index 82)
2025-04-03 18:46:10,610 - INFO - Starting user U149919 in cluster 1 (index 81)
2025-04-03 18:46:14,004 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:46:25,669 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:46:27,323 - ERROR - Failed on user U462807 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:46:27,323 - INFO - Starting user U382556 in cluster 2 (index 137)
2025-04-03 18:46:28,458 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:46:45,797 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:47:00,848 - INFO - Starting user U542308 in cluster 0 (index 83)
2025-04-03 18:47:02,218 - ERROR - Failed on user U382556 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:47:02,219 - INFO - Starting user U537300 in cluster 2 (index 138)
2025-04-03 18:47:04,855 - INFO - Starting user U538374 in cluster 1 (index 82)
2025-04-03 18:47:18,721 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:47:20,583 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:47:22,754 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 18:47:37,763 - ERROR - Failed on user U537300 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:47:37,763 - INFO - Starting user U679825 in cluster 2 (index 139)
2025-04-03 18:47:53,135 - INFO - Starting user U545687 in cluster 0 (index 84)
2025-04-03 18:47:56,480 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:47:57,755 - INFO - Starting user U635070 in cluster 1 (index 83)
2025-04-03 18:48:08,947 - ERROR - Failed on user U679825 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:48:08,947 - INFO - Starting user U152256 in cluster 2 (index 140)
2025-04-03 18:48:11,259 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:48:16,781 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:48:27,269 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:48:44,771 - ERROR - Failed on user U152256 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:48:44,772 - INFO - Starting user U405992 in cluster 2 (index 141)
2025-04-03 18:48:47,145 - INFO - Starting user U647027 in cluster 0 (index 85)
2025-04-03 18:48:51,127 - INFO - Starting user U297646 in cluster 1 (index 84)
2025-04-03 18:49:03,203 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:49:07,004 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:49:09,205 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:49:16,253 - ERROR - Failed on user U405992 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:49:16,254 - INFO - Starting user U411944 in cluster 2 (index 142)
2025-04-03 18:49:34,185 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:49:39,457 - INFO - Starting user U439886 in cluster 0 (index 86)
2025-04-03 18:49:45,316 - INFO - Starting user U224054 in cluster 1 (index 85)
2025-04-03 18:49:50,918 - ERROR - Failed on user U411944 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:49:50,919 - INFO - Starting user U314454 in cluster 2 (index 143)
2025-04-03 18:49:57,303 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:50:03,282 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:50:08,804 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:50:24,641 - ERROR - Failed on user U314454 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:50:24,641 - INFO - Starting user U360436 in cluster 2 (index 144)
2025-04-03 18:50:31,710 - INFO - Starting user U44391 in cluster 0 (index 87)
2025-04-03 18:50:35,358 - INFO - Starting user U118409 in cluster 1 (index 86)
2025-04-03 18:50:42,562 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:50:49,496 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:50:53,206 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:50:55,711 - ERROR - Failed on user U360436 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:50:55,711 - INFO - Starting user U349354 in cluster 2 (index 145)
2025-04-03 18:51:13,527 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:51:22,549 - INFO - Starting user U672038 in cluster 0 (index 88)
2025-04-03 18:51:27,642 - INFO - Starting user U55164 in cluster 1 (index 87)
2025-04-03 18:51:28,612 - ERROR - Failed on user U349354 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:51:28,612 - INFO - Starting user U493622 in cluster 2 (index 146)
2025-04-03 18:51:40,469 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:51:45,156 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:51:46,462 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:52:02,296 - ERROR - Failed on user U493622 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:52:02,296 - INFO - Starting user U480852 in cluster 2 (index 147)
2025-04-03 18:52:13,108 - INFO - Starting user U667322 in cluster 0 (index 89)
2025-04-03 18:52:16,837 - INFO - Starting user U609133 in cluster 1 (index 88)
2025-04-03 18:52:20,434 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:52:31,498 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 18:52:33,074 - ERROR - Failed on user U480852 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:52:33,074 - INFO - Starting user U103412 in cluster 2 (index 148)
2025-04-03 18:52:34,627 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:52:51,320 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:53:06,012 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U396158', 'user_index_in_cluster': 80, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 77, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U174024', 'user_index_in_cluster': 81, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 65, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U628595', 'user_index_in_cluster': 82, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 203, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U542308', 'user_index_in_cluster': 83, 'num_candidates': 4523, 'num_history_articles': 45, 'original_history_len': 45, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U545687', 'user_index_in_cluster': 84, 'num_candidates': 4525, 'num_history_articles': 38, 'original_history_len': 38, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U647027', 'user_index_in_cluster': 85, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 159, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0010085438784012627, 'ndcg_2000': 0.08934025858751811, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U439886', 'user_index_in_cluster': 86, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 90, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.0023094688221709007, 'ndcg_500': 0.04455604260424845, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0023094688221709007, 'ndcg_1000': 0.04455604260424845, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0023094688221709007, 'ndcg_2000': 0.04455604260424845, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U44391', 'user_index_in_cluster': 87, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U672038', 'user_index_in_cluster': 88, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 154, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U667322', 'user_index_in_cluster': 89, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 97, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:53:06,016 - INFO - Starting user U363456 in cluster 0 (index 90)
2025-04-03 18:53:07,746 - ERROR - Failed on user U103412 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:53:07,746 - INFO - Starting user U307287 in cluster 2 (index 149)
2025-04-03 18:53:10,247 - INFO - Starting user U496731 in cluster 1 (index 89)
2025-04-03 18:53:23,724 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:53:25,414 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:53:28,484 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:53:41,336 - ERROR - Failed on user U307287 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:53:41,336 - INFO - Starting user U689848 in cluster 2 (index 150)
2025-04-03 18:53:55,264 - INFO - Starting user U423743 in cluster 0 (index 91)
2025-04-03 18:53:59,581 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U51702', 'user_index_in_cluster': 80, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 192, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U149919', 'user_index_in_cluster': 81, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 146, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U538374', 'user_index_in_cluster': 82, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 142, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0011737089201877935, 'ndcg_1000': 0.06297472075317656, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0011737089201877935, 'ndcg_2000': 0.06297472075317656, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U635070', 'user_index_in_cluster': 83, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 254, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U297646', 'user_index_in_cluster': 84, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 320, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.125, 'ap_200': 0.007246376811594203, 'ndcg_200': 0.0355309448671821, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.25, 'ap_500': 0.0070245489500147884, 'ndcg_500': 0.06636042983605231, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.375, 'ap_1000': 0.006168916734383311, 'ndcg_1000': 0.09327895079739078, 'num_recommendations_1000': 1000, 'precision_2000': 0.003, 'recall_2000': 0.75, 'ap_2000': 0.0050781255305290506, 'ndcg_2000': 0.16712824474396254, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U224054', 'user_index_in_cluster': 85, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 180, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.008695652173913044, 'ndcg_200': 0.05692346628978079, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.008695652173913044, 'ndcg_500': 0.05692346628978079, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.005754295847856663, 'ndcg_1000': 0.09812133797286887, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 1.0, 'ap_2000': 0.004110592121279403, 'ndcg_2000': 0.1727753162576031, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U118409', 'user_index_in_cluster': 86, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 180, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U55164', 'user_index_in_cluster': 87, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 228, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U609133', 'user_index_in_cluster': 88, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 222, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U496731', 'user_index_in_cluster': 89, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 218, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.02702702702702703, 'ndcg_50': 0.11683606360696919, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.02702702702702703, 'ndcg_100': 0.11683606360696919, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.02702702702702703, 'ndcg_200': 0.11683606360696919, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.02702702702702703, 'ndcg_500': 0.11683606360696919, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.02702702702702703, 'ndcg_1000': 0.11683606360696919, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.02702702702702703, 'ndcg_2000': 0.11683606360696919, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 18:53:59,584 - INFO - Starting user U272330 in cluster 1 (index 90)
2025-04-03 18:53:59,621 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:54:11,334 - ERROR - Failed on user U689848 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:54:11,335 - INFO - Starting user U570874 in cluster 2 (index 151)
2025-04-03 18:54:13,206 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:54:17,477 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:54:29,621 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:54:45,777 - INFO - Starting user U645058 in cluster 0 (index 92)
2025-04-03 18:54:47,656 - ERROR - Failed on user U570874 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:54:47,656 - INFO - Starting user U394370 in cluster 2 (index 152)
2025-04-03 18:54:49,617 - INFO - Starting user U640148 in cluster 1 (index 91)
2025-04-03 18:55:03,951 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:55:05,034 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:55:07,153 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:55:21,867 - ERROR - Failed on user U394370 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:55:21,868 - INFO - Starting user U548312 in cluster 2 (index 153)
2025-04-03 18:55:37,790 - INFO - Starting user U447175 in cluster 0 (index 93)
2025-04-03 18:55:40,284 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:55:41,875 - INFO - Starting user U35158 in cluster 1 (index 92)
2025-04-03 18:55:53,427 - ERROR - Failed on user U548312 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:55:53,428 - INFO - Starting user U213052 in cluster 2 (index 154)
2025-04-03 18:55:55,610 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:56:00,840 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:56:12,933 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 18:56:29,142 - ERROR - Failed on user U213052 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:56:29,142 - INFO - Starting user U610238 in cluster 2 (index 155)
2025-04-03 18:56:31,540 - INFO - Starting user U347070 in cluster 0 (index 94)
2025-04-03 18:56:35,286 - INFO - Starting user U6278 in cluster 1 (index 93)
2025-04-03 18:56:46,731 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:56:49,553 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:56:52,786 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:56:59,078 - ERROR - Failed on user U610238 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:56:59,078 - INFO - Starting user U649325 in cluster 2 (index 156)
2025-04-03 18:57:17,667 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:57:20,635 - INFO - Starting user U679838 in cluster 0 (index 95)
2025-04-03 18:57:24,663 - INFO - Starting user U71856 in cluster 1 (index 94)
2025-04-03 18:57:36,333 - ERROR - Failed on user U649325 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:57:36,333 - INFO - Starting user U365036 in cluster 2 (index 157)
2025-04-03 18:57:39,171 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:57:42,142 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 18:57:54,750 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 18:58:10,767 - ERROR - Failed on user U365036 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:58:10,767 - INFO - Starting user U648357 in cluster 2 (index 158)
2025-04-03 18:58:12,886 - INFO - Starting user U191969 in cluster 0 (index 96)
2025-04-03 18:58:16,504 - INFO - Starting user U362332 in cluster 1 (index 95)
2025-04-03 18:58:28,181 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:58:30,729 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 18:58:34,692 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 18:58:41,070 - ERROR - Failed on user U648357 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:58:41,071 - INFO - Starting user U66246 in cluster 2 (index 159)
2025-04-03 18:58:58,930 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:59:02,095 - INFO - Starting user U372362 in cluster 0 (index 97)
2025-04-03 18:59:08,829 - INFO - Starting user U449089 in cluster 1 (index 96)
2025-04-03 18:59:13,570 - ERROR - Failed on user U66246 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:59:13,570 - INFO - Starting user U187601 in cluster 2 (index 160)
2025-04-03 18:59:13,667 - INFO - history_titles:[] empty for user: U187601!!!
2025-04-03 18:59:19,490 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:59:26,956 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 18:59:31,953 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 18:59:50,767 - ERROR - Failed on user U187601 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 18:59:50,768 - INFO - Starting user U19040 in cluster 2 (index 161)
2025-04-03 18:59:50,859 - INFO - history_titles:[] empty for user: U19040!!!
2025-04-03 18:59:53,218 - INFO - Starting user U204632 in cluster 0 (index 98)
2025-04-03 18:59:57,056 - INFO - Starting user U419448 in cluster 1 (index 97)
2025-04-03 19:00:08,242 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:00:11,701 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:00:14,741 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:00:21,375 - ERROR - Failed on user U19040 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:00:21,376 - INFO - Starting user U417772 in cluster 2 (index 162)
2025-04-03 19:00:39,271 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:00:43,981 - INFO - Starting user U444157 in cluster 0 (index 99)
2025-04-03 19:00:50,746 - INFO - Starting user U358761 in cluster 1 (index 98)
2025-04-03 19:00:55,775 - ERROR - Failed on user U417772 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:00:55,776 - INFO - Starting user U329649 in cluster 2 (index 163)
2025-04-03 19:01:01,548 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:01:09,113 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:01:14,365 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:01:33,116 - ERROR - Failed on user U329649 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:01:33,117 - INFO - Starting user U666459 in cluster 2 (index 164)
2025-04-03 19:01:35,861 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U363456', 'user_index_in_cluster': 90, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U423743', 'user_index_in_cluster': 91, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 58, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U645058', 'user_index_in_cluster': 92, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 138, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.002145922746781116, 'ndcg_500': 0.03412573602132208, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0024893919682914076, 'ndcg_1000': 0.06609449526483265, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0026064108089065815, 'ndcg_2000': 0.12480225666765438, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U447175', 'user_index_in_cluster': 93, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 147, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U347070', 'user_index_in_cluster': 94, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 52, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0019455252918287938, 'ndcg_1000': 0.06806372326281772, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0019455252918287938, 'ndcg_2000': 0.06806372326281772, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U679838', 'user_index_in_cluster': 95, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 78, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U191969', 'user_index_in_cluster': 96, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 212, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.2, 'ap_10': 0.125, 'ndcg_10': 0.10699313236726378, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.2, 'ap_20': 0.125, 'ndcg_20': 0.10699313236726378, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.125, 'ndcg_50': 0.10699313236726378, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.125, 'ndcg_100': 0.10699313236726378, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.125, 'ndcg_200': 0.10699313236726378, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.06475733634311512, 'ndcg_500': 0.14555853687256332, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.6, 'ap_1000': 0.04511330513489228, 'ndcg_1000': 0.18319605234743924, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.04511330513489228, 'ndcg_2000': 0.18319605234743924, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U372362', 'user_index_in_cluster': 97, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0014265335235378032, 'ndcg_1000': 0.06484674495968512, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0014265335235378032, 'ndcg_2000': 0.06484674495968512, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U204632', 'user_index_in_cluster': 98, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 123, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U444157', 'user_index_in_cluster': 99, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 89, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0015748031496062992, 'ndcg_1000': 0.036418390191532306, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.0015748031496062992, 'ndcg_2000': 0.036418390191532306, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:01:35,864 - INFO - Starting user U222297 in cluster 0 (index 100)
2025-04-03 19:01:39,493 - INFO - Starting user U200360 in cluster 1 (index 99)
2025-04-03 19:01:50,396 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:01:53,493 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 19:01:56,970 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:02:03,535 - ERROR - Failed on user U666459 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:02:03,535 - INFO - Starting user U26685 in cluster 2 (index 165)
2025-04-03 19:02:08,686 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U272330', 'user_index_in_cluster': 90, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 252, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U640148', 'user_index_in_cluster': 91, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 244, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U35158', 'user_index_in_cluster': 92, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 99, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U6278', 'user_index_in_cluster': 93, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 176, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U71856', 'user_index_in_cluster': 94, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 212, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U362332', 'user_index_in_cluster': 95, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 178, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U449089', 'user_index_in_cluster': 96, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 237, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U419448', 'user_index_in_cluster': 97, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 147, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U358761', 'user_index_in_cluster': 98, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 187, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U200360', 'user_index_in_cluster': 99, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 215, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006349206349206349, 'ndcg_2000': 0.057723988094499676, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:02:08,690 - INFO - Starting user U112455 in cluster 1 (index 100)
2025-04-03 19:02:22,259 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:02:27,381 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:02:36,180 - ERROR - Failed on user U26685 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:02:36,181 - INFO - Starting user U690012 in cluster 2 (index 166)
2025-04-03 19:02:44,214 - INFO - Starting user U522332 in cluster 1 (index 101)
2025-04-03 19:02:55,087 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:03:02,949 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:03:07,065 - ERROR - Failed on user U690012 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:03:07,065 - INFO - Starting user U340818 in cluster 2 (index 167)
2025-04-03 19:03:14,724 - INFO - Starting user U61455 in cluster 0 (index 101)
2025-04-03 19:03:22,200 - INFO - Starting user U466879 in cluster 1 (index 102)
2025-04-03 19:03:25,170 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:03:34,591 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:03:37,022 - ERROR - Failed on user U340818 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:03:37,022 - INFO - Starting user U442800 in cluster 2 (index 168)
2025-04-03 19:03:39,920 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:03:54,685 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:04:09,316 - INFO - Starting user U298725 in cluster 0 (index 102)
2025-04-03 19:04:10,792 - ERROR - Failed on user U442800 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:04:10,793 - INFO - Starting user U40368 in cluster 2 (index 169)
2025-04-03 19:04:13,041 - INFO - Starting user U132327 in cluster 1 (index 103)
2025-04-03 19:04:27,088 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:04:28,760 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:04:30,915 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 19:04:45,155 - ERROR - Failed on user U40368 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:04:45,155 - INFO - Starting user U166183 in cluster 2 (index 170)
2025-04-03 19:04:59,414 - INFO - Starting user U654323 in cluster 0 (index 103)
2025-04-03 19:05:03,250 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:05:03,505 - INFO - Starting user U603821 in cluster 1 (index 104)
2025-04-03 19:05:14,996 - ERROR - Failed on user U166183 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:05:14,997 - INFO - Starting user U299716 in cluster 2 (index 171)
2025-04-03 19:05:17,251 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:05:22,371 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:05:33,261 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:05:50,175 - ERROR - Failed on user U299716 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:05:50,176 - INFO - Starting user U115855 in cluster 2 (index 172)
2025-04-03 19:05:52,752 - INFO - Starting user U662232 in cluster 0 (index 104)
2025-04-03 19:05:56,594 - INFO - Starting user U518583 in cluster 1 (index 105)
2025-04-03 19:06:08,606 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:06:12,062 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:06:14,281 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:06:21,574 - ERROR - Failed on user U115855 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:06:21,575 - INFO - Starting user U9207 in cluster 2 (index 173)
2025-04-03 19:06:40,103 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:06:46,022 - INFO - Starting user U662056 in cluster 0 (index 105)
2025-04-03 19:06:51,006 - INFO - Starting user U378252 in cluster 1 (index 106)
2025-04-03 19:06:57,123 - ERROR - Failed on user U9207 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:06:57,124 - INFO - Starting user U89228 in cluster 2 (index 174)
2025-04-03 19:07:03,601 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:07:08,546 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:07:14,649 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:07:31,184 - ERROR - Failed on user U89228 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:07:31,184 - INFO - Starting user U139582 in cluster 2 (index 175)
2025-04-03 19:07:38,192 - INFO - Starting user U190662 in cluster 0 (index 106)
2025-04-03 19:07:41,593 - INFO - Starting user U693663 in cluster 1 (index 107)
2025-04-03 19:07:49,561 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:07:55,991 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:07:59,245 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:08:02,456 - ERROR - Failed on user U139582 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:08:02,456 - INFO - Starting user U331434 in cluster 2 (index 176)
2025-04-03 19:08:20,046 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:08:30,430 - INFO - Starting user U79094 in cluster 0 (index 107)
2025-04-03 19:08:35,710 - INFO - Starting user U297213 in cluster 1 (index 108)
2025-04-03 19:08:36,463 - ERROR - Failed on user U331434 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:08:36,464 - INFO - Starting user U536846 in cluster 2 (index 177)
2025-04-03 19:08:49,207 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:08:54,435 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:08:55,647 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:09:07,811 - ERROR - Failed on user U536846 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:09:07,812 - INFO - Starting user U437273 in cluster 2 (index 178)
2025-04-03 19:09:22,288 - INFO - Starting user U163662 in cluster 0 (index 108)
2025-04-03 19:09:26,549 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:09:27,487 - INFO - Starting user U434360 in cluster 1 (index 109)
2025-04-03 19:09:38,903 - ERROR - Failed on user U437273 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:09:38,903 - INFO - Starting user U314579 in cluster 2 (index 179)
2025-04-03 19:09:40,059 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:09:44,810 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 19:09:56,965 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:10:13,843 - INFO - Starting user U165561 in cluster 0 (index 109)
2025-04-03 19:10:16,026 - ERROR - Failed on user U314579 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:10:16,026 - INFO - Starting user U135387 in cluster 2 (index 180)
2025-04-03 19:10:18,868 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U112455', 'user_index_in_cluster': 100, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 250, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U522332', 'user_index_in_cluster': 101, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 239, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U466879', 'user_index_in_cluster': 102, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 206, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0007698229407236335, 'ndcg_2000': 0.045365941790552214, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U132327', 'user_index_in_cluster': 103, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 221, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0016806722689075631, 'ndcg_1000': 0.04234438812194874, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0016806722689075631, 'ndcg_2000': 0.04234438812194874, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U603821', 'user_index_in_cluster': 104, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 180, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U518583', 'user_index_in_cluster': 105, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 172, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U378252', 'user_index_in_cluster': 106, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.02564102564102564, 'ndcg_50': 0.11521147633589655, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.02564102564102564, 'ndcg_100': 0.11521147633589655, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.02564102564102564, 'ndcg_200': 0.11521147633589655, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 1.0, 'ap_500': 0.015098417148531043, 'ndcg_500': 0.18503519530063914, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 1.0, 'ap_1000': 0.015098417148531043, 'ndcg_1000': 0.18503519530063914, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.015098417148531043, 'ndcg_2000': 0.18503519530063914, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U693663', 'user_index_in_cluster': 107, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 197, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.0022522522522522522, 'ndcg_500': 0.038551176841359856, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.003045511922671232, 'ndcg_1000': 0.07611915832039592, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.003045511922671232, 'ndcg_2000': 0.07611915832039592, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U297213', 'user_index_in_cluster': 108, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 232, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.125, 'ap_1000': 0.0011904761904761906, 'ndcg_1000': 0.026033727079351695, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.375, 'ap_2000': 0.0014553351118242652, 'ndcg_2000': 0.07373341771891483, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U434360', 'user_index_in_cluster': 109, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 113, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0009606147934678194, 'ndcg_2000': 0.038940111263984875, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:10:18,874 - INFO - Starting user U651670 in cluster 1 (index 110)
2025-04-03 19:10:31,536 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:10:33,393 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:10:36,416 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:10:49,434 - ERROR - Failed on user U135387 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:10:49,435 - INFO - Starting user U352669 in cluster 2 (index 181)
2025-04-03 19:11:07,821 - INFO - Starting user U10388 in cluster 1 (index 111)
2025-04-03 19:11:08,594 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:11:11,933 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U222297', 'user_index_in_cluster': 100, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U61455', 'user_index_in_cluster': 101, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 67, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U298725', 'user_index_in_cluster': 102, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 114, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0010471204188481676, 'ndcg_1000': 0.0619286376291035, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.001306645565727921, 'ndcg_2000': 0.12134401299949775, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U654323', 'user_index_in_cluster': 103, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 186, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.6, 'ap_500': 0.005585988535159248, 'ndcg_500': 0.12143550522889329, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.6, 'ap_1000': 0.005585988535159248, 'ndcg_1000': 0.12143550522889329, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.0048777226470679905, 'ndcg_2000': 0.15371860906460597, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U662232', 'user_index_in_cluster': 104, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 81, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U662056', 'user_index_in_cluster': 105, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 90, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U190662', 'user_index_in_cluster': 106, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 80, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022935779816513763, 'ndcg_500': 0.11400572314167466, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022935779816513763, 'ndcg_1000': 0.11400572314167466, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022935779816513763, 'ndcg_2000': 0.11400572314167466, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U79094', 'user_index_in_cluster': 107, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 73, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U163662', 'user_index_in_cluster': 108, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 109, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U165561', 'user_index_in_cluster': 109, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 125, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.25, 'ap_50': 0.047619047619047616, 'ndcg_50': 0.08754031530847717, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.25, 'ap_100': 0.047619047619047616, 'ndcg_100': 0.08754031530847717, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.047619047619047616, 'ndcg_200': 0.08754031530847717, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.047619047619047616, 'ndcg_500': 0.08754031530847717, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.02540952380952381, 'ndcg_1000': 0.12956176396474092, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.75, 'ap_2000': 0.017508187371973613, 'ndcg_2000': 0.16577055954943792, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:11:11,939 - INFO - Starting user U662360 in cluster 0 (index 110)
2025-04-03 19:11:23,207 - ERROR - Failed on user U352669 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:11:23,208 - INFO - Starting user U623790 in cluster 2 (index 182)
2025-04-03 19:11:26,303 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:11:29,913 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:11:40,830 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:11:57,659 - ERROR - Failed on user U623790 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:11:57,659 - INFO - Starting user U435874 in cluster 2 (index 183)
2025-04-03 19:12:00,021 - INFO - Starting user U255796 in cluster 1 (index 112)
2025-04-03 19:12:04,679 - INFO - Starting user U703073 in cluster 0 (index 111)
2025-04-03 19:12:15,035 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:12:17,468 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:12:22,034 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:12:28,204 - ERROR - Failed on user U435874 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:12:28,204 - INFO - Starting user U254859 in cluster 2 (index 184)
2025-04-03 19:12:47,235 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:12:50,049 - INFO - Starting user U408043 in cluster 1 (index 113)
2025-04-03 19:12:54,398 - INFO - Starting user U50964 in cluster 0 (index 112)
2025-04-03 19:13:05,631 - ERROR - Failed on user U254859 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:13:05,632 - INFO - Starting user U234536 in cluster 2 (index 185)
2025-04-03 19:13:07,446 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:13:11,636 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:13:23,328 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 19:13:38,944 - ERROR - Failed on user U234536 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:13:38,945 - INFO - Starting user U592983 in cluster 2 (index 186)
2025-04-03 19:13:41,284 - INFO - Starting user U47876 in cluster 1 (index 114)
2025-04-03 19:13:45,395 - INFO - Starting user U274437 in cluster 0 (index 113)
2025-04-03 19:13:56,462 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:13:59,594 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 19:14:03,000 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:14:09,428 - ERROR - Failed on user U592983 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:14:09,428 - INFO - Starting user U49834 in cluster 2 (index 187)
2025-04-03 19:14:28,627 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:14:35,907 - INFO - Starting user U681667 in cluster 0 (index 114)
2025-04-03 19:14:43,397 - ERROR - Failed on user U49834 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:14:43,397 - INFO - Starting user U506133 in cluster 2 (index 188)
2025-04-03 19:14:54,284 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:15:01,106 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:15:01,236 - INFO - Starting user U618218 in cluster 0 (index 115)
2025-04-03 19:15:07,258 - INFO - Starting user U612395 in cluster 1 (index 115)
2025-04-03 19:15:12,939 - ERROR - Failed on user U506133 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:15:12,940 - INFO - Starting user U169821 in cluster 2 (index 189)
2025-04-03 19:15:19,413 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:15:25,520 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:15:31,146 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:15:47,609 - ERROR - Failed on user U169821 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:15:47,609 - INFO - Starting user U385752 in cluster 2 (index 190)
2025-04-03 19:15:54,830 - INFO - Starting user U343066 in cluster 0 (index 116)
2025-04-03 19:15:58,848 - INFO - Starting user U641296 in cluster 1 (index 116)
2025-04-03 19:16:06,709 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:16:13,119 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 19:16:16,684 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:16:20,561 - ERROR - Failed on user U385752 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:16:20,562 - INFO - Starting user U321603 in cluster 2 (index 191)
2025-04-03 19:16:28,039 - INFO - Starting user U697494 in cluster 1 (index 117)
2025-04-03 19:16:39,299 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:16:50,708 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:16:52,802 - ERROR - Failed on user U321603 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:16:52,802 - INFO - Starting user U237594 in cluster 2 (index 192)
2025-04-03 19:17:11,001 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:17:18,886 - INFO - Starting user U576774 in cluster 1 (index 118)
2025-04-03 19:17:25,781 - ERROR - Failed on user U237594 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:17:25,782 - INFO - Starting user U100898 in cluster 2 (index 193)
2025-04-03 19:17:27,951 - INFO - Starting user U667601 in cluster 0 (index 117)
2025-04-03 19:17:37,578 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:17:47,639 - INFO - Starting user U241450 in cluster 1 (index 119)
2025-04-03 19:17:49,282 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:17:52,725 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:18:01,153 - ERROR - Failed on user U100898 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:18:01,154 - INFO - Starting user U215435 in cluster 2 (index 194)
2025-04-03 19:18:04,356 - INFO - Starting user U707922 in cluster 0 (index 118)
2025-04-03 19:18:05,609 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:18:14,752 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U651670', 'user_index_in_cluster': 110, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 268, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.0006711409395973154, 'ndcg_2000': 0.032172085829418856, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U10388', 'user_index_in_cluster': 111, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.025, 'ndcg_50': 0.08759200575528697, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.025, 'ndcg_100': 0.08759200575528697, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.025, 'ndcg_200': 0.08759200575528697, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.025, 'ndcg_500': 0.08759200575528697, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.025, 'ndcg_1000': 0.08759200575528697, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.013064334085778782, 'ndcg_2000': 0.13107604337052695, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U255796', 'user_index_in_cluster': 112, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 296, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.5, 'ap_10': 0.125, 'ndcg_10': 0.19342640361727081, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.5, 'ap_20': 0.125, 'ndcg_20': 0.19342640361727081, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.125, 'ndcg_50': 0.19342640361727081, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.125, 'ndcg_100': 0.19342640361727081, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.125, 'ndcg_200': 0.19342640361727081, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.125, 'ndcg_500': 0.19342640361727081, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.125, 'ndcg_1000': 0.19342640361727081, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.125, 'ndcg_2000': 0.19342640361727081, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U408043', 'user_index_in_cluster': 113, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 202, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U47876', 'user_index_in_cluster': 114, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 159, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U612395', 'user_index_in_cluster': 115, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 194, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U641296', 'user_index_in_cluster': 116, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 675, 'num_future_clicks': 16, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.0625, 'ap_500': 0.0037174721189591076, 'ndcg_500': 0.020276973627648697, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.0625, 'ap_1000': 0.0037174721189591076, 'ndcg_1000': 0.020276973627648697, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.25, 'ap_2000': 0.0025527675536749913, 'ndcg_2000': 0.06762950607325934, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U697494', 'user_index_in_cluster': 117, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 169, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.25, 'ap_500': 0.0032362149689304214, 'ndcg_500': 0.05723421658501648, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.375, 'ap_1000': 0.004133761230933852, 'ndcg_1000': 0.08538324308035788, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.375, 'ap_2000': 0.004133761230933852, 'ndcg_2000': 0.08538324308035788, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U576774', 'user_index_in_cluster': 118, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 164, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.3333333333333333, 'ap_20': 0.07142857142857142, 'ndcg_20': 0.12011565579805131, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.07142857142857142, 'ndcg_50': 0.12011565579805131, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.07142857142857142, 'ndcg_100': 0.12011565579805131, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.07142857142857142, 'ndcg_200': 0.12011565579805131, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.07142857142857142, 'ndcg_500': 0.12011565579805131, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.07142857142857142, 'ndcg_1000': 0.12011565579805131, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.036447423544197735, 'ndcg_2000': 0.1651749845629099, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U241450', 'user_index_in_cluster': 119, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 194, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:18:14,759 - INFO - Starting user U567136 in cluster 1 (index 120)
2025-04-03 19:18:19,466 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:18:23,684 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:18:32,873 - ERROR - Failed on user U215435 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:18:32,873 - INFO - Starting user U214519 in cluster 2 (index 195)
2025-04-03 19:18:34,179 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:18:35,141 - INFO - Starting user U512245 in cluster 0 (index 119)
2025-04-03 19:18:41,952 - INFO - Starting user U144891 in cluster 1 (index 121)
2025-04-03 19:18:50,589 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:18:53,096 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:18:59,523 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 19:19:04,049 - ERROR - Failed on user U214519 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:19:04,049 - INFO - Starting user U66184 in cluster 2 (index 196)
2025-04-03 19:19:22,109 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:19:25,528 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U662360', 'user_index_in_cluster': 110, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 151, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U703073', 'user_index_in_cluster': 111, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 111, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U50964', 'user_index_in_cluster': 112, 'num_candidates': 4525, 'num_history_articles': 41, 'original_history_len': 41, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U274437', 'user_index_in_cluster': 113, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 150, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U681667', 'user_index_in_cluster': 114, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 98, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.02564102564102564, 'ndcg_50': 0.06372882143962796, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.02564102564102564, 'ndcg_100': 0.06372882143962796, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.02564102564102564, 'ndcg_200': 0.06372882143962796, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.02564102564102564, 'ndcg_500': 0.06372882143962796, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.02564102564102564, 'ndcg_1000': 0.06372882143962796, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.02564102564102564, 'ndcg_2000': 0.06372882143962796, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U618218', 'user_index_in_cluster': 115, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U343066', 'user_index_in_cluster': 116, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 55, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2857142857142857, 'ap_500': 0.004640596063568629, 'ndcg_500': 0.06681788475198602, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.42857142857142855, 'ap_1000': 0.004243155996402075, 'ndcg_1000': 0.09496261615930088, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.7142857142857143, 'ap_2000': 0.003708310828053626, 'ndcg_2000': 0.14689718052163578, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U667601', 'user_index_in_cluster': 117, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.02702702702702703, 'ndcg_50': 0.08942172406547305, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.02702702702702703, 'ndcg_100': 0.08942172406547305, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.02702702702702703, 'ndcg_200': 0.08942172406547305, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.02702702702702703, 'ndcg_500': 0.08942172406547305, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.014969117589204927, 'ndcg_1000': 0.13920588971694878, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.014969117589204927, 'ndcg_2000': 0.13920588971694878, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U707922', 'user_index_in_cluster': 118, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 113, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U512245', 'user_index_in_cluster': 119, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 51, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:19:25,533 - INFO - Starting user U551595 in cluster 0 (index 120)
2025-04-03 19:19:30,585 - INFO - Starting user U254570 in cluster 1 (index 122)
2025-04-03 19:19:41,719 - ERROR - Failed on user U66184 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:19:41,720 - INFO - Starting user U708782 in cluster 2 (index 197)
2025-04-03 19:19:41,807 - INFO - history_titles:[] empty for user: U708782!!!
2025-04-03 19:19:43,829 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:19:47,873 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:19:59,745 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:20:16,097 - ERROR - Failed on user U708782 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:20:16,098 - INFO - Starting user U96628 in cluster 2 (index 198)
2025-04-03 19:20:18,258 - INFO - Starting user U595390 in cluster 0 (index 121)
2025-04-03 19:20:22,319 - INFO - Starting user U104751 in cluster 1 (index 123)
2025-04-03 19:20:33,523 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:20:36,060 - INFO - lens: newsdf:101527candidate_pool:5905,candidate_pool:5905
2025-04-03 19:20:41,085 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:20:46,686 - ERROR - Failed on user U96628 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:20:46,686 - INFO - Starting user U600045 in cluster 2 (index 199)
2025-04-03 19:21:05,557 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:21:12,061 - INFO - Starting user U339804 in cluster 1 (index 124)
2025-04-03 19:21:19,404 - ERROR - Failed on user U600045 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:21:19,404 - INFO - Starting user U709210 in cluster 2 (index 200)
2025-04-03 19:21:29,911 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:21:37,393 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:21:50,232 - ERROR - Failed on user U709210 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:21:50,232 - INFO - Starting user U433421 in cluster 2 (index 201)
2025-04-03 19:21:50,690 - INFO - Starting user U255870 in cluster 1 (index 125)
2025-04-03 19:21:54,364 - INFO - Starting user U323740 in cluster 0 (index 122)
2025-04-03 19:22:07,636 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:22:08,118 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:22:12,384 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 19:22:21,311 - ERROR - Failed on user U433421 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:22:21,312 - INFO - Starting user U183527 in cluster 2 (index 202)
2025-04-03 19:22:39,315 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:22:48,826 - INFO - Starting user U325455 in cluster 0 (index 123)
2025-04-03 19:22:52,626 - INFO - Starting user U665939 in cluster 1 (index 126)
2025-04-03 19:22:54,749 - ERROR - Failed on user U183527 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:22:54,750 - INFO - Starting user U110230 in cluster 2 (index 203)
2025-04-03 19:23:07,773 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:23:10,642 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:23:13,410 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:23:30,915 - ERROR - Failed on user U110230 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:23:30,915 - INFO - Starting user U598618 in cluster 2 (index 204)
2025-04-03 19:23:49,179 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:23:50,258 - INFO - Starting user U83650 in cluster 0 (index 124)
2025-04-03 19:23:54,752 - INFO - Starting user U230361 in cluster 1 (index 127)
2025-04-03 19:24:07,622 - ERROR - Failed on user U598618 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:24:07,622 - INFO - Starting user U209593 in cluster 2 (index 205)
2025-04-03 19:24:08,449 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:24:12,771 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:24:25,402 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:24:42,684 - INFO - Starting user U437199 in cluster 0 (index 125)
2025-04-03 19:24:44,513 - ERROR - Failed on user U209593 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:24:44,513 - INFO - Starting user U587096 in cluster 2 (index 206)
2025-04-03 19:24:46,639 - INFO - Starting user U532883 in cluster 1 (index 128)
2025-04-03 19:25:00,530 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 19:25:02,501 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:25:04,763 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 19:25:20,766 - ERROR - Failed on user U587096 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:25:20,767 - INFO - Starting user U613723 in cluster 2 (index 207)
2025-04-03 19:25:34,800 - INFO - Starting user U663935 in cluster 0 (index 126)
2025-04-03 19:25:39,204 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:25:40,282 - INFO - Starting user U179767 in cluster 1 (index 129)
2025-04-03 19:25:51,523 - ERROR - Failed on user U613723 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:25:51,523 - INFO - Starting user U160802 in cluster 2 (index 208)
2025-04-03 19:25:54,169 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:25:57,976 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 19:26:09,816 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:26:27,856 - ERROR - Failed on user U160802 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:26:27,857 - INFO - Starting user U209103 in cluster 2 (index 209)
2025-04-03 19:26:32,641 - INFO - Starting user U92865 in cluster 0 (index 127)
2025-04-03 19:26:35,092 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U567136', 'user_index_in_cluster': 120, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 160, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U144891', 'user_index_in_cluster': 121, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 163, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U254570', 'user_index_in_cluster': 122, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 163, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U104751', 'user_index_in_cluster': 123, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 304, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.16666666666666666, 'ap_2000': 0.0007468259895444362, 'ndcg_2000': 0.02912994897743469, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U339804', 'user_index_in_cluster': 124, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 212, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0038461538461538464, 'ndcg_500': 0.07637697713792865, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0038461538461538464, 'ndcg_1000': 0.07637697713792865, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0026795065751192833, 'ndcg_2000': 0.1355062926060454, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U255870', 'user_index_in_cluster': 125, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 287, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.005813953488372093, 'ndcg_200': 0.13450571694798438, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.005813953488372093, 'ndcg_500': 0.13450571694798438, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.005813953488372093, 'ndcg_1000': 0.13450571694798438, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.005813953488372093, 'ndcg_2000': 0.13450571694798438, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U665939', 'user_index_in_cluster': 126, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 296, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006373486297004461, 'ndcg_2000': 0.05775390871250463, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U230361', 'user_index_in_cluster': 127, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 128, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U532883', 'user_index_in_cluster': 128, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 228, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.0021691973969631237, 'ndcg_500': 0.03831561465273523, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0021691973969631237, 'ndcg_1000': 0.03831561465273523, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.0021691973969631237, 'ndcg_2000': 0.03831561465273523, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U179767', 'user_index_in_cluster': 129, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 102, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:26:35,099 - INFO - Starting user U608243 in cluster 1 (index 130)
2025-04-03 19:26:46,478 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:26:51,346 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 19:26:54,387 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:27:00,300 - ERROR - Failed on user U209103 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:27:00,300 - INFO - Starting user U295822 in cluster 2 (index 210)
2025-04-03 19:27:18,394 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:27:23,542 - INFO - Starting user U289557 in cluster 0 (index 128)
2025-04-03 19:27:31,767 - INFO - Starting user U264027 in cluster 1 (index 131)
2025-04-03 19:27:35,432 - ERROR - Failed on user U295822 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:27:35,432 - INFO - Starting user U579568 in cluster 2 (index 211)
2025-04-03 19:27:41,071 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 19:27:49,660 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:27:53,401 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:28:08,834 - ERROR - Failed on user U579568 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:28:08,835 - INFO - Starting user U302484 in cluster 2 (index 212)
2025-04-03 19:28:16,078 - INFO - Starting user U176387 in cluster 0 (index 129)
2025-04-03 19:28:19,634 - INFO - Starting user U263201 in cluster 1 (index 132)
2025-04-03 19:28:27,623 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 19:28:35,240 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:28:37,319 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 19:28:41,054 - ERROR - Failed on user U302484 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:28:41,055 - INFO - Starting user U571800 in cluster 2 (index 213)
2025-04-03 19:28:58,657 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:29:07,991 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U551595', 'user_index_in_cluster': 120, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 89, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U595390', 'user_index_in_cluster': 121, 'num_candidates': 4515, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U323740', 'user_index_in_cluster': 122, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 85, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U325455', 'user_index_in_cluster': 123, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 99, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U83650', 'user_index_in_cluster': 124, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 127, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U437199', 'user_index_in_cluster': 125, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 108, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U663935', 'user_index_in_cluster': 126, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 109, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.023255813953488372, 'ndcg_50': 0.11230971199864632, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.023255813953488372, 'ndcg_100': 0.11230971199864632, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.023255813953488372, 'ndcg_200': 0.11230971199864632, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.023255813953488372, 'ndcg_500': 0.11230971199864632, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.023255813953488372, 'ndcg_1000': 0.11230971199864632, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.023255813953488372, 'ndcg_2000': 0.11230971199864632, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U92865', 'user_index_in_cluster': 127, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 68, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U289557', 'user_index_in_cluster': 128, 'num_candidates': 4518, 'num_history_articles': 46, 'original_history_len': 46, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U176387', 'user_index_in_cluster': 129, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 278, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:29:07,996 - INFO - Starting user U636676 in cluster 0 (index 130)
2025-04-03 19:29:12,479 - INFO - Starting user U173549 in cluster 1 (index 133)
2025-04-03 19:29:14,345 - ERROR - Failed on user U571800 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:29:14,345 - INFO - Starting user U326347 in cluster 2 (index 214)
2025-04-03 19:29:25,152 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:29:30,601 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:29:31,718 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:29:46,834 - ERROR - Failed on user U326347 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:29:46,834 - INFO - Starting user U265792 in cluster 2 (index 215)
2025-04-03 19:30:01,029 - INFO - Starting user U486027 in cluster 1 (index 134)
2025-04-03 19:30:04,705 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:30:04,735 - INFO - Starting user U554734 in cluster 0 (index 131)
2025-04-03 19:30:16,553 - ERROR - Failed on user U265792 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:30:16,554 - INFO - Starting user U26530 in cluster 2 (index 216)
2025-04-03 19:30:18,438 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:30:22,578 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:30:35,353 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:30:50,762 - ERROR - Failed on user U26530 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:30:50,763 - INFO - Starting user U667488 in cluster 2 (index 217)
2025-04-03 19:30:53,420 - INFO - Starting user U46840 in cluster 1 (index 135)
2025-04-03 19:30:57,097 - INFO - Starting user U62456 in cluster 0 (index 132)
2025-04-03 19:31:09,034 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:31:11,556 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:31:15,708 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 19:31:22,138 - ERROR - Failed on user U667488 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:31:22,138 - INFO - Starting user U535002 in cluster 2 (index 218)
2025-04-03 19:31:41,147 - INFO - lens: newsdf:101527candidate_pool:5906,candidate_pool:5906
2025-04-03 19:31:43,967 - INFO - Starting user U537689 in cluster 1 (index 136)
2025-04-03 19:31:47,986 - INFO - Starting user U180810 in cluster 0 (index 133)
2025-04-03 19:31:59,146 - ERROR - Failed on user U535002 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:31:59,146 - INFO - Starting user U517139 in cluster 2 (index 219)
2025-04-03 19:32:02,960 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:32:06,125 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:32:17,633 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:32:36,037 - ERROR - Failed on user U517139 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:32:36,037 - INFO - Starting user U310029 in cluster 2 (index 220)
2025-04-03 19:32:38,776 - INFO - Starting user U549037 in cluster 1 (index 137)
2025-04-03 19:32:44,560 - INFO - Starting user U522244 in cluster 0 (index 134)
2025-04-03 19:32:54,240 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:32:56,738 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:33:02,542 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:33:07,289 - ERROR - Failed on user U310029 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:33:07,290 - INFO - Starting user U555991 in cluster 2 (index 221)
2025-04-03 19:33:26,888 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:33:29,697 - INFO - Starting user U133082 in cluster 1 (index 138)
2025-04-03 19:33:33,450 - INFO - Starting user U272768 in cluster 0 (index 135)
2025-04-03 19:33:45,206 - ERROR - Failed on user U555991 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:33:45,206 - INFO - Starting user U142224 in cluster 2 (index 222)
2025-04-03 19:33:47,351 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:33:51,180 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:34:03,515 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:34:18,936 - ERROR - Failed on user U142224 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:34:18,936 - INFO - Starting user U575673 in cluster 2 (index 223)
2025-04-03 19:34:22,508 - INFO - Starting user U356529 in cluster 1 (index 139)
2025-04-03 19:34:26,264 - INFO - Starting user U340271 in cluster 0 (index 136)
2025-04-03 19:34:37,367 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:34:42,645 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:34:45,695 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 19:34:50,679 - ERROR - Failed on user U575673 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:34:50,680 - INFO - Starting user U130963 in cluster 2 (index 224)
2025-04-03 19:35:10,033 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:35:26,235 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U608243', 'user_index_in_cluster': 130, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 133, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.14285714285714285, 'ap_10': 0.1, 'ndcg_10': 0.07945707943276742, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.14285714285714285, 'ap_20': 0.1, 'ndcg_20': 0.07945707943276742, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.14285714285714285, 'ap_50': 0.1, 'ndcg_50': 0.07945707943276742, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.14285714285714285, 'ap_100': 0.1, 'ndcg_100': 0.07945707943276742, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.14285714285714285, 'ap_200': 0.1, 'ndcg_200': 0.07945707943276742, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2857142857142857, 'ap_500': 0.05214132762312634, 'ndcg_500': 0.11044524526403668, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.42857142857142855, 'ap_1000': 0.03621437345417725, 'ndcg_1000': 0.13959944895155335, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.42857142857142855, 'ap_2000': 0.03621437345417725, 'ndcg_2000': 0.13959944895155335, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U264027', 'user_index_in_cluster': 131, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 169, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.42857142857142855, 'ap_2000': 0.0014405603620881082, 'ndcg_2000': 0.0796382240763891, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U263201', 'user_index_in_cluster': 132, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 265, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0010169353282242352, 'ndcg_2000': 0.057613941252542857, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U173549', 'user_index_in_cluster': 133, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 122, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U486027', 'user_index_in_cluster': 134, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 201, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U46840', 'user_index_in_cluster': 135, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 201, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U537689', 'user_index_in_cluster': 136, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 207, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U549037', 'user_index_in_cluster': 137, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U133082', 'user_index_in_cluster': 138, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 172, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.003816793893129771, 'ndcg_500': 0.04856126184529052, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.003108877138641716, 'ndcg_1000': 0.0887904383862035, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.003108877138641716, 'ndcg_2000': 0.0887904383862035, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U356529', 'user_index_in_cluster': 139, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 223, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:35:26,240 - INFO - Starting user U586652 in cluster 1 (index 140)
2025-04-03 19:35:28,328 - ERROR - Failed on user U130963 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:35:28,329 - INFO - Starting user U533647 in cluster 2 (index 225)
2025-04-03 19:35:30,960 - INFO - Starting user U8318 in cluster 0 (index 137)
2025-04-03 19:35:44,305 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:35:46,290 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:35:48,859 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 19:36:01,545 - ERROR - Failed on user U533647 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:36:01,545 - INFO - Starting user U680579 in cluster 2 (index 226)
2025-04-03 19:36:01,649 - INFO - history_titles:[] empty for user: U680579!!!
2025-04-03 19:36:15,973 - INFO - Starting user U6203 in cluster 1 (index 141)
2025-04-03 19:36:19,714 - INFO - Starting user U23755 in cluster 0 (index 138)
2025-04-03 19:36:20,231 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:36:31,864 - ERROR - Failed on user U680579 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:36:31,864 - INFO - Starting user U691521 in cluster 2 (index 227)
2025-04-03 19:36:35,647 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:36:37,526 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:36:51,011 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:37:06,075 - ERROR - Failed on user U691521 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:37:06,075 - INFO - Starting user U476983 in cluster 2 (index 228)
2025-04-03 19:37:07,986 - INFO - Starting user U420180 in cluster 1 (index 142)
2025-04-03 19:37:11,924 - INFO - Starting user U97170 in cluster 0 (index 139)
2025-04-03 19:37:23,905 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:37:26,841 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:37:30,444 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:37:36,914 - ERROR - Failed on user U476983 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:37:36,914 - INFO - Starting user U185732 in cluster 2 (index 229)
2025-04-03 19:37:55,824 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:37:58,664 - INFO - Starting user U641298 in cluster 1 (index 143)
2025-04-03 19:38:02,782 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U636676', 'user_index_in_cluster': 130, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 111, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U554734', 'user_index_in_cluster': 131, 'num_candidates': 4523, 'num_history_articles': 45, 'original_history_len': 45, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U62456', 'user_index_in_cluster': 132, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U180810', 'user_index_in_cluster': 133, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.04, 'recall_50': 0.3333333333333333, 'ap_50': 0.04195804195804196, 'ndcg_50': 0.11874052687744659, 'num_recommendations_50': 50, 'precision_100': 0.02, 'recall_100': 0.3333333333333333, 'ap_100': 0.04195804195804196, 'ndcg_100': 0.11874052687744659, 'num_recommendations_100': 100, 'precision_200': 0.01, 'recall_200': 0.3333333333333333, 'ap_200': 0.04195804195804196, 'ndcg_200': 0.11874052687744659, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.3333333333333333, 'ap_500': 0.04195804195804196, 'ndcg_500': 0.11874052687744659, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.04195804195804196, 'ndcg_1000': 0.11874052687744659, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.5, 'ap_2000': 0.028625195836169057, 'ndcg_2000': 0.1473386439961907, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U522244', 'user_index_in_cluster': 134, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 104, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.0006666666666666666, 'ndcg_2000': 0.03214268219556734, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U272768', 'user_index_in_cluster': 135, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 59, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006341154090044388, 'ndcg_2000': 0.09412755601048532, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U340271', 'user_index_in_cluster': 136, 'num_candidates': 4520, 'num_history_articles': 46, 'original_history_len': 46, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U8318', 'user_index_in_cluster': 137, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U23755', 'user_index_in_cluster': 138, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0022727272727272726, 'ndcg_500': 0.06979768693237524, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0022727272727272726, 'ndcg_1000': 0.06979768693237524, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0022727272727272726, 'ndcg_2000': 0.06979768693237524, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U97170', 'user_index_in_cluster': 139, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:38:02,786 - INFO - Starting user U146919 in cluster 0 (index 140)
2025-04-03 19:38:14,444 - ERROR - Failed on user U185732 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:38:14,444 - INFO - Starting user U481573 in cluster 2 (index 230)
2025-04-03 19:38:16,480 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:38:20,827 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 19:38:34,553 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:38:52,383 - INFO - Starting user U180489 in cluster 1 (index 144)
2025-04-03 19:38:53,778 - ERROR - Failed on user U481573 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:38:53,778 - INFO - Starting user U99586 in cluster 2 (index 231)
2025-04-03 19:38:56,466 - INFO - Starting user U553694 in cluster 0 (index 141)
2025-04-03 19:39:10,517 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:39:11,759 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:39:14,538 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 19:39:28,105 - ERROR - Failed on user U99586 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:39:28,106 - INFO - Starting user U496122 in cluster 2 (index 232)
2025-04-03 19:39:43,122 - INFO - Starting user U123235 in cluster 1 (index 145)
2025-04-03 19:39:46,807 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:39:47,856 - INFO - Starting user U132084 in cluster 0 (index 142)
2025-04-03 19:39:59,193 - ERROR - Failed on user U496122 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:39:59,194 - INFO - Starting user U20822 in cluster 2 (index 233)
2025-04-03 19:40:05,390 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:40:10,070 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:40:17,880 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:40:34,341 - ERROR - Failed on user U20822 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:40:34,341 - INFO - Starting user U528072 in cluster 2 (index 234)
2025-04-03 19:40:40,853 - INFO - Starting user U243782 in cluster 1 (index 146)
2025-04-03 19:40:44,880 - INFO - Starting user U171084 in cluster 0 (index 143)
2025-04-03 19:40:52,909 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:40:59,130 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:41:03,263 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:41:06,155 - ERROR - Failed on user U528072 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:41:06,155 - INFO - Starting user U601272 in cluster 2 (index 235)
2025-04-03 19:41:25,513 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:41:33,573 - INFO - Starting user U646622 in cluster 1 (index 147)
2025-04-03 19:41:37,002 - INFO - Starting user U298870 in cluster 0 (index 144)
2025-04-03 19:41:42,932 - ERROR - Failed on user U601272 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:41:42,932 - INFO - Starting user U293078 in cluster 2 (index 236)
2025-04-03 19:41:51,305 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:41:55,114 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:42:01,989 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:42:18,413 - ERROR - Failed on user U293078 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:42:18,414 - INFO - Starting user U679117 in cluster 2 (index 237)
2025-04-03 19:42:24,988 - INFO - Starting user U148881 in cluster 1 (index 148)
2025-04-03 19:42:30,325 - INFO - Starting user U192980 in cluster 0 (index 145)
2025-04-03 19:42:37,029 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:42:44,877 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:42:49,614 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:42:49,918 - ERROR - Failed on user U679117 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:42:49,919 - INFO - Starting user U700851 in cluster 2 (index 238)
2025-04-03 19:43:08,810 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:43:18,666 - INFO - Starting user U625490 in cluster 1 (index 149)
2025-04-03 19:43:22,714 - INFO - Starting user U226273 in cluster 0 (index 146)
2025-04-03 19:43:24,663 - ERROR - Failed on user U700851 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:43:24,664 - INFO - Starting user U298691 in cluster 2 (index 239)
2025-04-03 19:43:36,639 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:43:41,470 - INFO - lens: newsdf:101527candidate_pool:5906,candidate_pool:5906
2025-04-03 19:43:43,786 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:44:03,147 - ERROR - Failed on user U298691 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:44:03,148 - INFO - Starting user U116976 in cluster 2 (index 240)
2025-04-03 19:44:13,299 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U586652', 'user_index_in_cluster': 140, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 221, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U6203', 'user_index_in_cluster': 141, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 172, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U420180', 'user_index_in_cluster': 142, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 231, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U641298', 'user_index_in_cluster': 143, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 154, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U180489', 'user_index_in_cluster': 144, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 159, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.16666666666666666, 'ap_10': 0.125, 'ndcg_10': 0.09546043308946733, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.16666666666666666, 'ap_20': 0.125, 'ndcg_20': 0.09546043308946733, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.16666666666666666, 'ap_50': 0.125, 'ndcg_50': 0.09546043308946733, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.16666666666666666, 'ap_100': 0.125, 'ndcg_100': 0.09546043308946733, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.16666666666666666, 'ap_200': 0.125, 'ndcg_200': 0.09546043308946733, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.3333333333333333, 'ap_500': 0.06636100386100387, 'ndcg_500': 0.13318027099161164, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.06636100386100387, 'ndcg_1000': 0.13318027099161164, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.5, 'ap_2000': 0.044876397149124427, 'ndcg_2000': 0.16167331713101712, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U123235', 'user_index_in_cluster': 145, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U243782', 'user_index_in_cluster': 146, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 224, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.002257336343115124, 'ndcg_500': 0.04438953717049075, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.002257336343115124, 'ndcg_1000': 0.04438953717049075, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 1.0, 'ap_2000': 0.0022145681151539156, 'ndcg_2000': 0.1574923908493296, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U646622', 'user_index_in_cluster': 147, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 336, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U148881', 'user_index_in_cluster': 148, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 156, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0011001100110011, 'ndcg_1000': 0.03971424835806256, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0011001100110011, 'ndcg_2000': 0.03971424835806256, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U625490', 'user_index_in_cluster': 149, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 114, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:44:13,303 - INFO - Starting user U37802 in cluster 1 (index 150)
2025-04-03 19:44:21,171 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:44:31,647 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:44:33,330 - ERROR - Failed on user U116976 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:44:33,330 - INFO - Starting user U281884 in cluster 2 (index 241)
2025-04-03 19:44:52,358 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:45:00,336 - INFO - Starting user U554188 in cluster 1 (index 151)
2025-04-03 19:45:08,026 - ERROR - Failed on user U281884 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:45:08,026 - INFO - Starting user U127074 in cluster 2 (index 242)
2025-04-03 19:45:08,351 - INFO - Starting user U271601 in cluster 0 (index 147)
2025-04-03 19:45:19,813 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:45:27,702 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:45:28,370 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:45:40,024 - ERROR - Failed on user U127074 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:45:40,024 - INFO - Starting user U552061 in cluster 2 (index 243)
2025-04-03 19:45:46,591 - INFO - Starting user U102844 in cluster 1 (index 152)
2025-04-03 19:45:51,600 - INFO - Starting user U673092 in cluster 0 (index 148)
2025-04-03 19:45:58,293 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:46:05,559 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:46:11,515 - ERROR - Failed on user U552061 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:46:11,516 - INFO - Starting user U179968 in cluster 2 (index 244)
2025-04-03 19:46:12,876 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:46:30,739 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:46:40,369 - INFO - Starting user U559100 in cluster 1 (index 153)
2025-04-03 19:46:44,596 - INFO - Starting user U311138 in cluster 0 (index 149)
2025-04-03 19:46:46,123 - ERROR - Failed on user U179968 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:46:46,124 - INFO - Starting user U295157 in cluster 2 (index 245)
2025-04-03 19:46:58,425 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:47:02,149 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:47:04,114 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:47:21,101 - ERROR - Failed on user U295157 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:47:21,101 - INFO - Starting user U211818 in cluster 2 (index 246)
2025-04-03 19:47:31,957 - INFO - Starting user U611442 in cluster 1 (index 154)
2025-04-03 19:47:36,016 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U146919', 'user_index_in_cluster': 140, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U553694', 'user_index_in_cluster': 141, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 107, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U132084', 'user_index_in_cluster': 142, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 111, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U171084', 'user_index_in_cluster': 143, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U298870', 'user_index_in_cluster': 144, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U192980', 'user_index_in_cluster': 145, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U226273', 'user_index_in_cluster': 146, 'num_candidates': 4516, 'num_history_articles': 50, 'original_history_len': 75, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U271601', 'user_index_in_cluster': 147, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 89, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U673092', 'user_index_in_cluster': 148, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U311138', 'user_index_in_cluster': 149, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:47:36,021 - INFO - Starting user U208395 in cluster 0 (index 150)
2025-04-03 19:47:38,877 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:47:49,505 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:47:50,618 - ERROR - Failed on user U211818 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:47:50,618 - INFO - Starting user U548066 in cluster 2 (index 247)
2025-04-03 19:47:53,648 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:48:08,831 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:48:23,722 - INFO - Starting user U516671 in cluster 1 (index 155)
2025-04-03 19:48:25,200 - ERROR - Failed on user U548066 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:48:25,201 - INFO - Starting user U203596 in cluster 2 (index 248)
2025-04-03 19:48:28,263 - INFO - Starting user U2135 in cluster 0 (index 151)
2025-04-03 19:48:44,069 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:48:46,027 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:48:48,155 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:48:58,131 - ERROR - Failed on user U203596 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:48:58,131 - INFO - Starting user U149612 in cluster 2 (index 249)
2025-04-03 19:49:16,552 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:49:21,501 - INFO - Starting user U283721 in cluster 1 (index 156)
2025-04-03 19:49:27,445 - INFO - Starting user U514647 in cluster 0 (index 152)
2025-04-03 19:49:32,899 - ERROR - Failed on user U149612 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:49:32,899 - INFO - Starting user U446031 in cluster 2 (index 250)
2025-04-03 19:49:39,235 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:49:46,087 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 19:49:51,742 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:50:08,202 - ERROR - Failed on user U446031 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:50:08,203 - INFO - Starting user U32908 in cluster 2 (index 251)
2025-04-03 19:50:14,793 - INFO - Starting user U464570 in cluster 1 (index 157)
2025-04-03 19:50:19,415 - INFO - Starting user U710956 in cluster 0 (index 153)
2025-04-03 19:50:26,791 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:50:32,832 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:50:37,853 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 19:50:39,950 - ERROR - Failed on user U32908 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:50:39,950 - INFO - Starting user U333455 in cluster 2 (index 252)
2025-04-03 19:50:57,970 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:51:08,170 - INFO - Starting user U398536 in cluster 1 (index 158)
2025-04-03 19:51:13,316 - INFO - Starting user U425888 in cluster 0 (index 154)
2025-04-03 19:51:13,917 - ERROR - Failed on user U333455 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:51:13,918 - INFO - Starting user U93648 in cluster 2 (index 253)
2025-04-03 19:51:29,667 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:51:31,629 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:51:33,149 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:51:47,754 - ERROR - Failed on user U93648 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:51:47,754 - INFO - Starting user U634948 in cluster 2 (index 254)
2025-04-03 19:52:02,207 - INFO - Starting user U628334 in cluster 1 (index 159)
2025-04-03 19:52:06,429 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:52:07,055 - INFO - Starting user U378228 in cluster 0 (index 155)
2025-04-03 19:52:18,197 - ERROR - Failed on user U634948 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:52:18,198 - INFO - Starting user U409091 in cluster 2 (index 255)
2025-04-03 19:52:20,018 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 19:52:24,786 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:52:36,534 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:52:52,005 - ERROR - Failed on user U409091 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:52:52,005 - INFO - Starting user U257387 in cluster 2 (index 256)
2025-04-03 19:52:54,915 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U37802', 'user_index_in_cluster': 150, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U554188', 'user_index_in_cluster': 151, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 9, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2222222222222222, 'ap_500': 0.0032716138675924584, 'ndcg_500': 0.05321350365506083, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.004119060407852337, 'ndcg_1000': 0.07928905453588474, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.5555555555555556, 'ap_2000': 0.003655807429082587, 'ndcg_2000': 0.12378370530985824, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U102844', 'user_index_in_cluster': 152, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 275, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U559100', 'user_index_in_cluster': 153, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 263, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.0011876484560570072, 'ndcg_1000': 0.031133893197770303, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.5, 'ap_2000': 0.0016505388352464475, 'ndcg_2000': 0.08946367307216144, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U611442', 'user_index_in_cluster': 154, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 140, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U516671', 'user_index_in_cluster': 155, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 147, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U283721', 'user_index_in_cluster': 156, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 144, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.125, 'ap_1000': 0.0015290519877675841, 'ndcg_1000': 0.02703722181675472, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.25, 'ap_2000': 0.00173258020491961, 'ndcg_2000': 0.05229607798920493, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U464570', 'user_index_in_cluster': 157, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 593, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U398536', 'user_index_in_cluster': 158, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 154, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U628334', 'user_index_in_cluster': 159, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.0022371364653243847, 'ndcg_500': 0.0343579220068627, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.0022371364653243847, 'ndcg_1000': 0.0343579220068627, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.5, 'ap_2000': 0.0021135173416988937, 'ndcg_2000': 0.09355569479482997, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:52:54,924 - INFO - Starting user U456929 in cluster 1 (index 160)
2025-04-03 19:52:58,481 - INFO - Starting user U446689 in cluster 0 (index 156)
2025-04-03 19:53:09,878 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:53:12,742 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:53:16,183 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:53:22,844 - ERROR - Failed on user U257387 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:53:22,844 - INFO - Starting user U695488 in cluster 2 (index 257)
2025-04-03 19:53:41,603 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:53:44,622 - INFO - Starting user U592302 in cluster 1 (index 161)
2025-04-03 19:53:48,566 - INFO - Starting user U484260 in cluster 0 (index 157)
2025-04-03 19:54:00,167 - ERROR - Failed on user U695488 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:54:00,168 - INFO - Starting user U358046 in cluster 2 (index 258)
2025-04-03 19:54:03,176 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:54:07,239 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:54:18,761 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:54:34,724 - ERROR - Failed on user U358046 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:54:34,724 - INFO - Starting user U175023 in cluster 2 (index 259)
2025-04-03 19:54:37,901 - INFO - Starting user U407597 in cluster 1 (index 162)
2025-04-03 19:54:41,318 - INFO - Starting user U685966 in cluster 0 (index 158)
2025-04-03 19:54:52,502 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:54:55,859 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:54:59,139 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:55:05,480 - ERROR - Failed on user U175023 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:55:05,480 - INFO - Starting user U340957 in cluster 2 (index 260)
2025-04-03 19:55:24,420 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:55:28,534 - INFO - Starting user U39405 in cluster 1 (index 163)
2025-04-03 19:55:34,395 - INFO - Starting user U43828 in cluster 0 (index 159)
2025-04-03 19:55:39,771 - ERROR - Failed on user U340957 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:55:39,771 - INFO - Starting user U190640 in cluster 2 (index 261)
2025-04-03 19:55:46,215 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:55:51,878 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:55:57,679 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:56:13,728 - ERROR - Failed on user U190640 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:56:13,729 - INFO - Starting user U236337 in cluster 2 (index 262)
2025-04-03 19:56:20,784 - INFO - Starting user U118486 in cluster 1 (index 164)
2025-04-03 19:56:24,542 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U208395', 'user_index_in_cluster': 150, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 193, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U2135', 'user_index_in_cluster': 151, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U514647', 'user_index_in_cluster': 152, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 67, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U710956', 'user_index_in_cluster': 153, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 90, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U425888', 'user_index_in_cluster': 154, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 81, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.001827912048846806, 'ndcg_1000': 0.08112832489324248, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.75, 'ap_2000': 0.0018713495469248505, 'ndcg_2000': 0.11801878153885524, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U378228', 'user_index_in_cluster': 155, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 157, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U446689', 'user_index_in_cluster': 156, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 63, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0019801980198019802, 'ndcg_1000': 0.11132146446336848, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0019801980198019802, 'ndcg_2000': 0.11132146446336848, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U484260', 'user_index_in_cluster': 157, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U685966', 'user_index_in_cluster': 158, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 75, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U43828', 'user_index_in_cluster': 159, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 170, 'num_future_clicks': 9, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.1111111111111111, 'ap_500': 0.0037174721189591076, 'ndcg_500': 0.029101264798011468, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.2222222222222222, 'ap_1000': 0.003046384515536561, 'ndcg_1000': 0.05328442628528931, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0025694259721239974, 'ndcg_2000': 0.0749285900582367, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 19:56:24,547 - INFO - Starting user U448351 in cluster 0 (index 160)
2025-04-03 19:56:32,331 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:56:39,772 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:56:42,628 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:56:46,159 - ERROR - Failed on user U236337 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:56:46,159 - INFO - Starting user U185844 in cluster 2 (index 263)
2025-04-03 19:57:04,307 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:57:11,482 - INFO - Starting user U174774 in cluster 1 (index 165)
2025-04-03 19:57:17,962 - INFO - Starting user U497160 in cluster 0 (index 161)
2025-04-03 19:57:23,112 - ERROR - Failed on user U185844 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:57:23,113 - INFO - Starting user U577819 in cluster 2 (index 264)
2025-04-03 19:57:29,116 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:57:36,551 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:57:41,896 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:58:00,364 - ERROR - Failed on user U577819 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:58:00,364 - INFO - Starting user U565977 in cluster 2 (index 265)
2025-04-03 19:58:03,147 - INFO - Starting user U582712 in cluster 1 (index 166)
2025-04-03 19:58:06,535 - INFO - Starting user U177703 in cluster 0 (index 162)
2025-04-03 19:58:17,838 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 19:58:20,899 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:58:24,441 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 19:58:31,314 - ERROR - Failed on user U565977 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:58:31,315 - INFO - Starting user U236658 in cluster 2 (index 266)
2025-04-03 19:58:49,600 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 19:58:53,678 - INFO - Starting user U483813 in cluster 1 (index 167)
2025-04-03 19:59:01,139 - INFO - Starting user U404683 in cluster 0 (index 163)
2025-04-03 19:59:05,358 - ERROR - Failed on user U236658 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:59:05,358 - INFO - Starting user U52622 in cluster 2 (index 267)
2025-04-03 19:59:12,058 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 19:59:19,283 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 19:59:23,701 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 19:59:42,873 - ERROR - Failed on user U52622 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 19:59:42,874 - INFO - Starting user U292533 in cluster 2 (index 268)
2025-04-03 19:59:44,958 - INFO - Starting user U155504 in cluster 1 (index 168)
2025-04-03 19:59:48,811 - INFO - Starting user U661611 in cluster 0 (index 164)
2025-04-03 20:00:01,054 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:00:03,321 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:00:07,772 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:00:14,584 - ERROR - Failed on user U292533 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:00:14,584 - INFO - Starting user U16360 in cluster 2 (index 269)
2025-04-03 20:00:32,583 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:00:38,727 - INFO - Starting user U374178 in cluster 1 (index 169)
2025-04-03 20:00:43,513 - INFO - Starting user U297397 in cluster 0 (index 165)
2025-04-03 20:00:48,136 - ERROR - Failed on user U16360 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:00:48,137 - INFO - Starting user U21649 in cluster 2 (index 270)
2025-04-03 20:00:56,340 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 20:01:01,622 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:01:06,816 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:01:24,363 - ERROR - Failed on user U21649 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:01:24,364 - INFO - Starting user U78662 in cluster 2 (index 271)
2025-04-03 20:01:34,794 - INFO - Starting user U66991 in cluster 0 (index 166)
2025-04-03 20:01:39,390 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U456929', 'user_index_in_cluster': 160, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 187, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U592302', 'user_index_in_cluster': 161, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 232, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U407597', 'user_index_in_cluster': 162, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 218, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U39405', 'user_index_in_cluster': 163, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 106, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0017857142857142857, 'ndcg_1000': 0.06714375788082666, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0014607163137827532, 'ndcg_2000': 0.12400616116475024, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U118486', 'user_index_in_cluster': 164, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 138, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0025906735751295338, 'ndcg_500': 0.05459148056699469, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0025906735751295338, 'ndcg_1000': 0.05459148056699469, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0019678317438525947, 'ndcg_2000': 0.0991186325374297, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U174774', 'user_index_in_cluster': 165, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 131, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U582712', 'user_index_in_cluster': 166, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 177, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U483813', 'user_index_in_cluster': 167, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 141, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0024449877750611247, 'ndcg_500': 0.07064330878532636, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0024449877750611247, 'ndcg_1000': 0.07064330878532636, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0024449877750611247, 'ndcg_2000': 0.07064330878532636, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U155504', 'user_index_in_cluster': 168, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 239, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U374178', 'user_index_in_cluster': 169, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 207, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.25, 'ap_500': 0.004150549412932599, 'ndcg_500': 0.060194767840449366, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.375, 'ap_1000': 0.0038097858095254514, 'ndcg_1000': 0.08572676502226748, 'num_recommendations_1000': 1000, 'precision_2000': 0.003, 'recall_2000': 0.75, 'ap_2000': 0.0036704876432032767, 'ndcg_2000': 0.15832704170876627, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:01:39,392 - INFO - Starting user U631045 in cluster 1 (index 170)
2025-04-03 20:01:42,499 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:01:52,433 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:01:54,167 - ERROR - Failed on user U78662 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:01:54,167 - INFO - Starting user U232080 in cluster 2 (index 272)
2025-04-03 20:01:57,653 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:02:13,498 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:02:26,476 - INFO - Starting user U49895 in cluster 0 (index 167)
2025-04-03 20:02:30,679 - INFO - Starting user U569581 in cluster 1 (index 171)
2025-04-03 20:02:32,149 - ERROR - Failed on user U232080 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:02:32,150 - INFO - Starting user U623182 in cluster 2 (index 273)
2025-04-03 20:02:44,879 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:02:48,415 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:02:49,691 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:03:06,300 - ERROR - Failed on user U623182 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:03:06,301 - INFO - Starting user U373916 in cluster 2 (index 274)
2025-04-03 20:03:16,861 - INFO - Starting user U242029 in cluster 0 (index 168)
2025-04-03 20:03:20,475 - INFO - Starting user U184982 in cluster 1 (index 172)
2025-04-03 20:03:24,771 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:03:34,951 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:03:37,967 - ERROR - Failed on user U373916 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:03:37,967 - INFO - Starting user U83843 in cluster 2 (index 275)
2025-04-03 20:03:38,314 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:03:56,958 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:04:09,117 - INFO - Starting user U247623 in cluster 0 (index 169)
2025-04-03 20:04:13,145 - INFO - Starting user U373716 in cluster 1 (index 173)
2025-04-03 20:04:14,237 - ERROR - Failed on user U83843 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:04:14,237 - INFO - Starting user U289673 in cluster 2 (index 276)
2025-04-03 20:04:26,818 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:04:31,566 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:04:31,988 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:04:48,644 - ERROR - Failed on user U289673 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:04:48,644 - INFO - Starting user U705188 in cluster 2 (index 277)
2025-04-03 20:04:58,835 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U448351', 'user_index_in_cluster': 160, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 80, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U497160', 'user_index_in_cluster': 161, 'num_candidates': 4525, 'num_history_articles': 20, 'original_history_len': 20, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U177703', 'user_index_in_cluster': 162, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 86, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U404683', 'user_index_in_cluster': 163, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U661611', 'user_index_in_cluster': 164, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 66, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U297397', 'user_index_in_cluster': 165, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 59, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U66991', 'user_index_in_cluster': 166, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 153, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U49895', 'user_index_in_cluster': 167, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006422607578676942, 'ndcg_2000': 0.0942908803224738, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U242029', 'user_index_in_cluster': 168, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U247623', 'user_index_in_cluster': 169, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 108, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:04:58,837 - INFO - Starting user U451470 in cluster 0 (index 170)
2025-04-03 20:05:02,680 - INFO - Starting user U657307 in cluster 1 (index 174)
2025-04-03 20:05:06,442 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:05:16,835 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:05:19,183 - ERROR - Failed on user U705188 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:05:19,183 - INFO - Starting user U419352 in cluster 2 (index 278)
2025-04-03 20:05:20,047 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:05:37,105 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:05:50,837 - INFO - Starting user U133926 in cluster 0 (index 171)
2025-04-03 20:05:53,023 - ERROR - Failed on user U419352 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:05:53,023 - INFO - Starting user U582559 in cluster 2 (index 279)
2025-04-03 20:05:55,806 - INFO - Starting user U310456 in cluster 1 (index 175)
2025-04-03 20:06:08,328 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:06:11,783 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:06:13,510 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:06:27,222 - ERROR - Failed on user U582559 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:06:27,223 - INFO - Starting user U65224 in cluster 2 (index 280)
2025-04-03 20:06:41,603 - INFO - Starting user U330728 in cluster 0 (index 172)
2025-04-03 20:06:45,426 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:06:46,901 - INFO - Starting user U339311 in cluster 1 (index 176)
2025-04-03 20:06:58,433 - ERROR - Failed on user U65224 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:06:58,433 - INFO - Starting user U30186 in cluster 2 (index 281)
2025-04-03 20:06:59,596 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:07:05,127 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:07:17,539 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:07:33,416 - INFO - Starting user U626749 in cluster 0 (index 173)
2025-04-03 20:07:35,220 - ERROR - Failed on user U30186 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:07:35,220 - INFO - Starting user U318607 in cluster 2 (index 282)
2025-04-03 20:07:37,351 - INFO - Starting user U57649 in cluster 1 (index 177)
2025-04-03 20:07:53,127 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:07:53,165 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:07:58,542 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 20:08:06,191 - ERROR - Failed on user U318607 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:08:06,192 - INFO - Starting user U138813 in cluster 2 (index 283)
2025-04-03 20:08:25,164 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:08:28,605 - INFO - Starting user U334421 in cluster 0 (index 174)
2025-04-03 20:08:32,769 - INFO - Starting user U651873 in cluster 1 (index 178)
2025-04-03 20:08:44,252 - ERROR - Failed on user U138813 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:08:44,253 - INFO - Starting user U508102 in cluster 2 (index 284)
2025-04-03 20:08:46,772 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:08:50,567 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:09:03,184 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:09:20,032 - ERROR - Failed on user U508102 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:09:20,033 - INFO - Starting user U648482 in cluster 2 (index 285)
2025-04-03 20:09:22,170 - INFO - Starting user U54372 in cluster 0 (index 175)
2025-04-03 20:09:26,464 - INFO - Starting user U650301 in cluster 1 (index 179)
2025-04-03 20:09:38,836 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:09:40,992 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 20:09:44,962 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:09:52,252 - ERROR - Failed on user U648482 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:09:52,253 - INFO - Starting user U14718 in cluster 2 (index 286)
2025-04-03 20:10:10,647 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:10:24,721 - INFO - Starting user U695557 in cluster 0 (index 176)
2025-04-03 20:10:26,984 - ERROR - Failed on user U14718 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:10:26,984 - INFO - Starting user U564373 in cluster 2 (index 287)
2025-04-03 20:10:29,372 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U631045', 'user_index_in_cluster': 170, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 328, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0008616549425092522, 'ndcg_2000': 0.07280030471365963, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U569581', 'user_index_in_cluster': 171, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 314, 'num_future_clicks': 2, 'precision_5': 0.2, 'recall_5': 0.5, 'ap_5': 0.3333333333333333, 'ndcg_5': 0.3065735963827292, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.5, 'ap_10': 0.3333333333333333, 'ndcg_10': 0.3065735963827292, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.5, 'ap_20': 0.3333333333333333, 'ndcg_20': 0.3065735963827292, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.5, 'ap_50': 0.3333333333333333, 'ndcg_50': 0.3065735963827292, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.3333333333333333, 'ndcg_100': 0.3065735963827292, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.3333333333333333, 'ndcg_200': 0.3065735963827292, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.3333333333333333, 'ndcg_500': 0.3065735963827292, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.3333333333333333, 'ndcg_1000': 0.3065735963827292, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.16724071947952543, 'ndcg_2000': 0.3635186016493208, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U184982', 'user_index_in_cluster': 172, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 203, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U373716', 'user_index_in_cluster': 173, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 262, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U657307', 'user_index_in_cluster': 174, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 193, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U310456', 'user_index_in_cluster': 175, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 211, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.125, 'ap_100': 0.01818181818181818, 'ndcg_100': 0.04355557842812834, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.125, 'ap_200': 0.01818181818181818, 'ndcg_200': 0.04355557842812834, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.375, 'ap_500': 0.01032597453223524, 'ndcg_500': 0.10250365109470842, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.5, 'ap_1000': 0.008853128349287295, 'ndcg_1000': 0.12826528329327444, 'num_recommendations_1000': 1000, 'precision_2000': 0.003, 'recall_2000': 0.75, 'ap_2000': 0.007143814530184841, 'ndcg_2000': 0.17632268062988052, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U339311', 'user_index_in_cluster': 176, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 217, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U57649', 'user_index_in_cluster': 177, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 297, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.025, 'ndcg_50': 0.06330507011061545, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.025, 'ndcg_100': 0.06330507011061545, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.025, 'ndcg_200': 0.06330507011061545, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.025, 'ndcg_500': 0.06330507011061545, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.025, 'ndcg_1000': 0.06330507011061545, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.01328926598263615, 'ndcg_2000': 0.09620660781537886, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U651873', 'user_index_in_cluster': 178, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 189, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022522522522522522, 'ndcg_500': 0.11366656890143952, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022522522522522522, 'ndcg_1000': 0.11366656890143952, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022522522522522522, 'ndcg_2000': 0.11366656890143952, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U650301', 'user_index_in_cluster': 179, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 134, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:10:29,377 - INFO - Starting user U32504 in cluster 1 (index 180)
2025-04-03 20:10:43,150 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:10:45,816 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:10:48,604 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:11:01,261 - ERROR - Failed on user U564373 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:11:01,261 - INFO - Starting user U411922 in cluster 2 (index 288)
2025-04-03 20:11:15,383 - INFO - Starting user U87201 in cluster 0 (index 177)
2025-04-03 20:11:20,055 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:11:20,429 - INFO - Starting user U310934 in cluster 1 (index 181)
2025-04-03 20:11:32,095 - ERROR - Failed on user U411922 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:11:32,096 - INFO - Starting user U212457 in cluster 2 (index 289)
2025-04-03 20:11:34,427 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:11:39,058 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:11:50,472 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:12:06,183 - ERROR - Failed on user U212457 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:12:06,183 - INFO - Starting user U708895 in cluster 2 (index 290)
2025-04-03 20:12:08,369 - INFO - Starting user U361942 in cluster 0 (index 178)
2025-04-03 20:12:12,353 - INFO - Starting user U150702 in cluster 1 (index 182)
2025-04-03 20:12:24,139 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:12:26,781 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:12:30,548 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:12:37,232 - ERROR - Failed on user U708895 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:12:37,233 - INFO - Starting user U466712 in cluster 2 (index 291)
2025-04-03 20:12:57,325 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 20:12:59,352 - INFO - Starting user U294637 in cluster 0 (index 179)
2025-04-03 20:13:03,236 - INFO - Starting user U291797 in cluster 1 (index 183)
2025-04-03 20:13:14,943 - ERROR - Failed on user U466712 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:13:14,943 - INFO - Starting user U16840 in cluster 2 (index 292)
2025-04-03 20:13:17,288 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:13:22,144 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:13:34,200 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:13:49,956 - ERROR - Failed on user U16840 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:13:49,956 - INFO - Starting user U584363 in cluster 2 (index 293)
2025-04-03 20:13:52,490 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U451470', 'user_index_in_cluster': 170, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 77, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U133926', 'user_index_in_cluster': 171, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 165, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U330728', 'user_index_in_cluster': 172, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U626749', 'user_index_in_cluster': 173, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 137, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.0010362694300518134, 'ndcg_1000': 0.030516951844400318, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0014696095009440798, 'ndcg_2000': 0.06065987996460677, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U334421', 'user_index_in_cluster': 174, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 113, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U54372', 'user_index_in_cluster': 175, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 76, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U695557', 'user_index_in_cluster': 176, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 121, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002277904328018223, 'ndcg_500': 0.06982371896474258, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002277904328018223, 'ndcg_1000': 0.06982371896474258, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.002277904328018223, 'ndcg_2000': 0.06982371896474258, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U87201', 'user_index_in_cluster': 177, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 59, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U361942', 'user_index_in_cluster': 178, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 186, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U294637', 'user_index_in_cluster': 179, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:13:52,494 - INFO - Starting user U357135 in cluster 0 (index 180)
2025-04-03 20:13:56,468 - INFO - Starting user U261963 in cluster 1 (index 184)
2025-04-03 20:14:08,147 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:14:11,827 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:14:14,774 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:14:20,159 - ERROR - Failed on user U584363 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:14:20,160 - INFO - Starting user U628509 in cluster 2 (index 294)
2025-04-03 20:14:39,928 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:14:43,669 - INFO - Starting user U58589 in cluster 0 (index 181)
2025-04-03 20:14:49,729 - INFO - Starting user U540408 in cluster 1 (index 185)
2025-04-03 20:14:55,208 - ERROR - Failed on user U628509 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:14:55,209 - INFO - Starting user U615280 in cluster 2 (index 295)
2025-04-03 20:15:01,808 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:15:07,771 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:15:13,510 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:15:29,527 - ERROR - Failed on user U615280 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:15:29,527 - INFO - Starting user U565429 in cluster 2 (index 296)
2025-04-03 20:15:36,036 - INFO - Starting user U79726 in cluster 0 (index 182)
2025-04-03 20:15:40,021 - INFO - Starting user U216895 in cluster 1 (index 186)
2025-04-03 20:15:48,059 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:15:54,294 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:15:58,973 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:16:00,929 - ERROR - Failed on user U565429 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:16:00,930 - INFO - Starting user U494748 in cluster 2 (index 297)
2025-04-03 20:16:19,341 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:16:25,635 - INFO - Starting user U541128 in cluster 0 (index 183)
2025-04-03 20:16:33,621 - INFO - Starting user U133273 in cluster 1 (index 187)
2025-04-03 20:16:37,406 - ERROR - Failed on user U494748 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:16:37,406 - INFO - Starting user U31118 in cluster 2 (index 298)
2025-04-03 20:16:44,445 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:16:51,409 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:16:55,547 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:17:11,623 - ERROR - Failed on user U31118 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:17:11,624 - INFO - Starting user U428705 in cluster 2 (index 299)
2025-04-03 20:17:18,556 - INFO - Starting user U478722 in cluster 0 (index 184)
2025-04-03 20:17:22,447 - INFO - Starting user U616313 in cluster 1 (index 188)
2025-04-03 20:17:30,193 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:17:36,836 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 20:17:40,384 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:17:43,330 - ERROR - Failed on user U428705 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:17:43,330 - INFO - Starting user U496793 in cluster 2 (index 300)
2025-04-03 20:18:02,348 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:18:09,276 - INFO - Starting user U323699 in cluster 0 (index 185)
2025-04-03 20:18:15,139 - INFO - Starting user U283800 in cluster 1 (index 189)
2025-04-03 20:18:20,786 - ERROR - Failed on user U496793 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:18:20,787 - INFO - Starting user U441580 in cluster 2 (index 301)
2025-04-03 20:18:27,268 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:18:34,661 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:18:39,727 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:18:55,124 - ERROR - Failed on user U441580 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:18:55,124 - INFO - Starting user U707363 in cluster 2 (index 302)
2025-04-03 20:19:01,573 - INFO - Starting user U286234 in cluster 0 (index 186)
2025-04-03 20:19:05,305 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U32504', 'user_index_in_cluster': 180, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U310934', 'user_index_in_cluster': 181, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 344, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U150702', 'user_index_in_cluster': 182, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.024390243902439025, 'ndcg_50': 0.08702728145052864, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.024390243902439025, 'ndcg_100': 0.08702728145052864, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.024390243902439025, 'ndcg_200': 0.08702728145052864, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.024390243902439025, 'ndcg_500': 0.08702728145052864, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.024390243902439025, 'ndcg_1000': 0.08702728145052864, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.024390243902439025, 'ndcg_2000': 0.08702728145052864, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U291797', 'user_index_in_cluster': 183, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 123, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U261963', 'user_index_in_cluster': 184, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U540408', 'user_index_in_cluster': 185, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 208, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022271714922048997, 'ndcg_500': 0.11345868229591442, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022271714922048997, 'ndcg_1000': 0.11345868229591442, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022271714922048997, 'ndcg_2000': 0.11345868229591442, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U216895', 'user_index_in_cluster': 186, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 218, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.001199040767386091, 'ndcg_1000': 0.06317436806733923, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.001199040767386091, 'ndcg_2000': 0.06317436806733923, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U133273', 'user_index_in_cluster': 187, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 163, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U616313', 'user_index_in_cluster': 188, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 188, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U283800', 'user_index_in_cluster': 189, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 288, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:19:05,309 - INFO - Starting user U159677 in cluster 1 (index 190)
2025-04-03 20:19:13,315 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:19:20,128 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:19:23,026 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 20:19:26,226 - ERROR - Failed on user U707363 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:19:26,226 - INFO - Starting user U627481 in cluster 2 (index 303)
2025-04-03 20:19:45,836 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:19:51,951 - INFO - Starting user U703956 in cluster 0 (index 187)
2025-04-03 20:19:58,168 - INFO - Starting user U568941 in cluster 1 (index 191)
2025-04-03 20:20:03,545 - ERROR - Failed on user U627481 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:20:03,546 - INFO - Starting user U450661 in cluster 2 (index 304)
2025-04-03 20:20:10,301 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:20:16,544 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:20:21,761 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:20:37,632 - ERROR - Failed on user U450661 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:20:37,633 - INFO - Starting user U469234 in cluster 2 (index 305)
2025-04-03 20:20:44,456 - INFO - Starting user U594810 in cluster 0 (index 188)
2025-04-03 20:20:48,243 - INFO - Starting user U389414 in cluster 1 (index 192)
2025-04-03 20:20:55,874 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:21:02,694 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:21:06,793 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:21:09,013 - ERROR - Failed on user U469234 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:21:09,013 - INFO - Starting user U283472 in cluster 2 (index 306)
2025-04-03 20:21:28,372 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:21:33,816 - INFO - Starting user U53734 in cluster 0 (index 189)
2025-04-03 20:21:41,872 - INFO - Starting user U302546 in cluster 1 (index 193)
2025-04-03 20:21:45,795 - ERROR - Failed on user U283472 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:21:45,795 - INFO - Starting user U656737 in cluster 2 (index 307)
2025-04-03 20:21:52,876 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:22:00,098 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:22:04,135 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:22:20,316 - ERROR - Failed on user U656737 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:22:20,316 - INFO - Starting user U321057 in cluster 2 (index 308)
2025-04-03 20:22:26,572 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U357135', 'user_index_in_cluster': 180, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 102, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U58589', 'user_index_in_cluster': 181, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 133, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U79726', 'user_index_in_cluster': 182, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 98, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U541128', 'user_index_in_cluster': 183, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 63, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U478722', 'user_index_in_cluster': 184, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 51, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U323699', 'user_index_in_cluster': 185, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 91, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U286234', 'user_index_in_cluster': 186, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 126, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U703956', 'user_index_in_cluster': 187, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 74, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.003424657534246575, 'ndcg_500': 0.07482188961281717, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.003424657534246575, 'ndcg_1000': 0.07482188961281717, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.002359997160905671, 'ndcg_2000': 0.13270205146639671, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U594810', 'user_index_in_cluster': 188, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 58, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U53734', 'user_index_in_cluster': 189, 'num_candidates': 4524, 'num_history_articles': 48, 'original_history_len': 48, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:22:26,574 - INFO - Starting user U472700 in cluster 0 (index 190)
2025-04-03 20:22:30,441 - INFO - Starting user U591224 in cluster 1 (index 194)
2025-04-03 20:22:38,835 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:22:45,164 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:22:48,997 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:22:52,114 - ERROR - Failed on user U321057 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:22:52,115 - INFO - Starting user U65975 in cluster 2 (index 309)
2025-04-03 20:23:10,562 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:23:17,313 - INFO - Starting user U478551 in cluster 0 (index 191)
2025-04-03 20:23:24,382 - INFO - Starting user U579919 in cluster 1 (index 195)
2025-04-03 20:23:29,035 - ERROR - Failed on user U65975 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:23:29,036 - INFO - Starting user U533695 in cluster 2 (index 310)
2025-04-03 20:23:34,773 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:23:42,535 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:23:46,772 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:24:05,535 - ERROR - Failed on user U533695 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:24:05,535 - INFO - Starting user U629893 in cluster 2 (index 311)
2025-04-03 20:24:07,545 - INFO - Starting user U243594 in cluster 0 (index 192)
2025-04-03 20:24:11,221 - INFO - Starting user U380044 in cluster 1 (index 196)
2025-04-03 20:24:22,887 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:24:25,018 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:24:28,977 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:24:36,050 - ERROR - Failed on user U629893 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:24:36,050 - INFO - Starting user U228699 in cluster 2 (index 312)
2025-04-03 20:24:54,312 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:25:01,406 - INFO - Starting user U132294 in cluster 1 (index 197)
2025-04-03 20:25:06,913 - INFO - Starting user U19265 in cluster 0 (index 193)
2025-04-03 20:25:12,790 - ERROR - Failed on user U228699 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:25:12,790 - INFO - Starting user U72890 in cluster 2 (index 313)
2025-04-03 20:25:19,999 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 20:25:24,635 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:25:31,334 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 20:25:47,320 - ERROR - Failed on user U72890 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:25:47,321 - INFO - Starting user U445910 in cluster 2 (index 314)
2025-04-03 20:25:53,491 - INFO - Starting user U180134 in cluster 1 (index 198)
2025-04-03 20:25:57,012 - INFO - Starting user U492572 in cluster 0 (index 194)
2025-04-03 20:26:05,252 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:26:11,413 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 20:26:14,644 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:26:18,150 - ERROR - Failed on user U445910 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:26:18,150 - INFO - Starting user U675607 in cluster 2 (index 315)
2025-04-03 20:26:37,558 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:26:44,273 - INFO - Starting user U160279 in cluster 1 (index 199)
2025-04-03 20:26:50,149 - INFO - Starting user U296940 in cluster 0 (index 195)
2025-04-03 20:26:55,274 - ERROR - Failed on user U675607 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:26:55,275 - INFO - Starting user U479167 in cluster 2 (index 316)
2025-04-03 20:27:02,221 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:27:08,023 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:27:13,595 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:27:29,510 - ERROR - Failed on user U479167 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:27:29,510 - INFO - Starting user U653483 in cluster 2 (index 317)
2025-04-03 20:27:36,430 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U159677', 'user_index_in_cluster': 190, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 188, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U568941', 'user_index_in_cluster': 191, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 206, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U389414', 'user_index_in_cluster': 192, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 232, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.00352701953997384, 'ndcg_500': 0.07786398288079267, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.8, 'ap_1000': 0.004066350558623766, 'ndcg_1000': 0.1488842328879095, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.004066350558623766, 'ndcg_2000': 0.1488842328879095, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U302546', 'user_index_in_cluster': 193, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 172, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U591224', 'user_index_in_cluster': 194, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 193, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U579919', 'user_index_in_cluster': 195, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 213, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U380044', 'user_index_in_cluster': 196, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 231, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U132294', 'user_index_in_cluster': 197, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 142, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U180134', 'user_index_in_cluster': 198, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 200, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U160279', 'user_index_in_cluster': 199, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 156, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:27:36,432 - INFO - Starting user U175201 in cluster 1 (index 200)
2025-04-03 20:27:40,083 - INFO - Starting user U164231 in cluster 0 (index 196)
2025-04-03 20:27:48,056 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:27:54,664 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:27:59,022 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:28:01,753 - ERROR - Failed on user U653483 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:28:01,754 - INFO - Starting user U305522 in cluster 2 (index 318)
2025-04-03 20:28:20,347 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:28:26,906 - INFO - Starting user U280379 in cluster 1 (index 201)
2025-04-03 20:28:34,038 - INFO - Starting user U479840 in cluster 0 (index 197)
2025-04-03 20:28:38,565 - ERROR - Failed on user U305522 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:28:38,566 - INFO - Starting user U22043 in cluster 2 (index 319)
2025-04-03 20:28:45,168 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:28:52,116 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:28:57,702 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:29:16,457 - ERROR - Failed on user U22043 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:29:16,457 - INFO - Starting user U658148 in cluster 2 (index 320)
2025-04-03 20:29:18,853 - INFO - Starting user U391080 in cluster 1 (index 202)
2025-04-03 20:29:22,431 - INFO - Starting user U110788 in cluster 0 (index 198)
2025-04-03 20:29:34,700 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:29:37,993 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:29:40,797 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:29:47,758 - ERROR - Failed on user U658148 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:29:47,758 - INFO - Starting user U498433 in cluster 2 (index 321)
2025-04-03 20:30:06,845 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:30:10,243 - INFO - Starting user U369897 in cluster 1 (index 203)
2025-04-03 20:30:17,097 - INFO - Starting user U404460 in cluster 0 (index 199)
2025-04-03 20:30:21,919 - ERROR - Failed on user U498433 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:30:21,919 - INFO - Starting user U383286 in cluster 2 (index 322)
2025-04-03 20:30:28,304 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:30:36,377 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:30:41,053 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:30:59,740 - ERROR - Failed on user U383286 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:30:59,740 - INFO - Starting user U469862 in cluster 2 (index 323)
2025-04-03 20:31:01,998 - INFO - Starting user U620808 in cluster 1 (index 204)
2025-04-03 20:31:07,312 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U472700', 'user_index_in_cluster': 190, 'num_candidates': 4522, 'num_history_articles': 45, 'original_history_len': 45, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U478551', 'user_index_in_cluster': 191, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 79, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U243594', 'user_index_in_cluster': 192, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 79, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U19265', 'user_index_in_cluster': 193, 'num_candidates': 4523, 'num_history_articles': 37, 'original_history_len': 37, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U492572', 'user_index_in_cluster': 194, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 83, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0038461538461538464, 'ndcg_500': 0.07637697713792865, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0038461538461538464, 'ndcg_1000': 0.07637697713792865, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0028800147221199855, 'ndcg_2000': 0.13750423661499594, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U296940', 'user_index_in_cluster': 195, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U164231', 'user_index_in_cluster': 196, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 184, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U479840', 'user_index_in_cluster': 197, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 87, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U110788', 'user_index_in_cluster': 198, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 55, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U404460', 'user_index_in_cluster': 199, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 271, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:31:07,315 - INFO - Starting user U280542 in cluster 0 (index 200)
2025-04-03 20:31:17,402 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:31:20,270 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:31:25,253 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:31:30,771 - ERROR - Failed on user U469862 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:31:30,772 - INFO - Starting user U122578 in cluster 2 (index 324)
2025-04-03 20:31:48,755 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:31:52,267 - INFO - Starting user U111344 in cluster 1 (index 205)
2025-04-03 20:31:58,814 - INFO - Starting user U658589 in cluster 0 (index 201)
2025-04-03 20:32:03,791 - ERROR - Failed on user U122578 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:32:03,792 - INFO - Starting user U262378 in cluster 2 (index 325)
2025-04-03 20:32:10,564 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:32:16,527 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:32:21,275 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:32:38,885 - ERROR - Failed on user U262378 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:32:38,886 - INFO - Starting user U376617 in cluster 2 (index 326)
2025-04-03 20:32:45,260 - INFO - Starting user U660859 in cluster 1 (index 206)
2025-04-03 20:32:49,178 - INFO - Starting user U454784 in cluster 0 (index 202)
2025-04-03 20:32:56,097 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 20:33:01,828 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:33:06,489 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:33:09,176 - ERROR - Failed on user U376617 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:33:09,176 - INFO - Starting user U200357 in cluster 2 (index 327)
2025-04-03 20:33:26,321 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:33:32,708 - INFO - Starting user U703150 in cluster 1 (index 207)
2025-04-03 20:33:39,147 - INFO - Starting user U702513 in cluster 0 (index 203)
2025-04-03 20:33:44,426 - ERROR - Failed on user U200357 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:33:44,427 - INFO - Starting user U497494 in cluster 2 (index 328)
2025-04-03 20:33:49,280 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:33:55,795 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:34:01,029 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:34:16,012 - ERROR - Failed on user U497494 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:34:16,012 - INFO - Starting user U255304 in cluster 2 (index 329)
2025-04-03 20:34:22,135 - INFO - Starting user U620898 in cluster 1 (index 208)
2025-04-03 20:34:25,575 - INFO - Starting user U345874 in cluster 0 (index 204)
2025-04-03 20:34:32,385 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:34:40,244 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:34:42,670 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:34:45,172 - ERROR - Failed on user U255304 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:34:45,173 - INFO - Starting user U155703 in cluster 2 (index 330)
2025-04-03 20:35:02,331 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:35:12,858 - INFO - Starting user U292106 in cluster 1 (index 209)
2025-04-03 20:35:17,734 - INFO - Starting user U647987 in cluster 0 (index 205)
2025-04-03 20:35:18,923 - ERROR - Failed on user U155703 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:35:18,924 - INFO - Starting user U270756 in cluster 2 (index 331)
2025-04-03 20:35:29,031 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:35:34,855 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:35:35,564 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:35:51,982 - ERROR - Failed on user U270756 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:35:51,982 - INFO - Starting user U72896 in cluster 2 (index 332)
2025-04-03 20:36:05,382 - INFO - Starting user U356691 in cluster 0 (index 206)
2025-04-03 20:36:08,694 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:36:09,160 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U175201', 'user_index_in_cluster': 200, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 271, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U280379', 'user_index_in_cluster': 201, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0024814478045346554, 'ndcg_1000': 0.06586554242729976, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0024814478045346554, 'ndcg_2000': 0.06586554242729976, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U391080', 'user_index_in_cluster': 202, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 134, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U369897', 'user_index_in_cluster': 203, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 137, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U620808', 'user_index_in_cluster': 204, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 210, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U111344', 'user_index_in_cluster': 205, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 168, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.029411764705882353, 'ndcg_50': 0.06612234188543999, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.029411764705882353, 'ndcg_100': 0.06612234188543999, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.029411764705882353, 'ndcg_200': 0.06612234188543999, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.029411764705882353, 'ndcg_500': 0.06612234188543999, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.016356047369442826, 'ndcg_1000': 0.10280594707911707, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.016356047369442826, 'ndcg_2000': 0.10280594707911707, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U660859', 'user_index_in_cluster': 206, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 344, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0021598272138228943, 'ndcg_500': 0.06921974579791113, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0021598272138228943, 'ndcg_1000': 0.06921974579791113, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0021598272138228943, 'ndcg_2000': 0.06921974579791113, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U703150', 'user_index_in_cluster': 207, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U620898', 'user_index_in_cluster': 208, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 132, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.022727272727272728, 'ndcg_50': 0.06175696970122604, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.022727272727272728, 'ndcg_100': 0.06175696970122604, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.022727272727272728, 'ndcg_200': 0.06175696970122604, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.013646741386467414, 'ndcg_500': 0.10039415644141232, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.013646741386467414, 'ndcg_1000': 0.10039415644141232, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.009841322014770098, 'ndcg_2000': 0.13302308736724802, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U292106', 'user_index_in_cluster': 209, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 140, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:36:09,162 - INFO - Starting user U131321 in cluster 1 (index 210)
2025-04-03 20:36:21,088 - ERROR - Failed on user U72896 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:36:21,088 - INFO - Starting user U273504 in cluster 2 (index 333)
2025-04-03 20:36:21,613 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:36:25,725 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:36:37,940 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:36:54,726 - INFO - Starting user U521216 in cluster 0 (index 207)
2025-04-03 20:36:56,446 - ERROR - Failed on user U273504 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:36:56,447 - INFO - Starting user U281727 in cluster 2 (index 334)
2025-04-03 20:36:58,461 - INFO - Starting user U590526 in cluster 1 (index 211)
2025-04-03 20:37:11,083 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:37:12,765 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:37:15,087 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:37:28,281 - ERROR - Failed on user U281727 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:37:28,281 - INFO - Starting user U704123 in cluster 2 (index 335)
2025-04-03 20:37:42,044 - INFO - Starting user U82163 in cluster 0 (index 208)
2025-04-03 20:37:44,632 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:37:46,022 - INFO - Starting user U324476 in cluster 1 (index 212)
2025-04-03 20:37:57,529 - ERROR - Failed on user U704123 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:37:57,529 - INFO - Starting user U458626 in cluster 2 (index 336)
2025-04-03 20:37:58,782 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:38:02,091 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 20:38:14,126 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:38:29,494 - ERROR - Failed on user U458626 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:38:29,494 - INFO - Starting user U420557 in cluster 2 (index 337)
2025-04-03 20:38:31,454 - INFO - Starting user U125865 in cluster 0 (index 209)
2025-04-03 20:38:35,142 - INFO - Starting user U454981 in cluster 1 (index 213)
2025-04-03 20:38:46,783 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:38:48,309 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:38:52,076 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:39:00,371 - ERROR - Failed on user U420557 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:39:00,371 - INFO - Starting user U555732 in cluster 2 (index 338)
2025-04-03 20:39:14,640 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U280542', 'user_index_in_cluster': 200, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U658589', 'user_index_in_cluster': 201, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 86, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.005917159763313609, 'ndcg_200': 0.1349638598663645, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.005917159763313609, 'ndcg_500': 0.1349638598663645, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.005917159763313609, 'ndcg_1000': 0.1349638598663645, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.005917159763313609, 'ndcg_2000': 0.1349638598663645, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U454784', 'user_index_in_cluster': 202, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 51, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U702513', 'user_index_in_cluster': 203, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 65, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U345874', 'user_index_in_cluster': 204, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0007434944237918215, 'ndcg_2000': 0.05898786794274458, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U647987', 'user_index_in_cluster': 205, 'num_candidates': 4525, 'num_history_articles': 47, 'original_history_len': 47, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U356691', 'user_index_in_cluster': 206, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U521216', 'user_index_in_cluster': 207, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 90, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U82163', 'user_index_in_cluster': 208, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 126, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U125865', 'user_index_in_cluster': 209, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 134, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:39:14,643 - INFO - Starting user U636890 in cluster 0 (index 210)
2025-04-03 20:39:17,096 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:39:21,394 - INFO - Starting user U388915 in cluster 1 (index 214)
2025-04-03 20:39:29,397 - ERROR - Failed on user U555732 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:39:29,397 - INFO - Starting user U16911 in cluster 2 (index 339)
2025-04-03 20:39:31,350 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:39:39,011 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:39:46,033 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:40:02,981 - ERROR - Failed on user U16911 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:40:02,981 - INFO - Starting user U710596 in cluster 2 (index 340)
2025-04-03 20:40:04,687 - INFO - Starting user U480221 in cluster 0 (index 211)
2025-04-03 20:40:08,146 - INFO - Starting user U210902 in cluster 1 (index 215)
2025-04-03 20:40:19,342 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:40:22,505 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:40:24,777 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:40:32,454 - ERROR - Failed on user U710596 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:40:32,455 - INFO - Starting user U415350 in cluster 2 (index 341)
2025-04-03 20:40:48,919 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:40:54,083 - INFO - Starting user U494367 in cluster 0 (index 212)
2025-04-03 20:40:59,819 - INFO - Starting user U149637 in cluster 1 (index 216)
2025-04-03 20:41:05,712 - ERROR - Failed on user U415350 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:41:05,712 - INFO - Starting user U401155 in cluster 2 (index 342)
2025-04-03 20:41:10,645 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:41:15,987 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:41:21,863 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:41:36,884 - ERROR - Failed on user U401155 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:41:36,884 - INFO - Starting user U180538 in cluster 2 (index 343)
2025-04-03 20:41:42,713 - INFO - Starting user U313331 in cluster 0 (index 213)
2025-04-03 20:41:46,286 - INFO - Starting user U214291 in cluster 1 (index 217)
2025-04-03 20:41:53,877 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:41:59,615 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 20:42:03,498 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:42:07,353 - ERROR - Failed on user U180538 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:42:07,353 - INFO - Starting user U377024 in cluster 2 (index 344)
2025-04-03 20:42:23,526 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:42:27,636 - INFO - Starting user U590690 in cluster 0 (index 214)
2025-04-03 20:42:33,609 - INFO - Starting user U271820 in cluster 1 (index 218)
2025-04-03 20:42:39,220 - ERROR - Failed on user U377024 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:42:39,220 - INFO - Starting user U167612 in cluster 2 (index 345)
2025-04-03 20:42:45,034 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:42:49,932 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:42:55,882 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:43:11,961 - ERROR - Failed on user U167612 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:43:11,961 - INFO - Starting user U415497 in cluster 2 (index 346)
2025-04-03 20:43:17,693 - INFO - Starting user U347881 in cluster 0 (index 215)
2025-04-03 20:43:21,265 - INFO - Starting user U310003 in cluster 1 (index 219)
2025-04-03 20:43:28,788 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:43:34,365 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:43:37,872 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:43:41,744 - ERROR - Failed on user U415497 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:43:41,745 - INFO - Starting user U244783 in cluster 2 (index 347)
2025-04-03 20:43:58,744 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:44:05,414 - INFO - Starting user U307155 in cluster 0 (index 216)
2025-04-03 20:44:10,569 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U131321', 'user_index_in_cluster': 210, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 140, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.038461538461538464, 'ndcg_50': 0.07132875491150918, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.038461538461538464, 'ndcg_100': 0.07132875491150918, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.038461538461538464, 'ndcg_200': 0.07132875491150918, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.022180621738143863, 'ndcg_500': 0.11165988089307603, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.022180621738143863, 'ndcg_1000': 0.11165988089307603, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.012250320225053802, 'ndcg_2000': 0.17591459231915704, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U590526', 'user_index_in_cluster': 211, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0014245014245014246, 'ndcg_1000': 0.041277818349892394, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.001390676763811092, 'ndcg_2000': 0.07836327678423154, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U324476', 'user_index_in_cluster': 212, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 170, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006357279084551812, 'ndcg_2000': 0.057733945515215275, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U454981', 'user_index_in_cluster': 213, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 157, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U388915', 'user_index_in_cluster': 214, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 354, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U210902', 'user_index_in_cluster': 215, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 118, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U149637', 'user_index_in_cluster': 216, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 151, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U214291', 'user_index_in_cluster': 217, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 206, 'num_future_clicks': 16, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.0625, 'ap_500': 0.004366812227074236, 'ndcg_500': 0.020874843482475352, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.25, 'ap_1000': 0.003916451818463205, 'ndcg_1000': 0.07197672271127896, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.3125, 'ap_2000': 0.0037620922723806266, 'ndcg_2000': 0.08737515420446748, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U271820', 'user_index_in_cluster': 218, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 143, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U310003', 'user_index_in_cluster': 219, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 221, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:44:10,572 - INFO - Starting user U333437 in cluster 1 (index 220)
2025-04-03 20:44:16,864 - ERROR - Failed on user U244783 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:44:16,864 - INFO - Starting user U100103 in cluster 2 (index 348)
2025-04-03 20:44:21,743 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:44:27,529 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:44:33,621 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:44:51,758 - ERROR - Failed on user U100103 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:44:51,758 - INFO - Starting user U345783 in cluster 2 (index 349)
2025-04-03 20:44:53,563 - INFO - Starting user U440175 in cluster 0 (index 217)
2025-04-03 20:44:57,419 - INFO - Starting user U139321 in cluster 1 (index 221)
2025-04-03 20:45:08,927 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:45:10,860 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:45:13,790 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:45:21,779 - ERROR - Failed on user U345783 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:45:21,779 - INFO - Starting user U440231 in cluster 2 (index 350)
2025-04-03 20:45:38,600 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:45:42,250 - INFO - Starting user U117000 in cluster 0 (index 218)
2025-04-03 20:45:48,913 - INFO - Starting user U296475 in cluster 1 (index 222)
2025-04-03 20:45:53,887 - ERROR - Failed on user U440231 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:45:53,887 - INFO - Starting user U477020 in cluster 2 (index 351)
2025-04-03 20:45:58,724 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:46:05,641 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:46:10,400 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:46:25,526 - ERROR - Failed on user U477020 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:46:25,526 - INFO - Starting user U125902 in cluster 2 (index 352)
2025-04-03 20:46:31,333 - INFO - Starting user U604351 in cluster 0 (index 219)
2025-04-03 20:46:35,119 - INFO - Starting user U401768 in cluster 1 (index 223)
2025-04-03 20:46:42,316 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:46:47,479 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:46:51,220 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:46:55,165 - ERROR - Failed on user U125902 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:46:55,165 - INFO - Starting user U475964 in cluster 2 (index 353)
2025-04-03 20:47:12,301 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:47:19,578 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U636890', 'user_index_in_cluster': 210, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 93, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.045454545454545456, 'ndcg_50': 0.10374097460838268, 'num_recommendations_50': 50, 'precision_100': 0.02, 'recall_100': 0.6666666666666666, 'ap_100': 0.03967642526964561, 'ndcg_100': 0.18318695696188048, 'num_recommendations_100': 100, 'precision_200': 0.01, 'recall_200': 0.6666666666666666, 'ap_200': 0.03967642526964561, 'ndcg_200': 0.18318695696188048, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.6666666666666666, 'ap_500': 0.03967642526964561, 'ndcg_500': 0.18318695696188048, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 1.0, 'ap_1000': 0.02794571849067555, 'ndcg_1000': 0.23317394694700855, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 1.0, 'ap_2000': 0.02794571849067555, 'ndcg_2000': 0.23317394694700855, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U480221', 'user_index_in_cluster': 211, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 62, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.0058823529411764705, 'ndcg_200': 0.045722155380280315, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.0058823529411764705, 'ndcg_500': 0.045722155380280315, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0058823529411764705, 'ndcg_1000': 0.045722155380280315, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.00360562829783076, 'ndcg_2000': 0.07785022914986965, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U494367', 'user_index_in_cluster': 212, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 285, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U313331', 'user_index_in_cluster': 213, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U590690', 'user_index_in_cluster': 214, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U347881', 'user_index_in_cluster': 215, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 108, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U307155', 'user_index_in_cluster': 216, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 143, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2857142857142857, 'ap_500': 0.004337772844987051, 'ndcg_500': 0.06580627357825869, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.2857142857142857, 'ap_1000': 0.004337772844987051, 'ndcg_1000': 0.06580627357825869, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.5714285714285714, 'ap_2000': 0.0033859315582444142, 'ndcg_2000': 0.11822262768994174, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U440175', 'user_index_in_cluster': 217, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U117000', 'user_index_in_cluster': 218, 'num_candidates': 4522, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U604351', 'user_index_in_cluster': 219, 'num_candidates': 4524, 'num_history_articles': 30, 'original_history_len': 30, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:47:19,583 - INFO - Starting user U498617 in cluster 0 (index 220)
2025-04-03 20:47:31,429 - ERROR - Failed on user U475964 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:47:31,436 - INFO - Starting user U652653 in cluster 2 (index 354)
2025-04-03 20:47:42,025 - INFO - Starting user U297347 in cluster 1 (index 224)
2025-04-03 20:47:45,593 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:47:58,608 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:48:04,080 - INFO - Starting user U518586 in cluster 0 (index 221)
2025-04-03 20:48:06,686 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:48:11,856 - ERROR - Failed on user U652653 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:48:11,857 - INFO - Starting user U58940 in cluster 2 (index 355)
2025-04-03 20:48:15,540 - INFO - Starting user U178702 in cluster 1 (index 225)
2025-04-03 20:48:30,216 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:48:32,460 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:48:37,285 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:48:55,960 - ERROR - Failed on user U58940 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:48:55,960 - INFO - Starting user U314055 in cluster 2 (index 356)
2025-04-03 20:49:08,006 - INFO - Starting user U223425 in cluster 0 (index 222)
2025-04-03 20:49:11,363 - INFO - Starting user U145258 in cluster 1 (index 226)
2025-04-03 20:49:12,652 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:49:24,246 - ERROR - Failed on user U314055 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:49:24,247 - INFO - Starting user U297765 in cluster 2 (index 357)
2025-04-03 20:49:24,456 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:49:27,947 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 20:49:41,398 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:49:57,185 - ERROR - Failed on user U297765 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:49:57,186 - INFO - Starting user U611884 in cluster 2 (index 358)
2025-04-03 20:50:03,988 - INFO - Starting user U268203 in cluster 1 (index 227)
2025-04-03 20:50:07,674 - INFO - Starting user U96945 in cluster 0 (index 223)
2025-04-03 20:50:14,075 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:50:20,917 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:50:26,328 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:50:27,099 - ERROR - Failed on user U611884 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:50:27,100 - INFO - Starting user U47255 in cluster 2 (index 359)
2025-04-03 20:50:43,769 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:50:55,186 - INFO - Starting user U287460 in cluster 1 (index 228)
2025-04-03 20:50:58,452 - INFO - Starting user U484275 in cluster 0 (index 224)
2025-04-03 20:51:00,349 - ERROR - Failed on user U47255 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:51:00,349 - INFO - Starting user U216060 in cluster 2 (index 360)
2025-04-03 20:51:11,606 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:51:15,184 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:51:16,550 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:51:33,761 - ERROR - Failed on user U216060 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:51:33,762 - INFO - Starting user U304002 in cluster 2 (index 361)
2025-04-03 20:51:47,642 - INFO - Starting user U549408 in cluster 0 (index 225)
2025-04-03 20:51:51,099 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:51:52,345 - INFO - Starting user U599715 in cluster 1 (index 229)
2025-04-03 20:52:03,939 - ERROR - Failed on user U304002 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:52:03,939 - INFO - Starting user U40924 in cluster 2 (index 362)
2025-04-03 20:52:04,464 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:52:09,533 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:52:20,228 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:52:35,381 - ERROR - Failed on user U40924 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:52:35,382 - INFO - Starting user U709334 in cluster 2 (index 363)
2025-04-03 20:52:37,160 - INFO - Starting user U222627 in cluster 0 (index 226)
2025-04-03 20:52:41,156 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U333437', 'user_index_in_cluster': 220, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 159, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U139321', 'user_index_in_cluster': 221, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 152, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U296475', 'user_index_in_cluster': 222, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 314, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U401768', 'user_index_in_cluster': 223, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 156, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U297347', 'user_index_in_cluster': 224, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 127, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U178702', 'user_index_in_cluster': 225, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 388, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U145258', 'user_index_in_cluster': 226, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 272, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.002140861134873111, 'ndcg_1000': 0.07250144901109482, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.002140861134873111, 'ndcg_2000': 0.07250144901109482, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U268203', 'user_index_in_cluster': 227, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 222, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U287460', 'user_index_in_cluster': 228, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 140, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0011325028312570782, 'ndcg_1000': 0.04794476887926172, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0011325028312570782, 'ndcg_2000': 0.04794476887926172, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U599715', 'user_index_in_cluster': 229, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 222, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:52:41,166 - INFO - Starting user U674676 in cluster 1 (index 230)
2025-04-03 20:52:51,703 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:52:53,718 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:52:58,823 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:53:04,885 - ERROR - Failed on user U709334 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:53:04,886 - INFO - Starting user U608652 in cluster 2 (index 364)
2025-04-03 20:53:21,782 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:53:27,344 - INFO - Starting user U185925 in cluster 0 (index 227)
2025-04-03 20:53:33,055 - INFO - Starting user U87038 in cluster 1 (index 231)
2025-04-03 20:53:38,646 - ERROR - Failed on user U608652 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:53:38,646 - INFO - Starting user U2587 in cluster 2 (index 365)
2025-04-03 20:53:44,144 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 20:53:49,416 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:53:55,144 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:54:10,669 - ERROR - Failed on user U2587 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:54:10,669 - INFO - Starting user U104735 in cluster 2 (index 366)
2025-04-03 20:54:17,281 - INFO - Starting user U400522 in cluster 0 (index 228)
2025-04-03 20:54:21,832 - INFO - Starting user U108238 in cluster 1 (index 232)
2025-04-03 20:54:27,049 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:54:33,848 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 20:54:38,293 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 20:54:39,744 - ERROR - Failed on user U104735 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:54:39,745 - INFO - Starting user U303428 in cluster 2 (index 367)
2025-04-03 20:54:57,007 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:55:07,397 - INFO - Starting user U645848 in cluster 0 (index 229)
2025-04-03 20:55:10,992 - INFO - Starting user U599465 in cluster 1 (index 233)
2025-04-03 20:55:13,289 - ERROR - Failed on user U303428 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:55:13,289 - INFO - Starting user U23930 in cluster 2 (index 368)
2025-04-03 20:55:23,704 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 20:55:27,859 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:55:29,420 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:55:46,392 - ERROR - Failed on user U23930 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:55:46,393 - INFO - Starting user U364839 in cluster 2 (index 369)
2025-04-03 20:55:56,518 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U498617', 'user_index_in_cluster': 220, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 106, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U518586', 'user_index_in_cluster': 221, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U223425', 'user_index_in_cluster': 222, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U96945', 'user_index_in_cluster': 223, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 100, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U484275', 'user_index_in_cluster': 224, 'num_candidates': 4522, 'num_history_articles': 40, 'original_history_len': 40, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U549408', 'user_index_in_cluster': 225, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 91, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.2, 'ap_50': 0.024390243902439025, 'ndcg_50': 0.06289692884934815, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.2, 'ap_100': 0.024390243902439025, 'ndcg_100': 0.06289692884934815, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.024390243902439025, 'ndcg_200': 0.06289692884934815, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.024390243902439025, 'ndcg_500': 0.06289692884934815, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.024390243902439025, 'ndcg_1000': 0.06289692884934815, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.012705326032852165, 'ndcg_2000': 0.0939062196397454, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U222627', 'user_index_in_cluster': 226, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0038461538461538464, 'ndcg_500': 0.12456548450209481, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0038461538461538464, 'ndcg_1000': 0.12456548450209481, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0038461538461538464, 'ndcg_2000': 0.12456548450209481, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U185925', 'user_index_in_cluster': 227, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002145922746781116, 'ndcg_500': 0.06914716574992143, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002145922746781116, 'ndcg_1000': 0.06914716574992143, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.002145922746781116, 'ndcg_2000': 0.06914716574992143, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U400522', 'user_index_in_cluster': 228, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 63, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U645848', 'user_index_in_cluster': 229, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 50, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 20:55:56,524 - INFO - Starting user U309010 in cluster 0 (index 230)
2025-04-03 20:55:59,937 - INFO - Starting user U502503 in cluster 1 (index 234)
2025-04-03 20:56:03,153 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:56:13,490 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:56:15,662 - ERROR - Failed on user U364839 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:56:15,662 - INFO - Starting user U553520 in cluster 2 (index 370)
2025-04-03 20:56:16,162 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:56:32,283 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:56:46,107 - INFO - Starting user U530061 in cluster 0 (index 231)
2025-04-03 20:56:47,972 - ERROR - Failed on user U553520 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:56:47,973 - INFO - Starting user U476133 in cluster 2 (index 371)
2025-04-03 20:56:49,969 - INFO - Starting user U562957 in cluster 1 (index 235)
2025-04-03 20:57:02,661 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:57:04,196 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 20:57:06,277 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:57:21,134 - ERROR - Failed on user U476133 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:57:21,134 - INFO - Starting user U368610 in cluster 2 (index 372)
2025-04-03 20:57:35,412 - INFO - Starting user U144830 in cluster 0 (index 232)
2025-04-03 20:57:38,339 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:57:39,409 - INFO - Starting user U493670 in cluster 1 (index 236)
2025-04-03 20:57:50,945 - ERROR - Failed on user U368610 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:57:50,945 - INFO - Starting user U623612 in cluster 2 (index 373)
2025-04-03 20:57:51,722 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:57:56,275 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 20:58:08,185 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:58:24,257 - INFO - Starting user U607560 in cluster 0 (index 233)
2025-04-03 20:58:26,301 - ERROR - Failed on user U623612 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:58:26,302 - INFO - Starting user U445194 in cluster 2 (index 374)
2025-04-03 20:58:28,471 - INFO - Starting user U700504 in cluster 1 (index 237)
2025-04-03 20:58:41,829 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:58:42,831 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 20:58:44,934 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:58:59,385 - ERROR - Failed on user U445194 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:58:59,386 - INFO - Starting user U609208 in cluster 2 (index 375)
2025-04-03 20:59:13,082 - INFO - Starting user U69534 in cluster 0 (index 234)
2025-04-03 20:59:15,713 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 20:59:17,756 - INFO - Starting user U585191 in cluster 1 (index 238)
2025-04-03 20:59:29,103 - ERROR - Failed on user U609208 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 20:59:29,103 - INFO - Starting user U241365 in cluster 2 (index 376)
2025-04-03 20:59:29,544 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:59:34,196 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 20:59:46,155 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:00:02,012 - INFO - Starting user U323165 in cluster 0 (index 235)
2025-04-03 21:00:04,234 - ERROR - Failed on user U241365 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:00:04,234 - INFO - Starting user U48955 in cluster 2 (index 377)
2025-04-03 21:00:06,546 - INFO - Starting user U2237 in cluster 1 (index 239)
2025-04-03 21:00:18,284 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:00:20,438 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:00:23,242 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:00:35,985 - ERROR - Failed on user U48955 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:00:35,985 - INFO - Starting user U457034 in cluster 2 (index 378)
2025-04-03 21:00:49,573 - INFO - Starting user U682759 in cluster 0 (index 236)
2025-04-03 21:00:52,332 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:00:58,555 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U674676', 'user_index_in_cluster': 230, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 94, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U87038', 'user_index_in_cluster': 231, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U108238', 'user_index_in_cluster': 232, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 121, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.0016, 'ndcg_1000': 0.02958835040523622, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.14285714285714285, 'ap_2000': 0.0016, 'ndcg_2000': 0.02958835040523622, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U599465', 'user_index_in_cluster': 233, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 234, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0011890606420927466, 'ndcg_1000': 0.04017215978353372, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0014603311868472391, 'ndcg_2000': 0.07853902969232292, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U502503', 'user_index_in_cluster': 234, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 180, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U562957', 'user_index_in_cluster': 235, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 177, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U493670', 'user_index_in_cluster': 236, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 223, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002242152466367713, 'ndcg_500': 0.06964312449351173, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 1.0, 'ap_1000': 0.0023346684661935653, 'ndcg_1000': 0.1329308362269132, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0023346684661935653, 'ndcg_2000': 0.1329308362269132, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U700504', 'user_index_in_cluster': 237, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 99, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U585191', 'user_index_in_cluster': 238, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 136, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U2237', 'user_index_in_cluster': 239, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 216, 'num_future_clicks': 12, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.08333333333333333, 'ap_50': 0.025, 'ndcg_50': 0.036650682182838984, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.08333333333333333, 'ap_100': 0.025, 'ndcg_100': 0.036650682182838984, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.08333333333333333, 'ap_200': 0.025, 'ndcg_200': 0.036650682182838984, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.08333333333333333, 'ap_500': 0.025, 'ndcg_500': 0.036650682182838984, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.013646788990825689, 'ndcg_1000': 0.05674906446329078, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.4166666666666667, 'ap_2000': 0.0069766582997007375, 'ndcg_2000': 0.11237095890106558, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:00:58,557 - INFO - Starting user U177368 in cluster 1 (index 240)
2025-04-03 21:01:06,627 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:01:06,636 - ERROR - Failed on user U457034 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:01:06,637 - INFO - Starting user U177577 in cluster 2 (index 379)
2025-04-03 21:01:14,833 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:01:23,269 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:01:40,376 - INFO - Starting user U676706 in cluster 0 (index 237)
2025-04-03 21:01:41,961 - ERROR - Failed on user U177577 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:01:41,962 - INFO - Starting user U565821 in cluster 2 (index 380)
2025-04-03 21:01:45,962 - INFO - Starting user U454528 in cluster 1 (index 241)
2025-04-03 21:01:57,182 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:01:58,381 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:02:02,238 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:02:14,466 - ERROR - Failed on user U565821 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:02:14,467 - INFO - Starting user U28021 in cluster 2 (index 381)
2025-04-03 21:02:30,744 - INFO - Starting user U356108 in cluster 0 (index 238)
2025-04-03 21:02:31,422 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:02:32,623 - INFO - Starting user U273745 in cluster 1 (index 242)
2025-04-03 21:02:44,163 - ERROR - Failed on user U28021 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:02:44,163 - INFO - Starting user U35218 in cluster 2 (index 382)
2025-04-03 21:02:47,122 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:02:49,598 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:03:01,408 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:03:18,826 - ERROR - Failed on user U35218 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:03:18,827 - INFO - Starting user U367100 in cluster 2 (index 383)
2025-04-03 21:03:20,610 - INFO - Starting user U609379 in cluster 0 (index 239)
2025-04-03 21:03:24,715 - INFO - Starting user U483487 in cluster 1 (index 243)
2025-04-03 21:03:35,438 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:03:37,501 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:03:41,140 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:03:48,343 - ERROR - Failed on user U367100 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:03:48,343 - INFO - Starting user U192185 in cluster 2 (index 384)
2025-04-03 21:04:05,867 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:04:09,114 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U309010', 'user_index_in_cluster': 230, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 96, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U530061', 'user_index_in_cluster': 231, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U144830', 'user_index_in_cluster': 232, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 117, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U607560', 'user_index_in_cluster': 233, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U69534', 'user_index_in_cluster': 234, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 59, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0019230769230769232, 'ndcg_1000': 0.05199683869534326, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0019230769230769232, 'ndcg_2000': 0.05199683869534326, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U323165', 'user_index_in_cluster': 235, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 159, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.14285714285714285, 'ap_500': 0.0037174721189591076, 'ndcg_500': 0.034032760759656215, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.0037174721189591076, 'ndcg_1000': 0.034032760759656215, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.42857142857142855, 'ap_2000': 0.0022057359306424484, 'ndcg_2000': 0.08531207355795477, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U682759', 'user_index_in_cluster': 236, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 95, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U676706', 'user_index_in_cluster': 237, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 128, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U356108', 'user_index_in_cluster': 238, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U609379', 'user_index_in_cluster': 239, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 92, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022371364653243847, 'ndcg_500': 0.11354146719982265, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022371364653243847, 'ndcg_1000': 0.11354146719982265, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022371364653243847, 'ndcg_2000': 0.11354146719982265, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:04:09,116 - INFO - Starting user U489483 in cluster 0 (index 240)
2025-04-03 21:04:13,010 - INFO - Starting user U233223 in cluster 1 (index 244)
2025-04-03 21:04:24,606 - ERROR - Failed on user U192185 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:04:24,606 - INFO - Starting user U48372 in cluster 2 (index 385)
2025-04-03 21:04:26,138 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:04:29,555 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:04:41,139 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:04:57,432 - ERROR - Failed on user U48372 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:04:57,432 - INFO - Starting user U346481 in cluster 2 (index 386)
2025-04-03 21:04:59,441 - INFO - Starting user U65705 in cluster 0 (index 241)
2025-04-03 21:05:03,804 - INFO - Starting user U275845 in cluster 1 (index 245)
2025-04-03 21:05:13,657 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:05:15,781 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:05:19,985 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:05:26,469 - ERROR - Failed on user U346481 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:05:26,470 - INFO - Starting user U254432 in cluster 2 (index 387)
2025-04-03 21:05:44,224 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:05:47,227 - INFO - Starting user U606640 in cluster 0 (index 242)
2025-04-03 21:05:51,268 - INFO - Starting user U441958 in cluster 1 (index 246)
2025-04-03 21:06:02,788 - ERROR - Failed on user U254432 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:06:02,788 - INFO - Starting user U344115 in cluster 2 (index 388)
2025-04-03 21:06:03,874 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:06:08,494 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:06:19,422 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:06:34,909 - ERROR - Failed on user U344115 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:06:34,910 - INFO - Starting user U249042 in cluster 2 (index 389)
2025-04-03 21:06:37,172 - INFO - Starting user U92307 in cluster 0 (index 243)
2025-04-03 21:06:40,495 - INFO - Starting user U82405 in cluster 1 (index 247)
2025-04-03 21:06:51,099 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:06:53,937 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:06:57,043 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:07:04,073 - ERROR - Failed on user U249042 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:07:04,074 - INFO - Starting user U276609 in cluster 2 (index 390)
2025-04-03 21:07:20,849 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:07:22,326 - INFO - Starting user U685007 in cluster 1 (index 248)
2025-04-03 21:07:30,475 - INFO - Starting user U44555 in cluster 0 (index 244)
2025-04-03 21:07:38,525 - ERROR - Failed on user U276609 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:07:38,525 - INFO - Starting user U59901 in cluster 2 (index 391)
2025-04-03 21:07:38,977 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:07:46,689 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:07:55,088 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:08:12,388 - INFO - Starting user U638788 in cluster 1 (index 249)
2025-04-03 21:08:14,252 - ERROR - Failed on user U59901 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:08:14,252 - INFO - Starting user U20035 in cluster 2 (index 392)
2025-04-03 21:08:16,030 - INFO - Starting user U650108 in cluster 0 (index 245)
2025-04-03 21:08:30,261 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:08:31,689 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:08:32,766 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:08:47,571 - ERROR - Failed on user U20035 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:08:47,572 - INFO - Starting user U43466 in cluster 2 (index 393)
2025-04-03 21:09:01,197 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U177368', 'user_index_in_cluster': 240, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 188, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0019569471624266144, 'ndcg_1000': 0.06812746586282871, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0019569471624266144, 'ndcg_2000': 0.06812746586282871, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U454528', 'user_index_in_cluster': 241, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 470, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0009039285153709891, 'ndcg_2000': 0.08838676899092424, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U273745', 'user_index_in_cluster': 242, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U483487', 'user_index_in_cluster': 243, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 114, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0016339869281045752, 'ndcg_1000': 0.0366273875652177, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.0016902189753737733, 'ndcg_2000': 0.10142883253584867, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U233223', 'user_index_in_cluster': 244, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 355, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U275845', 'user_index_in_cluster': 245, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 255, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.002288329519450801, 'ndcg_500': 0.05348035488898642, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.002288329519450801, 'ndcg_1000': 0.05348035488898642, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.002288329519450801, 'ndcg_2000': 0.05348035488898642, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U441958', 'user_index_in_cluster': 246, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 211, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0010111223458038423, 'ndcg_1000': 0.04715760220556748, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0010111223458038423, 'ndcg_2000': 0.04715760220556748, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U82405', 'user_index_in_cluster': 247, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 427, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.0037735849056603774, 'ndcg_500': 0.03756571118663525, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.0037735849056603774, 'ndcg_1000': 0.03756571118663525, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.002402788324863212, 'ndcg_2000': 0.06527377119723173, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U685007', 'user_index_in_cluster': 248, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 155, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U638788', 'user_index_in_cluster': 249, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 200, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0015105740181268882, 'ndcg_1000': 0.05006779922820306, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0014911221819847097, 'ndcg_2000': 0.09515004552885717, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:09:01,200 - INFO - Starting user U365287 in cluster 1 (index 250)
2025-04-03 21:09:04,196 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:09:05,786 - INFO - Starting user U347351 in cluster 0 (index 246)
2025-04-03 21:09:17,287 - ERROR - Failed on user U43466 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:09:17,287 - INFO - Starting user U26100 in cluster 2 (index 394)
2025-04-03 21:09:17,466 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:09:22,989 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:09:34,861 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:09:50,216 - INFO - Starting user U310925 in cluster 1 (index 251)
2025-04-03 21:09:52,701 - ERROR - Failed on user U26100 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:09:52,702 - INFO - Starting user U706934 in cluster 2 (index 395)
2025-04-03 21:09:54,087 - INFO - Starting user U267898 in cluster 0 (index 247)
2025-04-03 21:10:06,878 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:10:09,377 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:10:10,676 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:10:24,502 - ERROR - Failed on user U706934 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:10:24,502 - INFO - Starting user U352977 in cluster 2 (index 396)
2025-04-03 21:10:38,257 - INFO - Starting user U311538 in cluster 1 (index 252)
2025-04-03 21:10:41,487 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:10:41,975 - INFO - Starting user U269434 in cluster 0 (index 248)
2025-04-03 21:10:53,682 - ERROR - Failed on user U352977 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:10:53,682 - INFO - Starting user U396717 in cluster 2 (index 397)
2025-04-03 21:10:55,355 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:10:58,999 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 21:11:11,321 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:11:27,095 - ERROR - Failed on user U396717 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:11:27,095 - INFO - Starting user U87669 in cluster 2 (index 398)
2025-04-03 21:11:28,771 - INFO - Starting user U273049 in cluster 1 (index 253)
2025-04-03 21:11:34,410 - INFO - Starting user U694091 in cluster 0 (index 249)
2025-04-03 21:11:43,976 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:11:45,315 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:11:51,284 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:11:57,075 - ERROR - Failed on user U87669 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:11:57,076 - INFO - Starting user U8626 in cluster 2 (index 399)
2025-04-03 21:11:57,182 - INFO - history_titles:[] empty for user: U8626!!!
2025-04-03 21:12:13,675 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:12:18,193 - INFO - Starting user U421192 in cluster 1 (index 254)
2025-04-03 21:12:24,275 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U489483', 'user_index_in_cluster': 240, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U65705', 'user_index_in_cluster': 241, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 77, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U606640', 'user_index_in_cluster': 242, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 95, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U92307', 'user_index_in_cluster': 243, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 79, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U44555', 'user_index_in_cluster': 244, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 112, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U650108', 'user_index_in_cluster': 245, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 127, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U347351', 'user_index_in_cluster': 246, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 59, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U267898', 'user_index_in_cluster': 247, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 128, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U269434', 'user_index_in_cluster': 248, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 155, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U694091', 'user_index_in_cluster': 249, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 86, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:12:24,277 - INFO - Starting user U161406 in cluster 0 (index 250)
2025-04-03 21:12:29,912 - ERROR - Failed on user U8626 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:12:29,913 - INFO - Starting user U259661 in cluster 2 (index 400)
2025-04-03 21:12:35,019 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:12:41,008 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:12:47,000 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:13:05,436 - ERROR - Failed on user U259661 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:13:05,436 - INFO - Starting user U657190 in cluster 2 (index 401)
2025-04-03 21:13:07,488 - INFO - Starting user U262119 in cluster 1 (index 255)
2025-04-03 21:13:11,081 - INFO - Starting user U520421 in cluster 0 (index 251)
2025-04-03 21:13:21,872 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:13:25,193 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:13:27,394 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:13:35,233 - ERROR - Failed on user U657190 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:13:35,233 - INFO - Starting user U310583 in cluster 2 (index 402)
2025-04-03 21:13:52,050 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:13:53,182 - INFO - Starting user U370762 in cluster 0 (index 252)
2025-04-03 21:13:56,297 - INFO - Starting user U675927 in cluster 1 (index 256)
2025-04-03 21:14:07,940 - ERROR - Failed on user U310583 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:14:07,940 - INFO - Starting user U70328 in cluster 2 (index 403)
2025-04-03 21:14:10,310 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:14:12,654 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:14:24,995 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:14:41,035 - ERROR - Failed on user U70328 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:14:41,035 - INFO - Starting user U276240 in cluster 2 (index 404)
2025-04-03 21:14:42,875 - INFO - Starting user U463690 in cluster 0 (index 253)
2025-04-03 21:14:46,709 - INFO - Starting user U371818 in cluster 1 (index 257)
2025-04-03 21:14:57,768 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:14:59,769 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:15:03,400 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:15:10,952 - ERROR - Failed on user U276240 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:15:10,952 - INFO - Starting user U379516 in cluster 2 (index 405)
2025-04-03 21:15:27,193 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:15:31,641 - INFO - Starting user U566644 in cluster 0 (index 254)
2025-04-03 21:15:37,287 - INFO - Starting user U196786 in cluster 1 (index 258)
2025-04-03 21:15:43,259 - ERROR - Failed on user U379516 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:15:43,259 - INFO - Starting user U502642 in cluster 2 (index 406)
2025-04-03 21:15:48,113 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:15:54,491 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:16:00,565 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:16:18,680 - ERROR - Failed on user U502642 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:16:18,681 - INFO - Starting user U219778 in cluster 2 (index 407)
2025-04-03 21:16:20,554 - INFO - Starting user U647661 in cluster 0 (index 255)
2025-04-03 21:16:24,376 - INFO - Starting user U66721 in cluster 1 (index 259)
2025-04-03 21:16:35,100 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:16:37,239 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:16:40,832 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:16:47,964 - ERROR - Failed on user U219778 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:16:47,965 - INFO - Starting user U194742 in cluster 2 (index 408)
2025-04-03 21:17:05,148 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:17:08,573 - INFO - Starting user U178875 in cluster 0 (index 256)
2025-04-03 21:17:13,992 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U365287', 'user_index_in_cluster': 250, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 181, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0010463800904977377, 'ndcg_2000': 0.09031383363248767, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U310925', 'user_index_in_cluster': 251, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 143, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U311538', 'user_index_in_cluster': 252, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 223, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.000946969696969697, 'ndcg_2000': 0.061035423246534826, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U273049', 'user_index_in_cluster': 253, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 233, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U421192', 'user_index_in_cluster': 254, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 290, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U262119', 'user_index_in_cluster': 255, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 225, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0014577259475218659, 'ndcg_1000': 0.041423298149491784, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0013694779641517081, 'ndcg_2000': 0.0782197420239467, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U675927', 'user_index_in_cluster': 256, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 219, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0009587727708533077, 'ndcg_2000': 0.046797280128401335, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U371818', 'user_index_in_cluster': 257, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 210, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.00909090909090909, 'ndcg_200': 0.04991749282721042, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.00909090909090909, 'ndcg_500': 0.04991749282721042, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.00909090909090909, 'ndcg_1000': 0.04991749282721042, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.005260762127714917, 'ndcg_2000': 0.08237245506249186, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U196786', 'user_index_in_cluster': 258, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 210, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U66721', 'user_index_in_cluster': 259, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 423, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:17:13,995 - INFO - Starting user U374957 in cluster 1 (index 260)
2025-04-03 21:17:20,087 - ERROR - Failed on user U194742 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:17:20,087 - INFO - Starting user U226984 in cluster 2 (index 409)
2025-04-03 21:17:25,377 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:17:30,290 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:17:36,804 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:17:51,767 - ERROR - Failed on user U226984 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:17:51,768 - INFO - Starting user U406007 in cluster 2 (index 410)
2025-04-03 21:17:57,775 - INFO - Starting user U229258 in cluster 0 (index 257)
2025-04-03 21:18:01,192 - INFO - Starting user U117399 in cluster 1 (index 261)
2025-04-03 21:18:08,159 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:18:14,171 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:18:17,303 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:18:21,360 - ERROR - Failed on user U406007 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:18:21,360 - INFO - Starting user U174609 in cluster 2 (index 411)
2025-04-03 21:18:38,659 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:18:47,396 - INFO - Starting user U31626 in cluster 0 (index 258)
2025-04-03 21:18:51,212 - INFO - Starting user U96447 in cluster 1 (index 262)
2025-04-03 21:18:53,720 - ERROR - Failed on user U174609 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:18:53,720 - INFO - Starting user U94268 in cluster 2 (index 412)
2025-04-03 21:19:03,945 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:19:08,119 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:19:10,369 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:19:25,973 - ERROR - Failed on user U94268 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:19:25,974 - INFO - Starting user U81295 in cluster 2 (index 413)
2025-04-03 21:19:35,870 - INFO - Starting user U438231 in cluster 0 (index 259)
2025-04-03 21:19:39,441 - INFO - Starting user U366987 in cluster 1 (index 263)
2025-04-03 21:19:42,629 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:19:53,338 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:19:55,623 - ERROR - Failed on user U81295 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:19:55,623 - INFO - Starting user U375247 in cluster 2 (index 414)
2025-04-03 21:19:56,620 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:20:12,509 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:20:28,301 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U161406', 'user_index_in_cluster': 250, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.16666666666666666, 'ap_50': 0.023255813953488372, 'ndcg_50': 0.055427457405235275, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.16666666666666666, 'ap_100': 0.023255813953488372, 'ndcg_100': 0.055427457405235275, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.16666666666666666, 'ap_200': 0.023255813953488372, 'ndcg_200': 0.055427457405235275, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.023255813953488372, 'ndcg_500': 0.055427457405235275, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.023255813953488372, 'ndcg_1000': 0.055427457405235275, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.007535227945154876, 'ndcg_2000': 0.1433639265320788, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U520421', 'user_index_in_cluster': 251, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 89, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0005376344086021505, 'ndcg_2000': 0.04320426012876544, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U370762', 'user_index_in_cluster': 252, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 101, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U463690', 'user_index_in_cluster': 253, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U566644', 'user_index_in_cluster': 254, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 62, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U647661', 'user_index_in_cluster': 255, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 93, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U178875', 'user_index_in_cluster': 256, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 54, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U229258', 'user_index_in_cluster': 257, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 143, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U31626', 'user_index_in_cluster': 258, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U438231', 'user_index_in_cluster': 259, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 79, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:20:28,303 - INFO - Starting user U184091 in cluster 0 (index 260)
2025-04-03 21:20:30,855 - ERROR - Failed on user U375247 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:20:30,855 - INFO - Starting user U671216 in cluster 2 (index 415)
2025-04-03 21:20:33,036 - INFO - Starting user U311780 in cluster 1 (index 264)
2025-04-03 21:20:45,698 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:20:49,148 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:20:50,337 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:21:03,273 - ERROR - Failed on user U671216 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:21:03,274 - INFO - Starting user U120824 in cluster 2 (index 416)
2025-04-03 21:21:17,334 - INFO - Starting user U11858 in cluster 0 (index 261)
2025-04-03 21:21:19,669 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:21:21,729 - INFO - Starting user U146965 in cluster 1 (index 265)
2025-04-03 21:21:33,253 - ERROR - Failed on user U120824 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:21:33,253 - INFO - Starting user U561996 in cluster 2 (index 417)
2025-04-03 21:21:34,042 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:21:39,914 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:21:49,941 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:22:05,319 - ERROR - Failed on user U561996 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:22:05,320 - INFO - Starting user U648671 in cluster 2 (index 418)
2025-04-03 21:22:08,428 - INFO - Starting user U643020 in cluster 0 (index 262)
2025-04-03 21:22:12,764 - INFO - Starting user U341603 in cluster 1 (index 266)
2025-04-03 21:22:21,868 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:22:25,379 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:22:28,882 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:22:35,020 - ERROR - Failed on user U648671 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:22:35,020 - INFO - Starting user U614198 in cluster 2 (index 419)
2025-04-03 21:22:51,457 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:22:56,624 - INFO - Starting user U207692 in cluster 0 (index 263)
2025-04-03 21:23:03,101 - INFO - Starting user U222828 in cluster 1 (index 267)
2025-04-03 21:23:08,451 - ERROR - Failed on user U614198 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:23:08,451 - INFO - Starting user U507923 in cluster 2 (index 420)
2025-04-03 21:23:13,374 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:23:19,324 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:23:24,668 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:23:42,270 - ERROR - Failed on user U507923 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:23:42,270 - INFO - Starting user U139659 in cluster 2 (index 421)
2025-04-03 21:23:44,200 - INFO - Starting user U177714 in cluster 0 (index 264)
2025-04-03 21:23:48,097 - INFO - Starting user U59668 in cluster 1 (index 268)
2025-04-03 21:23:59,392 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:24:00,866 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:24:05,268 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:24:11,427 - ERROR - Failed on user U139659 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:24:11,428 - INFO - Starting user U459766 in cluster 2 (index 422)
2025-04-03 21:24:28,140 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:24:29,391 - INFO - Starting user U73482 in cluster 0 (index 265)
2025-04-03 21:24:33,320 - INFO - Starting user U121987 in cluster 1 (index 269)
2025-04-03 21:24:44,940 - ERROR - Failed on user U459766 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:24:44,940 - INFO - Starting user U155380 in cluster 2 (index 423)
2025-04-03 21:24:45,032 - INFO - history_titles:[] empty for user: U155380!!!
2025-04-03 21:24:46,294 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:24:49,543 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:25:01,676 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:25:17,072 - INFO - Starting user U667182 in cluster 0 (index 266)
2025-04-03 21:25:19,040 - ERROR - Failed on user U155380 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:25:19,040 - INFO - Starting user U487341 in cluster 2 (index 424)
2025-04-03 21:25:20,757 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U374957', 'user_index_in_cluster': 260, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 131, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U117399', 'user_index_in_cluster': 261, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 235, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U96447', 'user_index_in_cluster': 262, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 310, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 1.0, 'ap_1000': 0.0022246669111355578, 'ndcg_1000': 0.13170586714695684, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0022246669111355578, 'ndcg_2000': 0.13170586714695684, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U366987', 'user_index_in_cluster': 263, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 271, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U311780', 'user_index_in_cluster': 264, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 154, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U146965', 'user_index_in_cluster': 265, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 298, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.003934240362811791, 'ndcg_500': 0.07978610252323248, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.003934240362811791, 'ndcg_1000': 0.07978610252323248, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.0031676797398912555, 'ndcg_2000': 0.14445006622228299, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U341603', 'user_index_in_cluster': 266, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 141, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U222828', 'user_index_in_cluster': 267, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 158, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U59668', 'user_index_in_cluster': 268, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 173, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006868131868131869, 'ndcg_2000': 0.05834615355720166, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U121987', 'user_index_in_cluster': 269, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 158, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:25:20,760 - INFO - Starting user U368086 in cluster 1 (index 270)
2025-04-03 21:25:33,809 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:25:35,798 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:25:37,429 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:25:51,426 - ERROR - Failed on user U487341 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:25:51,426 - INFO - Starting user U237295 in cluster 2 (index 425)
2025-04-03 21:26:05,209 - INFO - Starting user U116296 in cluster 0 (index 267)
2025-04-03 21:26:08,207 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:26:09,072 - INFO - Starting user U675702 in cluster 1 (index 271)
2025-04-03 21:26:20,686 - ERROR - Failed on user U237295 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:26:20,686 - INFO - Starting user U365143 in cluster 2 (index 426)
2025-04-03 21:26:21,463 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:26:26,033 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:26:37,915 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:26:53,490 - INFO - Starting user U272092 in cluster 0 (index 268)
2025-04-03 21:26:56,147 - ERROR - Failed on user U365143 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:26:56,147 - INFO - Starting user U205451 in cluster 2 (index 427)
2025-04-03 21:26:58,868 - INFO - Starting user U51406 in cluster 1 (index 272)
2025-04-03 21:27:10,782 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:27:13,230 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:27:15,292 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:27:27,854 - ERROR - Failed on user U205451 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:27:27,854 - INFO - Starting user U481545 in cluster 2 (index 428)
2025-04-03 21:27:41,072 - INFO - Starting user U632032 in cluster 0 (index 269)
2025-04-03 21:27:44,172 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:27:44,922 - INFO - Starting user U536514 in cluster 1 (index 273)
2025-04-03 21:27:56,629 - ERROR - Failed on user U481545 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:27:56,629 - INFO - Starting user U709373 in cluster 2 (index 429)
2025-04-03 21:27:57,650 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 21:28:01,402 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:28:12,948 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:28:28,361 - ERROR - Failed on user U709373 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:28:28,361 - INFO - Starting user U192718 in cluster 2 (index 430)
2025-04-03 21:28:30,526 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U184091', 'user_index_in_cluster': 260, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 84, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002288329519450801, 'ndcg_500': 0.06987601962312767, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002288329519450801, 'ndcg_1000': 0.06987601962312767, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0018449356076581263, 'ndcg_2000': 0.12838365788631062, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U11858', 'user_index_in_cluster': 261, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 94, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U643020', 'user_index_in_cluster': 262, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 79, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U207692', 'user_index_in_cluster': 263, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 89, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U177714', 'user_index_in_cluster': 264, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 168, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.00228310502283105, 'ndcg_500': 0.03447251704825597, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0025620070568700705, 'ndcg_1000': 0.0664550854388272, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0025620070568700705, 'ndcg_2000': 0.0664550854388272, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U73482', 'user_index_in_cluster': 265, 'num_candidates': 4521, 'num_history_articles': 44, 'original_history_len': 44, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U667182', 'user_index_in_cluster': 266, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 86, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.009174311926605505, 'ndcg_200': 0.050013598983283426, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.007898414241448447, 'ndcg_500': 0.09115796958053288, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.8, 'ap_1000': 0.006567335244304414, 'ndcg_1000': 0.16351167551057316, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.006567335244304414, 'ndcg_2000': 0.16351167551057316, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U116296', 'user_index_in_cluster': 267, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U272092', 'user_index_in_cluster': 268, 'num_candidates': 4527, 'num_history_articles': 45, 'original_history_len': 45, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U632032', 'user_index_in_cluster': 269, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 80, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:28:30,529 - INFO - Starting user U112471 in cluster 0 (index 270)
2025-04-03 21:28:34,068 - INFO - Starting user U113016 in cluster 1 (index 274)
2025-04-03 21:28:45,288 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:28:46,672 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:28:50,720 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:28:58,187 - ERROR - Failed on user U192718 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:28:58,188 - INFO - Starting user U529890 in cluster 2 (index 431)
2025-04-03 21:29:15,277 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:29:20,218 - INFO - Starting user U382670 in cluster 0 (index 271)
2025-04-03 21:29:27,605 - INFO - Starting user U686708 in cluster 1 (index 275)
2025-04-03 21:29:31,830 - ERROR - Failed on user U529890 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:29:31,830 - INFO - Starting user U617373 in cluster 2 (index 432)
2025-04-03 21:29:38,459 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:29:43,989 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:29:48,081 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:30:05,149 - ERROR - Failed on user U617373 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:30:05,150 - INFO - Starting user U99732 in cluster 2 (index 433)
2025-04-03 21:30:11,374 - INFO - Starting user U87077 in cluster 0 (index 272)
2025-04-03 21:30:14,811 - INFO - Starting user U259922 in cluster 1 (index 276)
2025-04-03 21:30:21,426 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:30:27,915 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:30:31,389 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:30:34,409 - ERROR - Failed on user U99732 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:30:34,409 - INFO - Starting user U950 in cluster 2 (index 434)
2025-04-03 21:30:51,164 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:31:00,349 - INFO - Starting user U494440 in cluster 0 (index 273)
2025-04-03 21:31:04,039 - INFO - Starting user U499537 in cluster 1 (index 277)
2025-04-03 21:31:06,372 - ERROR - Failed on user U950 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:31:06,373 - INFO - Starting user U325348 in cluster 2 (index 435)
2025-04-03 21:31:16,716 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:31:20,921 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:31:22,803 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:31:38,171 - ERROR - Failed on user U325348 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:31:38,172 - INFO - Starting user U354732 in cluster 2 (index 436)
2025-04-03 21:31:47,857 - INFO - Starting user U447487 in cluster 0 (index 274)
2025-04-03 21:31:51,514 - INFO - Starting user U517335 in cluster 1 (index 278)
2025-04-03 21:31:54,509 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:32:04,237 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:32:07,226 - ERROR - Failed on user U354732 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:32:07,227 - INFO - Starting user U242746 in cluster 2 (index 437)
2025-04-03 21:32:07,783 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:32:23,802 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:32:37,128 - INFO - Starting user U460946 in cluster 0 (index 275)
2025-04-03 21:32:39,796 - INFO - Starting user U267680 in cluster 1 (index 279)
2025-04-03 21:32:42,344 - ERROR - Failed on user U242746 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:32:42,345 - INFO - Starting user U159688 in cluster 2 (index 438)
2025-04-03 21:32:53,443 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:32:56,212 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:32:59,186 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:33:14,745 - ERROR - Failed on user U159688 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:33:14,745 - INFO - Starting user U452203 in cluster 2 (index 439)
2025-04-03 21:33:24,802 - INFO - Starting user U630360 in cluster 0 (index 276)
2025-04-03 21:33:28,933 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U368086', 'user_index_in_cluster': 270, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 550, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U675702', 'user_index_in_cluster': 271, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 223, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0009612262839757925, 'ndcg_2000': 0.07419835836485275, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U51406', 'user_index_in_cluster': 272, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 202, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U536514', 'user_index_in_cluster': 273, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 287, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U113016', 'user_index_in_cluster': 274, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 226, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U686708', 'user_index_in_cluster': 275, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 234, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U259922', 'user_index_in_cluster': 276, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 213, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.25, 'ap_2000': 0.0013222854011864227, 'ndcg_2000': 0.04994525615690273, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U499537', 'user_index_in_cluster': 277, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 168, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.002277904328018223, 'ndcg_500': 0.11387758076461034, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.002277904328018223, 'ndcg_1000': 0.11387758076461034, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.002277904328018223, 'ndcg_2000': 0.11387758076461034, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U517335', 'user_index_in_cluster': 278, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 291, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U267680', 'user_index_in_cluster': 279, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 217, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.002325581395348837, 'ndcg_500': 0.03457703137362822, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.002325581395348837, 'ndcg_1000': 0.03457703137362822, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0017812198875321798, 'ndcg_2000': 0.06296376027600065, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:33:28,936 - INFO - Starting user U699202 in cluster 1 (index 280)
2025-04-03 21:33:30,904 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:33:41,040 - INFO - lens: newsdf:101527candidate_pool:5903,candidate_pool:5903
2025-04-03 21:33:42,493 - ERROR - Failed on user U452203 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:33:42,493 - INFO - Starting user U465911 in cluster 2 (index 440)
2025-04-03 21:33:45,305 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:33:59,566 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:34:14,819 - ERROR - Failed on user U465911 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:34:14,819 - INFO - Starting user U352987 in cluster 2 (index 441)
2025-04-03 21:34:15,089 - INFO - Starting user U136306 in cluster 1 (index 281)
2025-04-03 21:34:31,325 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:34:32,711 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:34:43,109 - ERROR - Failed on user U352987 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:34:43,110 - INFO - Starting user U225166 in cluster 2 (index 442)
2025-04-03 21:34:57,137 - INFO - Starting user U74928 in cluster 1 (index 282)
2025-04-03 21:35:00,687 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:35:12,350 - ERROR - Failed on user U225166 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:35:12,350 - INFO - Starting user U619789 in cluster 2 (index 443)
2025-04-03 21:35:12,628 - INFO - Starting user U639742 in cluster 0 (index 277)
2025-04-03 21:35:14,685 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:35:24,927 - INFO - Starting user U593970 in cluster 1 (index 283)
2025-04-03 21:35:31,983 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:35:32,735 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:35:43,638 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:35:46,785 - ERROR - Failed on user U619789 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:35:46,785 - INFO - Starting user U406891 in cluster 2 (index 444)
2025-04-03 21:36:06,220 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:36:21,716 - INFO - Starting user U374451 in cluster 0 (index 278)
2025-04-03 21:36:22,898 - ERROR - Failed on user U406891 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:36:22,898 - INFO - Starting user U305834 in cluster 2 (index 445)
2025-04-03 21:36:27,229 - INFO - Starting user U379492 in cluster 1 (index 284)
2025-04-03 21:36:41,849 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:36:48,547 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:36:53,678 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:36:56,673 - ERROR - Failed on user U305834 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:36:56,673 - INFO - Starting user U703429 in cluster 2 (index 446)
2025-04-03 21:37:25,010 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:37:25,628 - INFO - Starting user U151456 in cluster 0 (index 279)
2025-04-03 21:37:33,643 - INFO - Starting user U290679 in cluster 1 (index 285)
2025-04-03 21:37:41,617 - ERROR - Failed on user U703429 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:37:41,618 - INFO - Starting user U127858 in cluster 2 (index 447)
2025-04-03 21:37:45,775 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:37:53,940 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:38:00,652 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:38:19,649 - ERROR - Failed on user U127858 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:38:19,649 - INFO - Starting user U668755 in cluster 2 (index 448)
2025-04-03 21:38:31,754 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U112471', 'user_index_in_cluster': 270, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 100, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U382670', 'user_index_in_cluster': 271, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U87077', 'user_index_in_cluster': 272, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 101, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U494440', 'user_index_in_cluster': 273, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U447487', 'user_index_in_cluster': 274, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 130, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U460946', 'user_index_in_cluster': 275, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U630360', 'user_index_in_cluster': 276, 'num_candidates': 4513, 'num_history_articles': 50, 'original_history_len': 58, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U639742', 'user_index_in_cluster': 277, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U374451', 'user_index_in_cluster': 278, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 81, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U151456', 'user_index_in_cluster': 279, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022371364653243847, 'ndcg_500': 0.11354146719982265, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022371364653243847, 'ndcg_1000': 0.11354146719982265, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022371364653243847, 'ndcg_2000': 0.11354146719982265, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:38:31,776 - INFO - Starting user U682028 in cluster 0 (index 280)
2025-04-03 21:38:36,467 - INFO - Starting user U454289 in cluster 1 (index 286)
2025-04-03 21:38:39,132 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:38:49,475 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:38:52,005 - ERROR - Failed on user U668755 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:38:52,005 - INFO - Starting user U516615 in cluster 2 (index 449)
2025-04-03 21:38:54,103 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:39:11,485 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:39:27,249 - ERROR - Failed on user U516615 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:39:27,250 - INFO - Starting user U515234 in cluster 2 (index 450)
2025-04-03 21:39:35,864 - INFO - Starting user U166827 in cluster 0 (index 281)
2025-04-03 21:39:41,666 - INFO - Starting user U494082 in cluster 1 (index 287)
2025-04-03 21:39:45,771 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:39:59,333 - ERROR - Failed on user U515234 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:39:59,334 - INFO - Starting user U115910 in cluster 2 (index 451)
2025-04-03 21:40:08,427 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:40:12,040 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:40:16,491 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:40:33,207 - ERROR - Failed on user U115910 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:40:33,207 - INFO - Starting user U306347 in cluster 2 (index 452)
2025-04-03 21:40:44,229 - INFO - Starting user U359830 in cluster 0 (index 282)
2025-04-03 21:40:48,406 - INFO - Starting user U147373 in cluster 1 (index 288)
2025-04-03 21:40:51,231 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:41:01,512 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:41:02,878 - ERROR - Failed on user U306347 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:41:02,878 - INFO - Starting user U125093 in cluster 2 (index 453)
2025-04-03 21:41:05,365 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:41:20,847 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:41:37,779 - INFO - Starting user U323208 in cluster 0 (index 283)
2025-04-03 21:41:38,805 - ERROR - Failed on user U125093 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:41:38,806 - INFO - Starting user U603109 in cluster 2 (index 454)
2025-04-03 21:41:38,962 - INFO - history_titles:[] empty for user: U603109!!!
2025-04-03 21:41:42,242 - INFO - Starting user U184531 in cluster 1 (index 289)
2025-04-03 21:41:55,159 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:41:55,632 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:41:58,862 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:42:08,682 - ERROR - Failed on user U603109 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:42:08,682 - INFO - Starting user U322538 in cluster 2 (index 455)
2025-04-03 21:42:08,783 - INFO - history_titles:[] empty for user: U322538!!!
2025-04-03 21:42:25,948 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:42:42,410 - ERROR - Failed on user U322538 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:42:42,410 - INFO - Starting user U356454 in cluster 2 (index 456)
2025-04-03 21:42:44,602 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U699202', 'user_index_in_cluster': 280, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0007012622720897616, 'ndcg_2000': 0.05851328114188784, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U136306', 'user_index_in_cluster': 281, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 125, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U74928', 'user_index_in_cluster': 282, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 127, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006802721088435374, 'ndcg_2000': 0.09503371364049186, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U593970', 'user_index_in_cluster': 283, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 287, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002898550724637681, 'ndcg_500': 0.07269403893303004, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002898550724637681, 'ndcg_1000': 0.07269403893303004, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.002898550724637681, 'ndcg_2000': 0.07269403893303004, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U379492', 'user_index_in_cluster': 284, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 253, 'num_future_clicks': 11, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.2727272727272727, 'ap_500': 0.004425698597341337, 'ndcg_500': 0.07060199067056439, 'num_recommendations_500': 500, 'precision_1000': 0.005, 'recall_1000': 0.45454545454545453, 'ap_1000': 0.005010565043048032, 'ndcg_1000': 0.11393782391760766, 'num_recommendations_1000': 1000, 'precision_2000': 0.0035, 'recall_2000': 0.6363636363636364, 'ap_2000': 0.004906861257703144, 'ndcg_2000': 0.1536343169481531, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U290679', 'user_index_in_cluster': 285, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 1.0, 'ap_10': 0.14285714285714285, 'ndcg_10': 0.3333333333333333, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 1.0, 'ap_20': 0.14285714285714285, 'ndcg_20': 0.3333333333333333, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 1.0, 'ap_50': 0.14285714285714285, 'ndcg_50': 0.3333333333333333, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 1.0, 'ap_100': 0.14285714285714285, 'ndcg_100': 0.3333333333333333, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.14285714285714285, 'ndcg_200': 0.3333333333333333, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.14285714285714285, 'ndcg_500': 0.3333333333333333, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.14285714285714285, 'ndcg_1000': 0.3333333333333333, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.14285714285714285, 'ndcg_2000': 0.3333333333333333, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U454289', 'user_index_in_cluster': 286, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 223, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U494082', 'user_index_in_cluster': 287, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 148, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0013736263736263737, 'ndcg_1000': 0.041050398123455, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0013736263736263737, 'ndcg_2000': 0.041050398123455, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U147373', 'user_index_in_cluster': 288, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 197, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U184531', 'user_index_in_cluster': 289, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 328, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:42:44,609 - INFO - Starting user U398274 in cluster 1 (index 290)
2025-04-03 21:42:49,504 - INFO - Starting user U711144 in cluster 0 (index 284)
2025-04-03 21:42:59,081 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:43:02,577 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:43:07,511 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:43:11,818 - ERROR - Failed on user U356454 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:43:11,818 - INFO - Starting user U477429 in cluster 2 (index 457)
2025-04-03 21:43:28,649 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:43:34,744 - INFO - Starting user U285205 in cluster 1 (index 291)
2025-04-03 21:43:39,884 - INFO - Starting user U367744 in cluster 0 (index 285)
2025-04-03 21:43:46,128 - ERROR - Failed on user U477429 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:43:46,128 - INFO - Starting user U94786 in cluster 2 (index 458)
2025-04-03 21:43:51,097 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:43:56,420 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:44:03,370 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:44:22,157 - ERROR - Failed on user U94786 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:44:22,157 - INFO - Starting user U316533 in cluster 2 (index 459)
2025-04-03 21:44:28,438 - INFO - Starting user U328556 in cluster 0 (index 286)
2025-04-03 21:44:32,853 - INFO - Starting user U39094 in cluster 1 (index 292)
2025-04-03 21:44:39,418 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:44:47,249 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:44:52,962 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:44:53,343 - ERROR - Failed on user U316533 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:44:53,343 - INFO - Starting user U128869 in cluster 2 (index 460)
2025-04-03 21:45:13,580 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:45:21,676 - INFO - Starting user U710497 in cluster 0 (index 287)
2025-04-03 21:45:29,885 - INFO - Starting user U678259 in cluster 1 (index 293)
2025-04-03 21:45:33,299 - ERROR - Failed on user U128869 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:45:33,301 - INFO - Starting user U234719 in cluster 2 (index 461)
2025-04-03 21:45:47,771 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:45:56,064 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:45:56,474 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:46:09,309 - ERROR - Failed on user U234719 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:46:09,310 - INFO - Starting user U553797 in cluster 2 (index 462)
2025-04-03 21:46:27,714 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:46:29,671 - INFO - Starting user U172608 in cluster 1 (index 294)
2025-04-03 21:46:34,011 - INFO - Starting user U579610 in cluster 0 (index 288)
2025-04-03 21:46:45,274 - ERROR - Failed on user U553797 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:46:45,275 - INFO - Starting user U612608 in cluster 2 (index 463)
2025-04-03 21:46:48,792 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:46:52,836 - INFO - lens: newsdf:101527candidate_pool:5905,candidate_pool:5905
2025-04-03 21:47:04,967 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:47:22,321 - ERROR - Failed on user U612608 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:47:22,322 - INFO - Starting user U586332 in cluster 2 (index 464)
2025-04-03 21:47:34,428 - INFO - Starting user U685362 in cluster 1 (index 295)
2025-04-03 21:47:40,402 - INFO - Starting user U31902 in cluster 0 (index 289)
2025-04-03 21:47:42,873 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:47:53,560 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:47:55,085 - ERROR - Failed on user U586332 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:47:55,085 - INFO - Starting user U141274 in cluster 2 (index 465)
2025-04-03 21:47:59,052 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:48:14,363 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:48:30,748 - ERROR - Failed on user U141274 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:48:30,749 - INFO - Starting user U591296 in cluster 2 (index 466)
2025-04-03 21:48:37,101 - INFO - Starting user U165264 in cluster 1 (index 296)
2025-04-03 21:48:42,880 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U682028', 'user_index_in_cluster': 280, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U166827', 'user_index_in_cluster': 281, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 62, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U359830', 'user_index_in_cluster': 282, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 53, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U323208', 'user_index_in_cluster': 283, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 78, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U711144', 'user_index_in_cluster': 284, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 73, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U367744', 'user_index_in_cluster': 285, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 94, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U328556', 'user_index_in_cluster': 286, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U710497', 'user_index_in_cluster': 287, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 231, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U579610', 'user_index_in_cluster': 288, 'num_candidates': 4515, 'num_history_articles': 50, 'original_history_len': 123, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U31902', 'user_index_in_cluster': 289, 'num_candidates': 4526, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:48:42,894 - INFO - Starting user U144625 in cluster 0 (index 290)
2025-04-03 21:48:49,277 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:48:56,470 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:49:02,300 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:49:02,593 - ERROR - Failed on user U591296 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:49:02,593 - INFO - Starting user U215002 in cluster 2 (index 467)
2025-04-03 21:49:22,605 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:49:33,156 - INFO - Starting user U694675 in cluster 1 (index 297)
2025-04-03 21:49:37,384 - INFO - Starting user U291447 in cluster 0 (index 291)
2025-04-03 21:49:38,360 - ERROR - Failed on user U215002 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:49:38,361 - INFO - Starting user U359886 in cluster 2 (index 468)
2025-04-03 21:49:51,395 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:49:56,146 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 21:49:59,871 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:50:18,861 - ERROR - Failed on user U359886 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:50:18,861 - INFO - Starting user U116173 in cluster 2 (index 469)
2025-04-03 21:50:32,846 - INFO - Starting user U252887 in cluster 0 (index 292)
2025-04-03 21:50:34,892 - INFO - Starting user U200059 in cluster 1 (index 298)
2025-04-03 21:50:38,097 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:50:51,177 - ERROR - Failed on user U116173 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:50:51,177 - INFO - Starting user U267799 in cluster 2 (index 470)
2025-04-03 21:50:52,329 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:50:53,291 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:51:10,644 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:51:28,592 - INFO - Starting user U216935 in cluster 0 (index 293)
2025-04-03 21:51:30,054 - ERROR - Failed on user U267799 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:51:30,055 - INFO - Starting user U440210 in cluster 2 (index 471)
2025-04-03 21:51:32,408 - INFO - Starting user U102335 in cluster 1 (index 299)
2025-04-03 21:51:48,073 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 21:51:48,992 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:51:52,062 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 21:52:07,292 - ERROR - Failed on user U440210 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:52:07,292 - INFO - Starting user U565461 in cluster 2 (index 472)
2025-04-03 21:52:21,941 - INFO - Starting user U671075 in cluster 0 (index 294)
2025-04-03 21:52:25,983 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U398274', 'user_index_in_cluster': 290, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 185, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U285205', 'user_index_in_cluster': 291, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 138, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006635700066357001, 'ndcg_2000': 0.09471113409444935, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U39094', 'user_index_in_cluster': 292, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U678259', 'user_index_in_cluster': 293, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 440, 'num_future_clicks': 11, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.09090909090909091, 'ap_50': 0.034482758620689655, 'ndcg_50': 0.042259191411409554, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.09090909090909091, 'ap_100': 0.034482758620689655, 'ndcg_100': 0.042259191411409554, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.09090909090909091, 'ap_200': 0.034482758620689655, 'ndcg_200': 0.042259191411409554, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.2727272727272727, 'ap_500': 0.01628362356055467, 'ndcg_500': 0.0916067953652781, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.2727272727272727, 'ap_1000': 0.01628362356055467, 'ndcg_1000': 0.0916067953652781, 'num_recommendations_1000': 1000, 'precision_2000': 0.0035, 'recall_2000': 0.6363636363636364, 'ap_2000': 0.008960469313371836, 'ndcg_2000': 0.16986680007682434, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U172608', 'user_index_in_cluster': 294, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 198, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U685362', 'user_index_in_cluster': 295, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 133, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U165264', 'user_index_in_cluster': 296, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 139, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U694675', 'user_index_in_cluster': 297, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 244, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U200059', 'user_index_in_cluster': 298, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 160, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0005614823133071309, 'ndcg_2000': 0.056776664805510337, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U102335', 'user_index_in_cluster': 299, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 164, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.001984126984126984, 'ndcg_1000': 0.05225739777934809, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0016569571090847687, 'ndcg_2000': 0.09671541513428553, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:52:25,990 - INFO - Starting user U521387 in cluster 1 (index 300)
2025-04-03 21:52:26,581 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:52:38,432 - ERROR - Failed on user U565461 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:52:38,433 - INFO - Starting user U667062 in cluster 2 (index 473)
2025-04-03 21:52:40,997 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:52:45,335 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:52:58,039 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:53:13,754 - ERROR - Failed on user U667062 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:53:13,755 - INFO - Starting user U433199 in cluster 2 (index 474)
2025-04-03 21:53:24,537 - INFO - Starting user U162008 in cluster 0 (index 295)
2025-04-03 21:53:29,449 - INFO - Starting user U405532 in cluster 1 (index 301)
2025-04-03 21:53:33,161 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:53:43,114 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:53:46,120 - ERROR - Failed on user U433199 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:53:46,120 - INFO - Starting user U24182 in cluster 2 (index 475)
2025-04-03 21:53:48,355 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:54:07,001 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:54:24,539 - INFO - Starting user U513015 in cluster 1 (index 302)
2025-04-03 21:54:25,389 - ERROR - Failed on user U24182 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:54:25,389 - INFO - Starting user U3735 in cluster 2 (index 476)
2025-04-03 21:54:29,149 - INFO - Starting user U313058 in cluster 0 (index 296)
2025-04-03 21:54:44,288 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 21:54:44,800 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:54:52,099 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 21:54:58,097 - ERROR - Failed on user U3735 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:54:58,097 - INFO - Starting user U153215 in cluster 2 (index 477)
2025-04-03 21:55:21,363 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:55:26,730 - INFO - Starting user U250392 in cluster 0 (index 297)
2025-04-03 21:55:35,465 - INFO - Starting user U271808 in cluster 1 (index 303)
2025-04-03 21:55:37,969 - ERROR - Failed on user U153215 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:55:37,970 - INFO - Starting user U487217 in cluster 2 (index 478)
2025-04-03 21:55:45,312 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:55:54,439 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:55:57,174 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:56:14,007 - ERROR - Failed on user U487217 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:56:14,007 - INFO - Starting user U142074 in cluster 2 (index 479)
2025-04-03 21:56:14,112 - INFO - history_titles:[] empty for user: U142074!!!
2025-04-03 21:56:21,109 - INFO - Starting user U18219 in cluster 0 (index 298)
2025-04-03 21:56:25,250 - INFO - Starting user U664682 in cluster 1 (index 304)
2025-04-03 21:56:32,372 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:56:38,639 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:56:42,392 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:56:45,623 - ERROR - Failed on user U142074 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:56:45,623 - INFO - Starting user U111204 in cluster 2 (index 480)
2025-04-03 21:57:03,955 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:57:13,153 - INFO - Starting user U4354 in cluster 0 (index 299)
2025-04-03 21:57:16,766 - INFO - Starting user U30913 in cluster 1 (index 305)
2025-04-03 21:57:18,926 - ERROR - Failed on user U111204 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:57:18,926 - INFO - Starting user U287841 in cluster 2 (index 481)
2025-04-03 21:57:30,599 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:57:34,175 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:57:36,598 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 21:57:54,269 - ERROR - Failed on user U287841 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:57:54,269 - INFO - Starting user U11618 in cluster 2 (index 482)
2025-04-03 21:58:15,504 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:58:18,017 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U144625', 'user_index_in_cluster': 290, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 55, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002304147465437788, 'ndcg_500': 0.06995506852758868, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002304147465437788, 'ndcg_1000': 0.06995506852758868, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.002304147465437788, 'ndcg_2000': 0.06995506852758868, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U291447', 'user_index_in_cluster': 291, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 104, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U252887', 'user_index_in_cluster': 292, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0008488964346349745, 'ndcg_2000': 0.09800704628598009, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U216935', 'user_index_in_cluster': 293, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 52, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U671075', 'user_index_in_cluster': 294, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 63, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U162008', 'user_index_in_cluster': 295, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 185, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U313058', 'user_index_in_cluster': 296, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 51, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U250392', 'user_index_in_cluster': 297, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 115, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.00228310502283105, 'ndcg_500': 0.04447215993108565, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.00228310502283105, 'ndcg_1000': 0.04447215993108565, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0017842260332664246, 'ndcg_2000': 0.08128465373060091, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U18219', 'user_index_in_cluster': 298, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 91, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U4354', 'user_index_in_cluster': 299, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 172, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.002150537634408602, 'ndcg_500': 0.03826185503540596, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.002150537634408602, 'ndcg_1000': 0.03826185503540596, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.002150537634408602, 'ndcg_2000': 0.03826185503540596, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 21:58:18,036 - INFO - Starting user U2674 in cluster 0 (index 300)
2025-04-03 21:58:26,362 - INFO - Starting user U423915 in cluster 1 (index 306)
2025-04-03 21:58:38,277 - ERROR - Failed on user U11618 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:58:38,278 - INFO - Starting user U286269 in cluster 2 (index 483)
2025-04-03 21:58:40,517 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:58:49,892 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:59:00,473 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 21:59:18,667 - ERROR - Failed on user U286269 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:59:18,668 - INFO - Starting user U451258 in cluster 2 (index 484)
2025-04-03 21:59:30,236 - INFO - Starting user U488857 in cluster 0 (index 301)
2025-04-03 21:59:36,683 - INFO - Starting user U167055 in cluster 1 (index 307)
2025-04-03 21:59:38,465 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 21:59:50,220 - ERROR - Failed on user U451258 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 21:59:50,220 - INFO - Starting user U546551 in cluster 2 (index 485)
2025-04-03 21:59:50,328 - INFO - history_titles:[] empty for user: U546551!!!
2025-04-03 21:59:50,417 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 21:59:55,446 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:00:09,973 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:00:25,715 - INFO - Starting user U219301 in cluster 0 (index 302)
2025-04-03 22:00:27,540 - ERROR - Failed on user U546551 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:00:27,540 - INFO - Starting user U132341 in cluster 2 (index 486)
2025-04-03 22:00:30,341 - INFO - Starting user U151980 in cluster 1 (index 308)
2025-04-03 22:00:44,205 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:00:48,608 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:00:49,733 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:01:06,717 - ERROR - Failed on user U132341 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:01:06,718 - INFO - Starting user U337455 in cluster 2 (index 487)
2025-04-03 22:01:17,623 - INFO - Starting user U219307 in cluster 0 (index 303)
2025-04-03 22:01:21,232 - INFO - Starting user U82472 in cluster 1 (index 309)
2025-04-03 22:01:26,247 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:01:39,801 - ERROR - Failed on user U337455 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:01:39,802 - INFO - Starting user U75905 in cluster 2 (index 488)
2025-04-03 22:01:44,478 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:01:46,011 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:01:58,573 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:02:16,319 - ERROR - Failed on user U75905 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:02:16,320 - INFO - Starting user U589238 in cluster 2 (index 489)
2025-04-03 22:02:21,841 - INFO - Starting user U28960 in cluster 0 (index 304)
2025-04-03 22:02:26,771 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U521387', 'user_index_in_cluster': 300, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 62, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U405532', 'user_index_in_cluster': 301, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 201, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U513015', 'user_index_in_cluster': 302, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 85, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.024390243902439025, 'ndcg_50': 0.08702728145052864, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.024390243902439025, 'ndcg_100': 0.08702728145052864, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.024390243902439025, 'ndcg_200': 0.08702728145052864, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.024390243902439025, 'ndcg_500': 0.08702728145052864, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.013623693379790941, 'ndcg_1000': 0.13666922487848923, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.013623693379790941, 'ndcg_2000': 0.13666922487848923, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U271808', 'user_index_in_cluster': 303, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 208, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U664682', 'user_index_in_cluster': 304, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 235, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U30913', 'user_index_in_cluster': 305, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 257, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U423915', 'user_index_in_cluster': 306, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 153, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U167055', 'user_index_in_cluster': 307, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.002145922746781116, 'ndcg_500': 0.11277416999668409, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.002145922746781116, 'ndcg_1000': 0.11277416999668409, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.002145922746781116, 'ndcg_2000': 0.11277416999668409, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U151980', 'user_index_in_cluster': 308, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 141, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.002288329519450801, 'ndcg_500': 0.05348035488898642, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.0025706982832632034, 'ndcg_1000': 0.10311150091796972, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0025706982832632034, 'ndcg_2000': 0.10311150091796972, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U82472', 'user_index_in_cluster': 309, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 99, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:02:26,776 - INFO - Starting user U558804 in cluster 1 (index 310)
2025-04-03 22:02:35,725 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:02:40,646 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:02:45,444 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:02:48,750 - ERROR - Failed on user U589238 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:02:48,751 - INFO - Starting user U411478 in cluster 2 (index 490)
2025-04-03 22:03:09,263 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 22:03:13,608 - INFO - Starting user U431998 in cluster 0 (index 305)
2025-04-03 22:03:17,335 - INFO - Starting user U173896 in cluster 1 (index 311)
2025-04-03 22:03:29,025 - ERROR - Failed on user U411478 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:03:29,026 - INFO - Starting user U509772 in cluster 2 (index 491)
2025-04-03 22:03:31,834 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:03:35,954 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:03:48,773 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:04:06,341 - ERROR - Failed on user U509772 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:04:06,342 - INFO - Starting user U561839 in cluster 2 (index 492)
2025-04-03 22:04:09,206 - INFO - Starting user U30587 in cluster 0 (index 306)
2025-04-03 22:04:13,011 - INFO - Starting user U99270 in cluster 1 (index 312)
2025-04-03 22:04:25,170 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:04:28,481 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:04:32,380 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:04:38,730 - ERROR - Failed on user U561839 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:04:38,731 - INFO - Starting user U291453 in cluster 2 (index 493)
2025-04-03 22:04:58,730 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:05:02,682 - INFO - Starting user U68395 in cluster 0 (index 307)
2025-04-03 22:05:06,640 - INFO - Starting user U294605 in cluster 1 (index 313)
2025-04-03 22:05:18,163 - ERROR - Failed on user U291453 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:05:18,163 - INFO - Starting user U369756 in cluster 2 (index 494)
2025-04-03 22:05:20,635 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:05:24,641 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:05:37,123 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:05:51,869 - INFO - Starting user U467663 in cluster 1 (index 314)
2025-04-03 22:05:55,611 - INFO - Starting user U324611 in cluster 0 (index 308)
2025-04-03 22:05:56,496 - ERROR - Failed on user U369756 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:05:56,497 - INFO - Starting user U386500 in cluster 2 (index 495)
2025-04-03 22:06:10,183 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:06:14,109 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:06:15,313 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:06:31,769 - ERROR - Failed on user U386500 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:06:31,769 - INFO - Starting user U416844 in cluster 2 (index 496)
2025-04-03 22:06:44,417 - INFO - Starting user U157942 in cluster 1 (index 315)
2025-04-03 22:06:48,444 - INFO - Starting user U604673 in cluster 0 (index 309)
2025-04-03 22:06:51,167 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:07:03,064 - ERROR - Failed on user U416844 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:07:03,064 - INFO - Starting user U104292 in cluster 2 (index 497)
2025-04-03 22:07:03,650 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:07:07,309 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:07:22,258 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:07:38,359 - INFO - Starting user U132243 in cluster 1 (index 316)
2025-04-03 22:07:40,024 - ERROR - Failed on user U104292 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:07:40,025 - INFO - Starting user U243674 in cluster 2 (index 498)
2025-04-03 22:07:43,185 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U2674', 'user_index_in_cluster': 300, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 64, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U488857', 'user_index_in_cluster': 301, 'num_candidates': 4522, 'num_history_articles': 44, 'original_history_len': 44, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U219301', 'user_index_in_cluster': 302, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 83, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U219307', 'user_index_in_cluster': 303, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 100, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U28960', 'user_index_in_cluster': 304, 'num_candidates': 4525, 'num_history_articles': 40, 'original_history_len': 40, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U431998', 'user_index_in_cluster': 305, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 87, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U30587', 'user_index_in_cluster': 306, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 131, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U68395', 'user_index_in_cluster': 307, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 77, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U324611', 'user_index_in_cluster': 308, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 55, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U604673', 'user_index_in_cluster': 309, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 94, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:07:43,195 - INFO - Starting user U598796 in cluster 0 (index 310)
2025-04-03 22:07:56,843 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:07:58,795 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:08:01,602 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:08:15,198 - ERROR - Failed on user U243674 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:08:15,198 - INFO - Starting user U550354 in cluster 2 (index 499)
2025-04-03 22:08:30,279 - INFO - Starting user U448242 in cluster 1 (index 317)
2025-04-03 22:08:34,056 - INFO - Starting user U515250 in cluster 0 (index 311)
2025-04-03 22:08:34,446 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:08:46,261 - ERROR - Failed on user U550354 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:08:46,261 - INFO - Starting user U611388 in cluster 2 (index 500)
2025-04-03 22:08:48,667 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:08:52,767 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:09:05,691 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:09:22,826 - INFO - Starting user U426280 in cluster 1 (index 318)
2025-04-03 22:09:24,571 - ERROR - Failed on user U611388 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:09:24,572 - INFO - Starting user U228900 in cluster 2 (index 501)
2025-04-03 22:09:28,832 - INFO - Starting user U430652 in cluster 0 (index 312)
2025-04-03 22:09:43,660 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:09:44,863 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:09:48,649 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:10:03,095 - ERROR - Failed on user U228900 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:10:03,095 - INFO - Starting user U124155 in cluster 2 (index 502)
2025-04-03 22:10:21,745 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:10:24,992 - INFO - Starting user U653729 in cluster 0 (index 313)
2025-04-03 22:10:28,520 - INFO - Starting user U641153 in cluster 1 (index 319)
2025-04-03 22:10:40,225 - ERROR - Failed on user U124155 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:10:40,226 - INFO - Starting user U6423 in cluster 2 (index 503)
2025-04-03 22:10:44,075 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:10:48,520 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:11:00,938 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:11:17,443 - ERROR - Failed on user U6423 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:11:17,444 - INFO - Starting user U567884 in cluster 2 (index 504)
2025-04-03 22:11:20,013 - INFO - Starting user U342883 in cluster 0 (index 314)
2025-04-03 22:11:24,646 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U558804', 'user_index_in_cluster': 310, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 132, 'num_future_clicks': 3, 'precision_5': 0.2, 'recall_5': 0.3333333333333333, 'ap_5': 0.3333333333333333, 'ndcg_5': 0.23463936301137822, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.3333333333333333, 'ap_10': 0.3333333333333333, 'ndcg_10': 0.23463936301137822, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.3333333333333333, 'ap_20': 0.3333333333333333, 'ndcg_20': 0.23463936301137822, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.3333333333333333, 'ndcg_50': 0.23463936301137822, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.3333333333333333, 'ndcg_100': 0.23463936301137822, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.3333333333333333, 'ndcg_200': 0.23463936301137822, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.3333333333333333, 'ndcg_500': 0.23463936301137822, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.16789215686274508, 'ndcg_1000': 0.2831476738879313, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.16789215686274508, 'ndcg_2000': 0.2831476738879313, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U173896', 'user_index_in_cluster': 311, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 171, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U99270', 'user_index_in_cluster': 312, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 181, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.002109704641350211, 'ndcg_500': 0.04390345773377356, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.00247935374517653, 'ndcg_1000': 0.08518127608366595, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.00247935374517653, 'ndcg_2000': 0.08518127608366595, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U294605', 'user_index_in_cluster': 313, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 418, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U467663', 'user_index_in_cluster': 314, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 211, 'num_future_clicks': 6, 'precision_5': 0.2, 'recall_5': 0.16666666666666666, 'ap_5': 0.3333333333333333, 'ndcg_5': 0.16958010263680806, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.16666666666666666, 'ap_10': 0.3333333333333333, 'ndcg_10': 0.15130120674940672, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.16666666666666666, 'ap_20': 0.3333333333333333, 'ndcg_20': 0.15130120674940672, 'num_recommendations_20': 20, 'precision_50': 0.04, 'recall_50': 0.3333333333333333, 'ap_50': 0.19105691056910568, 'ndcg_50': 0.20741852881589534, 'num_recommendations_50': 50, 'precision_100': 0.02, 'recall_100': 0.3333333333333333, 'ap_100': 0.19105691056910568, 'ndcg_100': 0.20741852881589534, 'num_recommendations_100': 100, 'precision_200': 0.01, 'recall_200': 0.3333333333333333, 'ap_200': 0.19105691056910568, 'ndcg_200': 0.20741852881589534, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.3333333333333333, 'ap_500': 0.19105691056910568, 'ndcg_500': 0.20741852881589534, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.19105691056910568, 'ndcg_1000': 0.20741852881589534, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.19105691056910568, 'ndcg_2000': 0.20741852881589534, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U157942', 'user_index_in_cluster': 315, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U132243', 'user_index_in_cluster': 316, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 148, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.16666666666666666, 'ap_500': 0.002242152466367713, 'ndcg_500': 0.034370503207043615, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.16666666666666666, 'ap_1000': 0.002242152466367713, 'ndcg_1000': 0.034370503207043615, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.5, 'ap_2000': 0.0021067025016682575, 'ndcg_2000': 0.09350605009426795, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U448242', 'user_index_in_cluster': 317, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 268, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U426280', 'user_index_in_cluster': 318, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 159, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U641153', 'user_index_in_cluster': 319, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 282, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0033444816053511705, 'ndcg_500': 0.07451217675674772, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0033444816053511705, 'ndcg_1000': 0.07451217675674772, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0033444816053511705, 'ndcg_2000': 0.07451217675674772, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:11:24,657 - INFO - Starting user U546368 in cluster 1 (index 320)
2025-04-03 22:11:36,736 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:11:39,652 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:11:43,733 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:11:49,791 - ERROR - Failed on user U567884 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:11:49,791 - INFO - Starting user U415765 in cluster 2 (index 505)
2025-04-03 22:12:10,737 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:12:14,465 - INFO - Starting user U500250 in cluster 0 (index 315)
2025-04-03 22:12:18,552 - INFO - Starting user U271875 in cluster 1 (index 321)
2025-04-03 22:12:29,773 - ERROR - Failed on user U415765 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:12:29,773 - INFO - Starting user U76193 in cluster 2 (index 506)
2025-04-03 22:12:34,333 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 22:12:38,090 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:12:48,510 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:13:07,972 - ERROR - Failed on user U76193 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:13:07,973 - INFO - Starting user U216270 in cluster 2 (index 507)
2025-04-03 22:13:14,135 - INFO - Starting user U555902 in cluster 1 (index 322)
2025-04-03 22:13:18,395 - INFO - Starting user U29113 in cluster 0 (index 316)
2025-04-03 22:13:26,921 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:13:32,757 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:13:36,831 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:13:40,317 - ERROR - Failed on user U216270 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:13:40,317 - INFO - Starting user U261823 in cluster 2 (index 508)
2025-04-03 22:14:01,361 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:14:07,816 - INFO - Starting user U110051 in cluster 1 (index 323)
2025-04-03 22:14:13,657 - INFO - Starting user U381100 in cluster 0 (index 317)
2025-04-03 22:14:18,958 - ERROR - Failed on user U261823 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:14:18,959 - INFO - Starting user U179902 in cluster 2 (index 509)
2025-04-03 22:14:26,777 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:14:33,320 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:14:39,075 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:14:56,009 - ERROR - Failed on user U179902 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:14:56,010 - INFO - Starting user U299267 in cluster 2 (index 510)
2025-04-03 22:14:56,131 - INFO - history_titles:[] empty for user: U299267!!!
2025-04-03 22:15:04,457 - INFO - Starting user U17172 in cluster 1 (index 324)
2025-04-03 22:15:08,090 - INFO - Starting user U351890 in cluster 0 (index 318)
2025-04-03 22:15:15,640 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:15:27,517 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:15:27,650 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:15:30,803 - ERROR - Failed on user U299267 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:15:30,804 - INFO - Starting user U273933 in cluster 2 (index 511)
2025-04-03 22:15:54,984 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:16:02,667 - INFO - Starting user U385831 in cluster 1 (index 325)
2025-04-03 22:16:09,176 - INFO - Starting user U144118 in cluster 0 (index 319)
2025-04-03 22:16:13,734 - ERROR - Failed on user U273933 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:16:13,735 - INFO - Starting user U148117 in cluster 2 (index 512)
2025-04-03 22:16:20,974 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:16:32,561 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:16:32,604 - INFO - Starting user U154052 in cluster 1 (index 326)
2025-04-03 22:16:33,035 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:16:41,527 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U598796', 'user_index_in_cluster': 310, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 76, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U515250', 'user_index_in_cluster': 311, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 52, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U430652', 'user_index_in_cluster': 312, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 50, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U653729', 'user_index_in_cluster': 313, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 68, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U342883', 'user_index_in_cluster': 314, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 138, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U500250', 'user_index_in_cluster': 315, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 68, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.14285714285714285, 'ap_10': 0.125, 'ndcg_10': 0.08671382849411972, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.14285714285714285, 'ap_20': 0.125, 'ndcg_20': 0.08671382849411972, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.14285714285714285, 'ap_50': 0.125, 'ndcg_50': 0.08671382849411972, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.14285714285714285, 'ap_100': 0.125, 'ndcg_100': 0.08671382849411972, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.14285714285714285, 'ap_200': 0.125, 'ndcg_200': 0.08671382849411972, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2857142857142857, 'ap_500': 0.06479885057471264, 'ndcg_500': 0.11806311618388143, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.5714285714285714, 'ap_1000': 0.035030859747995184, 'ndcg_1000': 0.17680673388280907, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.7142857142857143, 'ap_2000': 0.02885250236793257, 'ndcg_2000': 0.2036511782356921, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U29113', 'user_index_in_cluster': 316, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 97, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0005733944954128441, 'ndcg_2000': 0.05693625667978205, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U381100', 'user_index_in_cluster': 317, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 53, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0006600660066006601, 'ndcg_2000': 0.04441381090115882, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U351890', 'user_index_in_cluster': 318, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 87, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0027100271002710027, 'ndcg_500': 0.11721431102665532, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0027100271002710027, 'ndcg_1000': 0.11721431102665532, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0027100271002710027, 'ndcg_2000': 0.11721431102665532, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U144118', 'user_index_in_cluster': 319, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:16:41,534 - INFO - Starting user U28356 in cluster 0 (index 320)
2025-04-03 22:16:44,302 - ERROR - Failed on user U148117 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:16:44,303 - INFO - Starting user U387632 in cluster 2 (index 513)
2025-04-03 22:16:51,093 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:17:00,795 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:17:03,651 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:17:20,728 - ERROR - Failed on user U387632 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:17:20,729 - INFO - Starting user U169395 in cluster 2 (index 514)
2025-04-03 22:17:32,256 - INFO - Starting user U113873 in cluster 0 (index 321)
2025-04-03 22:17:36,167 - INFO - Starting user U169513 in cluster 1 (index 327)
2025-04-03 22:17:42,223 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:17:51,578 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:17:55,016 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:17:55,604 - ERROR - Failed on user U169395 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:17:55,605 - INFO - Starting user U572421 in cluster 2 (index 515)
2025-04-03 22:18:15,180 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 22:18:26,465 - INFO - Starting user U591805 in cluster 0 (index 322)
2025-04-03 22:18:32,211 - ERROR - Failed on user U572421 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:18:32,211 - INFO - Starting user U266572 in cluster 2 (index 516)
2025-04-03 22:18:33,513 - INFO - Starting user U583444 in cluster 1 (index 328)
2025-04-03 22:18:45,926 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:18:51,580 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:18:53,084 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 22:19:04,614 - ERROR - Failed on user U266572 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:19:04,615 - INFO - Starting user U172400 in cluster 2 (index 517)
2025-04-03 22:19:13,965 - INFO - Starting user U493516 in cluster 0 (index 323)
2025-04-03 22:19:19,515 - INFO - Starting user U210679 in cluster 1 (index 329)
2025-04-03 22:19:24,757 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:19:33,707 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:19:38,163 - ERROR - Failed on user U172400 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:19:38,163 - INFO - Starting user U440174 in cluster 2 (index 518)
2025-04-03 22:19:39,217 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:19:58,487 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 22:20:08,831 - INFO - Starting user U539681 in cluster 0 (index 324)
2025-04-03 22:20:14,052 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U546368', 'user_index_in_cluster': 320, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 275, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U271875', 'user_index_in_cluster': 321, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 277, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.002544529262086514, 'ndcg_500': 0.11598167361316676, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.002544529262086514, 'ndcg_1000': 0.11598167361316676, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.002544529262086514, 'ndcg_2000': 0.11598167361316676, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U555902', 'user_index_in_cluster': 322, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 190, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U110051', 'user_index_in_cluster': 323, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 263, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U17172', 'user_index_in_cluster': 324, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 233, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U385831', 'user_index_in_cluster': 325, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022222222222222222, 'ndcg_500': 0.11341747276953465, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022222222222222222, 'ndcg_1000': 0.11341747276953465, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022222222222222222, 'ndcg_2000': 0.11341747276953465, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U154052', 'user_index_in_cluster': 326, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 101, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U169513', 'user_index_in_cluster': 327, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 167, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.04, 'recall_50': 0.4, 'ap_50': 0.034922394678492244, 'ndcg_50': 0.12465389855057418, 'num_recommendations_50': 50, 'precision_100': 0.02, 'recall_100': 0.4, 'ap_100': 0.034922394678492244, 'ndcg_100': 0.12465389855057418, 'num_recommendations_100': 100, 'precision_200': 0.01, 'recall_200': 0.4, 'ap_200': 0.034922394678492244, 'ndcg_200': 0.12465389855057418, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.6, 'ap_500': 0.02587899904973076, 'ndcg_500': 0.1641257520381827, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.8, 'ap_1000': 0.020771647107461558, 'ndcg_1000': 0.19974583580673208, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.020771647107461558, 'ndcg_2000': 0.19974583580673208, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U583444', 'user_index_in_cluster': 328, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 117, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U210679', 'user_index_in_cluster': 329, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 102, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:20:14,063 - INFO - Starting user U161113 in cluster 1 (index 330)
2025-04-03 22:20:14,765 - ERROR - Failed on user U440174 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:20:14,766 - INFO - Starting user U155390 in cluster 2 (index 519)
2025-04-03 22:20:27,514 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:20:32,835 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:20:33,680 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:20:50,264 - ERROR - Failed on user U155390 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:20:50,265 - INFO - Starting user U51456 in cluster 2 (index 520)
2025-04-03 22:21:07,480 - INFO - Starting user U572541 in cluster 1 (index 331)
2025-04-03 22:21:10,651 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:21:11,133 - INFO - Starting user U247319 in cluster 0 (index 325)
2025-04-03 22:21:23,408 - ERROR - Failed on user U51456 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:21:23,409 - INFO - Starting user U454020 in cluster 2 (index 521)
2025-04-03 22:21:32,713 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:21:35,830 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:21:42,687 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:21:58,159 - ERROR - Failed on user U454020 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:21:58,160 - INFO - Starting user U172052 in cluster 2 (index 522)
2025-04-03 22:22:09,270 - INFO - Starting user U516258 in cluster 1 (index 332)
2025-04-03 22:22:13,800 - INFO - Starting user U318193 in cluster 0 (index 326)
2025-04-03 22:22:17,796 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:22:28,488 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:22:29,837 - ERROR - Failed on user U172052 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:22:29,838 - INFO - Starting user U154366 in cluster 2 (index 523)
2025-04-03 22:22:29,948 - INFO - history_titles:[] empty for user: U154366!!!
2025-04-03 22:22:33,497 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:22:50,451 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:23:04,896 - INFO - Starting user U370163 in cluster 1 (index 333)
2025-04-03 22:23:05,970 - ERROR - Failed on user U154366 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:23:05,970 - INFO - Starting user U335480 in cluster 2 (index 524)
2025-04-03 22:23:12,688 - INFO - Starting user U314826 in cluster 0 (index 327)
2025-04-03 22:23:22,796 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:23:24,347 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:23:31,481 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:23:40,958 - ERROR - Failed on user U335480 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:23:40,958 - INFO - Starting user U426662 in cluster 2 (index 525)
2025-04-03 22:23:49,739 - INFO - Starting user U684330 in cluster 1 (index 334)
2025-04-03 22:23:54,593 - INFO - Starting user U205238 in cluster 0 (index 328)
2025-04-03 22:23:58,945 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:24:09,894 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:24:11,155 - ERROR - Failed on user U426662 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:24:11,156 - INFO - Starting user U529462 in cluster 2 (index 526)
2025-04-03 22:24:13,050 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:24:31,841 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:24:46,801 - INFO - Starting user U179438 in cluster 1 (index 335)
2025-04-03 22:24:47,980 - ERROR - Failed on user U529462 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:24:47,980 - INFO - Starting user U165875 in cluster 2 (index 527)
2025-04-03 22:24:50,534 - INFO - Starting user U599267 in cluster 0 (index 329)
2025-04-03 22:25:05,184 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:25:06,613 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:25:09,733 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 22:25:24,735 - ERROR - Failed on user U165875 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:25:24,736 - INFO - Starting user U604504 in cluster 2 (index 528)
2025-04-03 22:25:45,781 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:25:52,809 - INFO - Starting user U43982 in cluster 1 (index 336)
2025-04-03 22:26:00,057 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U28356', 'user_index_in_cluster': 320, 'num_candidates': 4526, 'num_history_articles': 44, 'original_history_len': 44, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0011001100110011, 'ndcg_1000': 0.10173226925624419, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0011001100110011, 'ndcg_2000': 0.10173226925624419, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U113873', 'user_index_in_cluster': 321, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 80, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U591805', 'user_index_in_cluster': 322, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 76, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U493516', 'user_index_in_cluster': 323, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 101, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U539681', 'user_index_in_cluster': 324, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 102, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U247319', 'user_index_in_cluster': 325, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006176652254478073, 'ndcg_2000': 0.05750890584503807, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U318193', 'user_index_in_cluster': 326, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U314826', 'user_index_in_cluster': 327, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U205238', 'user_index_in_cluster': 328, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 104, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U599267', 'user_index_in_cluster': 329, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:26:00,072 - INFO - Starting user U585774 in cluster 0 (index 330)
2025-04-03 22:26:04,433 - ERROR - Failed on user U604504 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:26:04,433 - INFO - Starting user U485934 in cluster 2 (index 529)
2025-04-03 22:26:12,530 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:26:19,563 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:26:27,594 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:26:46,028 - ERROR - Failed on user U485934 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:26:46,028 - INFO - Starting user U410882 in cluster 2 (index 530)
2025-04-03 22:26:57,750 - INFO - Starting user U230701 in cluster 1 (index 337)
2025-04-03 22:27:02,972 - INFO - Starting user U91096 in cluster 0 (index 331)
2025-04-03 22:27:06,088 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:27:19,517 - ERROR - Failed on user U410882 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:27:19,517 - INFO - Starting user U94002 in cluster 2 (index 531)
2025-04-03 22:27:21,024 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:27:25,493 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:27:39,446 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:27:55,890 - INFO - Starting user U403032 in cluster 1 (index 338)
2025-04-03 22:27:57,158 - ERROR - Failed on user U94002 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:27:57,159 - INFO - Starting user U492092 in cluster 2 (index 532)
2025-04-03 22:28:00,365 - INFO - Starting user U341224 in cluster 0 (index 332)
2025-04-03 22:28:14,292 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:28:15,957 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:28:18,629 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:28:32,375 - ERROR - Failed on user U492092 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:28:32,376 - INFO - Starting user U641293 in cluster 2 (index 533)
2025-04-03 22:28:47,796 - INFO - Starting user U336284 in cluster 1 (index 339)
2025-04-03 22:28:52,382 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:28:52,735 - INFO - Starting user U458834 in cluster 0 (index 333)
2025-04-03 22:29:04,648 - ERROR - Failed on user U641293 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:29:04,648 - INFO - Starting user U8871 in cluster 2 (index 534)
2025-04-03 22:29:07,915 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:29:12,624 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:29:23,638 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:29:40,063 - ERROR - Failed on user U8871 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:29:40,063 - INFO - Starting user U190486 in cluster 2 (index 535)
2025-04-03 22:29:42,974 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U161113', 'user_index_in_cluster': 330, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 103, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U572541', 'user_index_in_cluster': 331, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 201, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.14285714285714285, 'ap_500': 0.0023094688221709007, 'ndcg_500': 0.03137302124039514, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.2857142857142857, 'ap_1000': 0.0023466295243154862, 'ndcg_1000': 0.059669231421042364, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.42857142857142855, 'ap_2000': 0.0021345451104710614, 'ndcg_2000': 0.08517445746192659, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U516258', 'user_index_in_cluster': 332, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 139, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U370163', 'user_index_in_cluster': 333, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 183, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U684330', 'user_index_in_cluster': 334, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 160, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U179438', 'user_index_in_cluster': 335, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 185, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.25, 'ap_100': 0.013888888888888888, 'ndcg_100': 0.06306803145689428, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.013888888888888888, 'ndcg_200': 0.06306803145689428, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.5, 'ap_500': 0.00988562091503268, 'ndcg_500': 0.10946657121706391, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.00988562091503268, 'ndcg_1000': 0.10946657121706391, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.75, 'ap_2000': 0.007301145997370755, 'ndcg_2000': 0.14678982779789923, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U43982', 'user_index_in_cluster': 336, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 346, 'num_future_clicks': 9, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.1111111111111111, 'ap_1000': 0.0010504201680672268, 'ndcg_1000': 0.023750773500859898, 'num_recommendations_1000': 1000, 'precision_2000': 0.003, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.0027548111260474154, 'ndcg_2000': 0.13813383093998743, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U230701', 'user_index_in_cluster': 337, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 217, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.000689655172413793, 'ndcg_2000': 0.04468116168832196, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U403032', 'user_index_in_cluster': 338, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 280, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006273525721455458, 'ndcg_2000': 0.09399078640152675, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U336284', 'user_index_in_cluster': 339, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 208, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.14285714285714285, 'ap_50': 0.022727272727272728, 'ndcg_50': 0.05005165434923901, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.14285714285714285, 'ap_100': 0.022727272727272728, 'ndcg_100': 0.05005165434923901, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.14285714285714285, 'ap_200': 0.022727272727272728, 'ndcg_200': 0.05005165434923901, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.14285714285714285, 'ap_500': 0.022727272727272728, 'ndcg_500': 0.05005165434923901, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.022727272727272728, 'ndcg_1000': 0.05005165434923901, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.14285714285714285, 'ap_2000': 0.022727272727272728, 'ndcg_2000': 0.05005165434923901, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:29:42,985 - INFO - Starting user U140049 in cluster 1 (index 340)
2025-04-03 22:29:49,691 - INFO - Starting user U279803 in cluster 0 (index 334)
2025-04-03 22:30:01,450 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:30:03,820 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:30:11,961 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:30:16,956 - ERROR - Failed on user U190486 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:30:16,956 - INFO - Starting user U120066 in cluster 2 (index 536)
2025-04-03 22:30:30,662 - INFO - Starting user U54873 in cluster 1 (index 341)
2025-04-03 22:30:34,910 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:30:35,548 - INFO - Starting user U659705 in cluster 0 (index 335)
2025-04-03 22:30:46,933 - ERROR - Failed on user U120066 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:30:46,934 - INFO - Starting user U252105 in cluster 2 (index 537)
2025-04-03 22:30:47,819 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:30:52,371 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:31:04,612 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:31:21,898 - INFO - Starting user U429580 in cluster 1 (index 342)
2025-04-03 22:31:23,868 - ERROR - Failed on user U252105 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:31:23,868 - INFO - Starting user U310224 in cluster 2 (index 538)
2025-04-03 22:31:27,140 - INFO - Starting user U603608 in cluster 0 (index 336)
2025-04-03 22:31:39,027 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 22:31:44,657 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:31:45,102 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:32:01,551 - ERROR - Failed on user U310224 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:32:01,552 - INFO - Starting user U585510 in cluster 2 (index 539)
2025-04-03 22:32:16,639 - INFO - Starting user U685860 in cluster 0 (index 337)
2025-04-03 22:32:19,087 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:32:20,256 - INFO - Starting user U330896 in cluster 1 (index 343)
2025-04-03 22:32:32,081 - ERROR - Failed on user U585510 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:32:32,081 - INFO - Starting user U324878 in cluster 2 (index 540)
2025-04-03 22:32:35,338 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:32:39,019 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 22:32:49,661 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:33:08,245 - ERROR - Failed on user U324878 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:33:08,245 - INFO - Starting user U267891 in cluster 2 (index 541)
2025-04-03 22:33:11,761 - INFO - Starting user U439621 in cluster 0 (index 338)
2025-04-03 22:33:16,908 - INFO - Starting user U184840 in cluster 1 (index 344)
2025-04-03 22:33:27,165 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:33:29,034 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 22:33:33,903 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:33:40,392 - ERROR - Failed on user U267891 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:33:40,392 - INFO - Starting user U679959 in cluster 2 (index 542)
2025-04-03 22:33:57,980 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:34:11,118 - INFO - Starting user U334007 in cluster 0 (index 339)
2025-04-03 22:34:15,701 - INFO - Starting user U287223 in cluster 1 (index 345)
2025-04-03 22:34:16,757 - ERROR - Failed on user U679959 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:34:16,757 - INFO - Starting user U412596 in cluster 2 (index 543)
2025-04-03 22:34:28,247 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:34:33,409 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:34:34,575 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:34:50,795 - ERROR - Failed on user U412596 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:34:50,796 - INFO - Starting user U164420 in cluster 2 (index 544)
2025-04-03 22:35:09,608 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U585774', 'user_index_in_cluster': 330, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 117, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U91096', 'user_index_in_cluster': 331, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U341224', 'user_index_in_cluster': 332, 'num_candidates': 4525, 'num_history_articles': 43, 'original_history_len': 43, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.0058823529411764705, 'ndcg_200': 0.052627097830362644, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.0058823529411764705, 'ndcg_500': 0.052627097830362644, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0058823529411764705, 'ndcg_1000': 0.052627097830362644, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0058823529411764705, 'ndcg_2000': 0.052627097830362644, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U458834', 'user_index_in_cluster': 333, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 153, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.000982397003406126, 'ndcg_2000': 0.0890764133094132, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U279803', 'user_index_in_cluster': 334, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U659705', 'user_index_in_cluster': 335, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 60, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U603608', 'user_index_in_cluster': 336, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 132, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U685860', 'user_index_in_cluster': 337, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 109, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U439621', 'user_index_in_cluster': 338, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 114, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U334007', 'user_index_in_cluster': 339, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 76, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.0045871559633027525, 'ndcg_500': 0.07886353517660344, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.0045871559633027525, 'ndcg_1000': 0.07886353517660344, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0045871559633027525, 'ndcg_2000': 0.07886353517660344, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:35:09,621 - INFO - Starting user U19894 in cluster 0 (index 340)
2025-04-03 22:35:10,350 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:35:16,494 - INFO - Starting user U593663 in cluster 1 (index 346)
2025-04-03 22:35:27,853 - ERROR - Failed on user U164420 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:35:27,854 - INFO - Starting user U634373 in cluster 2 (index 545)
2025-04-03 22:35:31,704 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:35:43,924 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:35:49,573 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:36:05,541 - INFO - Starting user U337636 in cluster 0 (index 341)
2025-04-03 22:36:06,806 - ERROR - Failed on user U634373 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:36:06,807 - INFO - Starting user U473510 in cluster 2 (index 546)
2025-04-03 22:36:14,005 - INFO - Starting user U248931 in cluster 1 (index 347)
2025-04-03 22:36:26,585 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:36:33,414 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:36:35,229 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:36:41,175 - ERROR - Failed on user U473510 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:36:41,176 - INFO - Starting user U647122 in cluster 2 (index 547)
2025-04-03 22:37:02,770 - INFO - Starting user U666073 in cluster 1 (index 348)
2025-04-03 22:37:04,079 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:37:10,847 - INFO - Starting user U155513 in cluster 0 (index 342)
2025-04-03 22:37:18,570 - ERROR - Failed on user U647122 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:37:18,570 - INFO - Starting user U549723 in cluster 2 (index 548)
2025-04-03 22:37:22,076 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:37:33,119 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:37:39,368 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:37:57,356 - ERROR - Failed on user U549723 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:37:57,357 - INFO - Starting user U260813 in cluster 2 (index 549)
2025-04-03 22:37:57,497 - INFO - history_titles:[] empty for user: U260813!!!
2025-04-03 22:38:00,893 - INFO - Starting user U154089 in cluster 1 (index 349)
2025-04-03 22:38:05,570 - INFO - Starting user U9543 in cluster 0 (index 343)
2025-04-03 22:38:17,195 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:38:25,089 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 22:38:28,219 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:38:31,955 - ERROR - Failed on user U260813 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:38:31,956 - INFO - Starting user U24012 in cluster 2 (index 550)
2025-04-03 22:38:53,072 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:39:00,189 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U140049', 'user_index_in_cluster': 340, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 215, 'num_future_clicks': 12, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.08333333333333333, 'ap_20': 0.06666666666666667, 'ndcg_20': 0.04908948394982232, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.08333333333333333, 'ap_50': 0.06666666666666667, 'ndcg_50': 0.04908948394982232, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.08333333333333333, 'ap_100': 0.06666666666666667, 'ndcg_100': 0.04908948394982232, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.08333333333333333, 'ap_200': 0.06666666666666667, 'ndcg_200': 0.04908948394982232, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.25, 'ap_500': 0.026694472217321127, 'ndcg_500': 0.0951301413939614, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.25, 'ap_1000': 0.026694472217321127, 'ndcg_1000': 0.0951301413939614, 'num_recommendations_1000': 1000, 'precision_2000': 0.003, 'recall_2000': 0.5, 'ap_2000': 0.015241856367603064, 'ndcg_2000': 0.15207393003767494, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U54873', 'user_index_in_cluster': 341, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 126, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.0022471910112359553, 'ndcg_500': 0.03853699157249674, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.0025707735085123335, 'ndcg_1000': 0.07448543565428035, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.002226932638307008, 'ndcg_2000': 0.1055177520835077, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U429580', 'user_index_in_cluster': 342, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 168, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U330896', 'user_index_in_cluster': 343, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 137, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U184840', 'user_index_in_cluster': 344, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 167, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.009259259259259259, 'ndcg_200': 0.09059256712629014, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.009259259259259259, 'ndcg_500': 0.09059256712629014, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.009259259259259259, 'ndcg_1000': 0.09059256712629014, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.009259259259259259, 'ndcg_2000': 0.09059256712629014, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U287223', 'user_index_in_cluster': 345, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0006325110689437065, 'ndcg_2000': 0.036732891584797575, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U593663', 'user_index_in_cluster': 346, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 138, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U248931', 'user_index_in_cluster': 347, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 262, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.25, 'ap_50': 0.024390243902439025, 'ndcg_50': 0.07239559903187814, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.25, 'ap_100': 0.024390243902439025, 'ndcg_100': 0.07239559903187814, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.024390243902439025, 'ndcg_200': 0.07239559903187814, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.024390243902439025, 'ndcg_500': 0.07239559903187814, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.024390243902439025, 'ndcg_1000': 0.07239559903187814, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.012826833890575167, 'ndcg_2000': 0.10912219161365737, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U666073', 'user_index_in_cluster': 348, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 135, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 1.0, 'ap_10': 0.1, 'ndcg_10': 0.2890648263178879, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 1.0, 'ap_20': 0.1, 'ndcg_20': 0.2890648263178879, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 1.0, 'ap_50': 0.1, 'ndcg_50': 0.2890648263178879, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 1.0, 'ap_100': 0.1, 'ndcg_100': 0.2890648263178879, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.1, 'ndcg_200': 0.2890648263178879, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.1, 'ndcg_500': 0.2890648263178879, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.1, 'ndcg_1000': 0.2890648263178879, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.1, 'ndcg_2000': 0.2890648263178879, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U154089', 'user_index_in_cluster': 349, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 215, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:39:00,197 - INFO - Starting user U19456 in cluster 1 (index 350)
2025-04-03 22:39:05,778 - INFO - Starting user U670754 in cluster 0 (index 344)
2025-04-03 22:39:11,242 - ERROR - Failed on user U24012 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:39:11,243 - INFO - Starting user U202182 in cluster 2 (index 551)
2025-04-03 22:39:18,394 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:39:29,644 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:39:36,357 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:39:55,003 - ERROR - Failed on user U202182 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:39:55,004 - INFO - Starting user U312805 in cluster 2 (index 552)
2025-04-03 22:40:08,098 - INFO - Starting user U395708 in cluster 1 (index 351)
2025-04-03 22:40:12,099 - INFO - Starting user U554590 in cluster 0 (index 345)
2025-04-03 22:40:13,398 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:40:25,091 - ERROR - Failed on user U312805 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:40:25,091 - INFO - Starting user U224846 in cluster 2 (index 553)
2025-04-03 22:40:26,662 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:40:30,216 - INFO - lens: newsdf:101527candidate_pool:5904,candidate_pool:5904
2025-04-03 22:40:43,485 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:40:56,509 - INFO - Starting user U424971 in cluster 1 (index 352)
2025-04-03 22:40:59,740 - ERROR - Failed on user U224846 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:40:59,740 - INFO - Starting user U231707 in cluster 2 (index 554)
2025-04-03 22:41:15,226 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:41:18,244 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:41:38,705 - ERROR - Failed on user U231707 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:41:38,706 - INFO - Starting user U48662 in cluster 2 (index 555)
2025-04-03 22:41:47,072 - INFO - Starting user U361933 in cluster 1 (index 353)
2025-04-03 22:41:57,982 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:42:14,382 - ERROR - Failed on user U48662 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:42:14,383 - INFO - Starting user U349588 in cluster 2 (index 556)
2025-04-03 22:42:19,649 - INFO - Starting user U164275 in cluster 0 (index 346)
2025-04-03 22:42:20,164 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:42:32,109 - INFO - Starting user U549858 in cluster 1 (index 354)
2025-04-03 22:42:34,548 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:42:43,543 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:42:47,325 - ERROR - Failed on user U349588 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:42:47,326 - INFO - Starting user U671685 in cluster 2 (index 557)
2025-04-03 22:42:50,939 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:43:16,557 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:43:25,731 - INFO - Starting user U307501 in cluster 1 (index 355)
2025-04-03 22:43:39,078 - INFO - Starting user U616498 in cluster 0 (index 347)
2025-04-03 22:43:39,528 - ERROR - Failed on user U671685 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:43:39,530 - INFO - Starting user U430526 in cluster 2 (index 558)
2025-04-03 22:43:46,370 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:44:00,729 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:44:05,305 - INFO - Starting user U455064 in cluster 1 (index 356)
2025-04-03 22:44:13,359 - ERROR - Failed on user U430526 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:44:13,359 - INFO - Starting user U142291 in cluster 2 (index 559)
2025-04-03 22:44:20,845 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:44:32,883 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:44:37,507 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:44:45,254 - ERROR - Failed on user U142291 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:44:45,254 - INFO - Starting user U473815 in cluster 2 (index 560)
2025-04-03 22:45:03,889 - INFO - Starting user U555069 in cluster 0 (index 348)
2025-04-03 22:45:13,005 - INFO - Starting user U602323 in cluster 1 (index 357)
2025-04-03 22:45:17,257 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:45:27,009 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:45:32,046 - ERROR - Failed on user U473815 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:45:32,047 - INFO - Starting user U616019 in cluster 2 (index 561)
2025-04-03 22:45:33,794 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:45:51,531 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:46:10,770 - ERROR - Failed on user U616019 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:46:10,771 - INFO - Starting user U52916 in cluster 2 (index 562)
2025-04-03 22:46:10,883 - INFO - history_titles:[] empty for user: U52916!!!
2025-04-03 22:46:13,475 - INFO - Starting user U665277 in cluster 0 (index 349)
2025-04-03 22:46:20,792 - INFO - Starting user U614152 in cluster 1 (index 358)
2025-04-03 22:46:29,865 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:46:33,294 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:46:40,554 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:46:44,844 - ERROR - Failed on user U52916 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:46:44,844 - INFO - Starting user U555866 in cluster 2 (index 563)
2025-04-03 22:47:11,940 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:47:24,239 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U19894', 'user_index_in_cluster': 340, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 85, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U337636', 'user_index_in_cluster': 341, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 99, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U155513', 'user_index_in_cluster': 342, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 50, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U9543', 'user_index_in_cluster': 343, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U670754', 'user_index_in_cluster': 344, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 73, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U554590', 'user_index_in_cluster': 345, 'num_candidates': 4514, 'num_history_articles': 50, 'original_history_len': 90, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0005665722379603399, 'ndcg_2000': 0.03619231332688363, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U164275', 'user_index_in_cluster': 346, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 87, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U616498', 'user_index_in_cluster': 347, 'num_candidates': 4525, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U555069', 'user_index_in_cluster': 348, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 112, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U665277', 'user_index_in_cluster': 349, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 127, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:47:24,311 - INFO - Starting user U120594 in cluster 0 (index 350)
2025-04-03 22:47:28,425 - ERROR - Failed on user U555866 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:47:28,426 - INFO - Starting user U577392 in cluster 2 (index 564)
2025-04-03 22:47:31,057 - INFO - Starting user U442568 in cluster 1 (index 359)
2025-04-03 22:47:52,023 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:48:08,354 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 22:48:09,867 - ERROR - Failed on user U577392 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:48:09,869 - INFO - Starting user U545090 in cluster 2 (index 565)
2025-04-03 22:48:14,147 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:48:37,772 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:49:00,098 - ERROR - Failed on user U545090 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:49:00,098 - INFO - Starting user U152360 in cluster 2 (index 566)
2025-04-03 22:49:23,568 - INFO - history_titles:[] empty for user: U152360!!!
2025-04-03 22:49:46,833 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:49:50,147 - INFO - Starting user U82179 in cluster 0 (index 351)
2025-04-03 22:49:55,536 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U19456', 'user_index_in_cluster': 350, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 151, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U395708', 'user_index_in_cluster': 351, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0009643201542912247, 'ndcg_2000': 0.06119483413929298, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U424971', 'user_index_in_cluster': 352, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 155, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U361933', 'user_index_in_cluster': 353, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 314, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.0006365372374283895, 'ndcg_2000': 0.036764539195630545, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U549858', 'user_index_in_cluster': 354, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 288, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U307501', 'user_index_in_cluster': 355, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 222, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.2, 'ap_2000': 0.0006365372374283895, 'ndcg_2000': 0.031940844980762315, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U455064', 'user_index_in_cluster': 356, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 196, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U602323', 'user_index_in_cluster': 357, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 264, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U614152', 'user_index_in_cluster': 358, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 141, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U442568', 'user_index_in_cluster': 359, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 182, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 22:49:55,630 - INFO - Starting user U476573 in cluster 1 (index 360)
2025-04-03 22:50:11,497 - ERROR - Failed on user U152360 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:50:11,497 - INFO - Starting user U115284 in cluster 2 (index 567)
2025-04-03 22:50:17,341 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:50:21,934 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:50:30,895 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:50:49,200 - ERROR - Failed on user U115284 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:50:49,200 - INFO - Starting user U2531 in cluster 2 (index 568)
2025-04-03 22:51:07,646 - INFO - Starting user U215815 in cluster 0 (index 352)
2025-04-03 22:51:10,635 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:51:12,875 - INFO - Starting user U141700 in cluster 1 (index 361)
2025-04-03 22:51:24,615 - ERROR - Failed on user U2531 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:51:24,616 - INFO - Starting user U389900 in cluster 2 (index 569)
2025-04-03 22:51:28,024 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:51:33,686 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:51:45,534 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:52:04,142 - ERROR - Failed on user U389900 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:52:04,142 - INFO - Starting user U301433 in cluster 2 (index 570)
2025-04-03 22:52:26,125 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:52:28,495 - INFO - Starting user U563818 in cluster 0 (index 353)
2025-04-03 22:52:32,639 - INFO - Starting user U176489 in cluster 1 (index 362)
2025-04-03 22:52:45,938 - ERROR - Failed on user U301433 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:52:45,939 - INFO - Starting user U41903 in cluster 2 (index 571)
2025-04-03 22:53:08,869 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:53:12,220 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 22:53:12,309 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:53:19,034 - ERROR - Failed on user U41903 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:53:19,036 - INFO - Starting user U370273 in cluster 2 (index 572)
2025-04-03 22:53:47,651 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:54:06,558 - ERROR - Failed on user U370273 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:54:06,558 - INFO - Starting user U334764 in cluster 2 (index 573)
2025-04-03 22:54:10,799 - INFO - Starting user U583071 in cluster 0 (index 354)
2025-04-03 22:54:16,184 - INFO - Starting user U414040 in cluster 1 (index 363)
2025-04-03 22:54:25,640 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:54:33,696 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:54:39,003 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:54:39,389 - ERROR - Failed on user U334764 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:54:39,390 - INFO - Starting user U525618 in cluster 2 (index 574)
2025-04-03 22:55:08,696 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:55:16,665 - INFO - Starting user U536294 in cluster 1 (index 364)
2025-04-03 22:55:22,170 - INFO - Starting user U372975 in cluster 0 (index 355)
2025-04-03 22:55:28,038 - ERROR - Failed on user U525618 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:55:28,039 - INFO - Starting user U632276 in cluster 2 (index 575)
2025-04-03 22:55:35,896 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:55:41,484 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 22:55:47,680 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:56:05,338 - ERROR - Failed on user U632276 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:56:05,339 - INFO - Starting user U529943 in cluster 2 (index 576)
2025-04-03 22:56:17,591 - INFO - Starting user U228764 in cluster 0 (index 356)
2025-04-03 22:56:21,253 - INFO - Starting user U39774 in cluster 1 (index 365)
2025-04-03 22:56:24,981 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 22:56:38,873 - ERROR - Failed on user U529943 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:56:38,873 - INFO - Starting user U136077 in cluster 2 (index 577)
2025-04-03 22:56:52,607 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 22:56:56,137 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:56:58,691 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:57:09,164 - ERROR - Failed on user U136077 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:57:09,165 - INFO - Starting user U630904 in cluster 2 (index 578)
2025-04-03 22:57:29,221 - INFO - Starting user U32239 in cluster 0 (index 357)
2025-04-03 22:57:33,837 - INFO - Starting user U190436 in cluster 1 (index 366)
2025-04-03 22:57:34,126 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:57:45,954 - ERROR - Failed on user U630904 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:57:45,954 - INFO - Starting user U244770 in cluster 2 (index 579)
2025-04-03 22:57:48,578 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 22:57:53,253 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:58:05,868 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 22:58:22,974 - ERROR - Failed on user U244770 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:58:22,975 - INFO - Starting user U312526 in cluster 2 (index 580)
2025-04-03 22:58:30,574 - INFO - Starting user U48793 in cluster 1 (index 367)
2025-04-03 22:58:34,773 - INFO - Starting user U276793 in cluster 0 (index 358)
2025-04-03 22:58:42,976 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:58:50,691 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:58:53,880 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 22:58:56,315 - ERROR - Failed on user U312526 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:58:56,316 - INFO - Starting user U684517 in cluster 2 (index 581)
2025-04-03 22:59:19,871 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 22:59:27,300 - INFO - Starting user U476482 in cluster 1 (index 368)
2025-04-03 22:59:35,463 - INFO - Starting user U21224 in cluster 0 (index 359)
2025-04-03 22:59:38,430 - ERROR - Failed on user U684517 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 22:59:38,430 - INFO - Starting user U85926 in cluster 2 (index 582)
2025-04-03 22:59:56,038 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 22:59:57,617 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:00:02,068 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:00:17,518 - ERROR - Failed on user U85926 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:00:17,519 - INFO - Starting user U342705 in cluster 2 (index 583)
2025-04-03 23:00:37,283 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:00:39,772 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U120594', 'user_index_in_cluster': 350, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 83, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U82179', 'user_index_in_cluster': 351, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 72, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U215815', 'user_index_in_cluster': 352, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U563818', 'user_index_in_cluster': 353, 'num_candidates': 4520, 'num_history_articles': 42, 'original_history_len': 42, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.14285714285714285, 'ap_500': 0.002109704641350211, 'ndcg_500': 0.030913519951548254, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.14285714285714285, 'ap_1000': 0.002109704641350211, 'ndcg_1000': 0.030913519951548254, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.5714285714285714, 'ap_2000': 0.0023944070920414932, 'ndcg_2000': 0.11184087511660616, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U583071', 'user_index_in_cluster': 354, 'num_candidates': 4525, 'num_history_articles': 39, 'original_history_len': 39, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U372975', 'user_index_in_cluster': 355, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 65, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0010695187165775401, 'ndcg_1000': 0.1013133788274246, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0010695187165775401, 'ndcg_2000': 0.1013133788274246, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U228764', 'user_index_in_cluster': 356, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0012165450121654502, 'ndcg_1000': 0.10325513229030667, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0012165450121654502, 'ndcg_2000': 0.10325513229030667, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U32239', 'user_index_in_cluster': 357, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 138, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U276793', 'user_index_in_cluster': 358, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 74, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0006609385327164573, 'ndcg_2000': 0.04442181800509434, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U21224', 'user_index_in_cluster': 359, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 91, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:00:39,786 - INFO - Starting user U555898 in cluster 0 (index 360)
2025-04-03 23:00:42,904 - INFO - Starting user U646307 in cluster 1 (index 369)
2025-04-03 23:00:55,076 - ERROR - Failed on user U342705 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:00:55,077 - INFO - Starting user U704868 in cluster 2 (index 584)
2025-04-03 23:00:58,629 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:01:03,167 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:01:28,063 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:01:35,843 - INFO - Starting user U529258 in cluster 0 (index 361)
2025-04-03 23:01:42,313 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U476573', 'user_index_in_cluster': 360, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 416, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U141700', 'user_index_in_cluster': 361, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 205, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.2, 'ap_200': 0.009708737864077669, 'ndcg_200': 0.050617604148479614, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.009804863981543784, 'ndcg_500': 0.09486357183180599, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.009804863981543784, 'ndcg_1000': 0.09486357183180599, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.009804863981543784, 'ndcg_2000': 0.09486357183180599, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U176489', 'user_index_in_cluster': 362, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 119, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U414040', 'user_index_in_cluster': 363, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 185, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U536294', 'user_index_in_cluster': 364, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 152, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U39774', 'user_index_in_cluster': 365, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 130, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.000691085003455425, 'ndcg_2000': 0.0952394931081978, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U190436', 'user_index_in_cluster': 366, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 145, 'num_future_clicks': 8, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.125, 'ap_10': 0.125, 'ndcg_10': 0.07979453856244817, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.125, 'ap_20': 0.125, 'ndcg_20': 0.07979453856244817, 'num_recommendations_20': 20, 'precision_50': 0.06, 'recall_50': 0.375, 'ap_50': 0.08014905149051489, 'ndcg_50': 0.1724959195163726, 'num_recommendations_50': 50, 'precision_100': 0.03, 'recall_100': 0.375, 'ap_100': 0.08014905149051489, 'ndcg_100': 0.1724959195163726, 'num_recommendations_100': 100, 'precision_200': 0.015, 'recall_200': 0.375, 'ap_200': 0.08014905149051489, 'ndcg_200': 0.1724959195163726, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.375, 'ap_500': 0.08014905149051489, 'ndcg_500': 0.1724959195163726, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.375, 'ap_1000': 0.08014905149051489, 'ndcg_1000': 0.1724959195163726, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.625, 'ap_2000': 0.04954558886835002, 'ndcg_2000': 0.22180070654452264, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U48793', 'user_index_in_cluster': 367, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 145, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0033333333333333335, 'ndcg_500': 0.05699543389735954, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0033333333333333335, 'ndcg_1000': 0.05699543389735954, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0033333333333333335, 'ndcg_2000': 0.05699543389735954, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U476482', 'user_index_in_cluster': 368, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 174, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.022727272727272728, 'ndcg_50': 0.0854499779566911, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.022727272727272728, 'ndcg_100': 0.0854499779566911, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.022727272727272728, 'ndcg_200': 0.0854499779566911, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.022727272727272728, 'ndcg_500': 0.0854499779566911, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.022727272727272728, 'ndcg_1000': 0.0854499779566911, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.012273554471006701, 'ndcg_2000': 0.13189809810546424, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U646307', 'user_index_in_cluster': 369, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 277, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:01:42,329 - INFO - Starting user U549853 in cluster 1 (index 370)
2025-04-03 23:01:47,166 - ERROR - Failed on user U704868 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:01:47,166 - INFO - Starting user U316455 in cluster 2 (index 585)
2025-04-03 23:01:54,608 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:02:01,470 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:02:06,105 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:02:23,254 - ERROR - Failed on user U316455 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:02:23,254 - INFO - Starting user U69205 in cluster 2 (index 586)
2025-04-03 23:02:39,576 - INFO - Starting user U693305 in cluster 0 (index 362)
2025-04-03 23:02:43,814 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 23:02:46,414 - INFO - Starting user U302113 in cluster 1 (index 371)
2025-04-03 23:02:57,940 - ERROR - Failed on user U69205 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:02:57,941 - INFO - Starting user U127580 in cluster 2 (index 587)
2025-04-03 23:02:58,758 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:03:04,990 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:03:17,489 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:03:34,750 - INFO - Starting user U651978 in cluster 0 (index 363)
2025-04-03 23:03:35,935 - ERROR - Failed on user U127580 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:03:35,935 - INFO - Starting user U341579 in cluster 2 (index 588)
2025-04-03 23:03:39,113 - INFO - Starting user U170455 in cluster 1 (index 372)
2025-04-03 23:03:54,349 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:04:03,457 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:04:05,848 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:04:09,327 - ERROR - Failed on user U341579 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:04:09,328 - INFO - Starting user U254115 in cluster 2 (index 589)
2025-04-03 23:04:28,337 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:04:46,851 - ERROR - Failed on user U254115 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:04:46,854 - INFO - Starting user U73966 in cluster 2 (index 590)
2025-04-03 23:04:53,186 - INFO - Starting user U648162 in cluster 0 (index 364)
2025-04-03 23:04:56,552 - INFO - Starting user U38198 in cluster 1 (index 373)
2025-04-03 23:05:13,626 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:05:14,876 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:05:18,298 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:05:34,433 - ERROR - Failed on user U73966 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:05:34,433 - INFO - Starting user U535615 in cluster 2 (index 591)
2025-04-03 23:05:55,588 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:05:59,831 - INFO - Starting user U683858 in cluster 0 (index 365)
2025-04-03 23:06:09,133 - INFO - Starting user U63413 in cluster 1 (index 374)
2025-04-03 23:06:12,139 - ERROR - Failed on user U535615 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:06:12,139 - INFO - Starting user U514981 in cluster 2 (index 592)
2025-04-03 23:06:19,568 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:06:29,990 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:06:33,159 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:06:50,070 - INFO - Starting user U36550 in cluster 0 (index 366)
2025-04-03 23:06:50,477 - ERROR - Failed on user U514981 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:06:50,477 - INFO - Starting user U499224 in cluster 2 (index 593)
2025-04-03 23:06:57,018 - INFO - Starting user U24771 in cluster 1 (index 375)
2025-04-03 23:07:14,022 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:07:29,368 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:07:29,543 - ERROR - Failed on user U499224 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:07:29,544 - INFO - Starting user U542654 in cluster 2 (index 594)
2025-04-03 23:07:33,738 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:07:51,235 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:08:09,893 - ERROR - Failed on user U542654 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:08:09,893 - INFO - Starting user U47257 in cluster 2 (index 595)
2025-04-03 23:08:21,819 - INFO - Starting user U90905 in cluster 0 (index 367)
2025-04-03 23:08:27,646 - INFO - Starting user U111356 in cluster 1 (index 376)
2025-04-03 23:08:30,134 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:08:43,092 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:08:43,678 - ERROR - Failed on user U47257 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:08:43,678 - INFO - Starting user U442882 in cluster 2 (index 596)
2025-04-03 23:08:49,026 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:09:15,890 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:09:31,241 - INFO - Starting user U400934 in cluster 0 (index 368)
2025-04-03 23:09:32,464 - ERROR - Failed on user U442882 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:09:32,465 - INFO - Starting user U340539 in cluster 2 (index 597)
2025-04-03 23:09:36,489 - INFO - Starting user U531041 in cluster 1 (index 377)
2025-04-03 23:09:49,618 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:09:50,694 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:09:54,454 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:10:08,514 - ERROR - Failed on user U340539 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:10:08,515 - INFO - Starting user U51875 in cluster 2 (index 598)
2025-04-03 23:10:25,433 - INFO - Starting user U619843 in cluster 0 (index 369)
2025-04-03 23:10:28,963 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:10:30,223 - INFO - Starting user U412248 in cluster 1 (index 378)
2025-04-03 23:10:41,975 - ERROR - Failed on user U51875 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:10:41,976 - INFO - Starting user U293823 in cluster 2 (index 599)
2025-04-03 23:10:51,611 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:10:54,814 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:11:03,120 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:11:22,442 - ERROR - Failed on user U293823 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:11:22,442 - INFO - Starting user U555360 in cluster 2 (index 600)
2025-04-03 23:11:30,798 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U555898', 'user_index_in_cluster': 360, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U529258', 'user_index_in_cluster': 361, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 142, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U693305', 'user_index_in_cluster': 362, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U651978', 'user_index_in_cluster': 363, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 158, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U648162', 'user_index_in_cluster': 364, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 139, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0005327650506126798, 'ndcg_2000': 0.05638144776199933, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U683858', 'user_index_in_cluster': 365, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 82, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U36550', 'user_index_in_cluster': 366, 'num_candidates': 4523, 'num_history_articles': 40, 'original_history_len': 40, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U90905', 'user_index_in_cluster': 367, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U400934', 'user_index_in_cluster': 368, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U619843', 'user_index_in_cluster': 369, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 87, 'num_future_clicks': 7, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.14285714285714285, 'ap_50': 0.034482758620689655, 'ndcg_50': 0.05601843521033356, 'num_recommendations_50': 50, 'precision_100': 0.02, 'recall_100': 0.2857142857142857, 'ap_100': 0.029741379310344828, 'ndcg_100': 0.09937534945739342, 'num_recommendations_100': 100, 'precision_200': 0.01, 'recall_200': 0.2857142857142857, 'ap_200': 0.029741379310344828, 'ndcg_200': 0.09937534945739342, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2857142857142857, 'ap_500': 0.029741379310344828, 'ndcg_500': 0.09937534945739342, 'num_recommendations_500': 500, 'precision_1000': 0.004, 'recall_1000': 0.5714285714285714, 'ap_1000': 0.01752700919233307, 'ndcg_1000': 0.1581004909764439, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.5714285714285714, 'ap_2000': 0.01752700919233307, 'ndcg_2000': 0.1581004909764439, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:11:30,809 - INFO - Starting user U154950 in cluster 0 (index 370)
2025-04-03 23:11:35,484 - INFO - Starting user U680501 in cluster 1 (index 379)
2025-04-03 23:11:41,287 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:11:50,297 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:11:55,125 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:11:55,622 - ERROR - Failed on user U555360 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:11:55,623 - INFO - Starting user U116772 in cluster 2 (index 601)
2025-04-03 23:12:13,644 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:12:30,355 - ERROR - Failed on user U116772 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:12:30,356 - INFO - Starting user U77576 in cluster 2 (index 602)
2025-04-03 23:12:45,567 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U549853', 'user_index_in_cluster': 370, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 178, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.0022988505747126436, 'ndcg_500': 0.03868077959027678, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0022988505747126436, 'ndcg_1000': 0.03868077959027678, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.001675464214236911, 'ndcg_2000': 0.06981552825327672, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U302113', 'user_index_in_cluster': 371, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 224, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U170455', 'user_index_in_cluster': 372, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 200, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U38198', 'user_index_in_cluster': 373, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 209, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.00228310502283105, 'ndcg_500': 0.05346031016790534, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.00228310502283105, 'ndcg_1000': 0.05346031016790534, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.001796860506173061, 'ndcg_2000': 0.09783032109359448, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U63413', 'user_index_in_cluster': 374, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 177, 'num_future_clicks': 9, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.1111111111111111, 'ap_50': 0.034482758620689655, 'ndcg_50': 0.04790111881133895, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.1111111111111111, 'ap_100': 0.034482758620689655, 'ndcg_100': 0.04790111881133895, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.1111111111111111, 'ap_200': 0.034482758620689655, 'ndcg_200': 0.04790111881133895, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.2222222222222222, 'ap_500': 0.01949363156259708, 'ndcg_500': 0.07461793994883725, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.014878993546439484, 'ndcg_1000': 0.1005746730588936, 'num_recommendations_1000': 1000, 'precision_2000': 0.0025, 'recall_2000': 0.5555555555555556, 'ap_2000': 0.010016468978367565, 'ndcg_2000': 0.14459260716876315, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U24771', 'user_index_in_cluster': 375, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 181, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U111356', 'user_index_in_cluster': 376, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 99, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U531041', 'user_index_in_cluster': 377, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 168, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U412248', 'user_index_in_cluster': 378, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 274, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0006365372374283895, 'ndcg_2000': 0.04419492265777481, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U680501', 'user_index_in_cluster': 379, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 231, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.2, 'ap_1000': 0.0010822510822510823, 'ndcg_1000': 0.03442094286864423, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.6, 'ap_2000': 0.001475785500014029, 'ndcg_2000': 0.0987998582968727, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:12:45,585 - INFO - Starting user U454435 in cluster 1 (index 380)
2025-04-03 23:12:49,513 - INFO - Starting user U304653 in cluster 0 (index 371)
2025-04-03 23:12:53,638 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:13:04,596 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:13:06,865 - ERROR - Failed on user U77576 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:13:06,866 - INFO - Starting user U550042 in cluster 2 (index 603)
2025-04-03 23:13:08,586 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 23:13:25,040 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:13:44,320 - ERROR - Failed on user U550042 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:13:44,320 - INFO - Starting user U467460 in cluster 2 (index 604)
2025-04-03 23:13:50,721 - INFO - Starting user U677394 in cluster 1 (index 381)
2025-04-03 23:13:57,165 - INFO - Starting user U206140 in cluster 0 (index 372)
2025-04-03 23:14:02,273 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:14:16,399 - ERROR - Failed on user U467460 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:14:16,399 - INFO - Starting user U607580 in cluster 2 (index 605)
2025-04-03 23:14:20,630 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:14:29,256 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:14:41,275 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:15:00,294 - ERROR - Failed on user U607580 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:15:00,295 - INFO - Starting user U666877 in cluster 2 (index 606)
2025-04-03 23:15:12,446 - INFO - Starting user U472351 in cluster 1 (index 382)
2025-04-03 23:15:16,644 - INFO - Starting user U615478 in cluster 0 (index 373)
2025-04-03 23:15:20,478 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:15:34,344 - ERROR - Failed on user U666877 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:15:34,345 - INFO - Starting user U333220 in cluster 2 (index 607)
2025-04-03 23:15:34,835 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:15:39,093 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:15:52,434 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:16:06,727 - INFO - Starting user U356326 in cluster 1 (index 383)
2025-04-03 23:16:10,316 - ERROR - Failed on user U333220 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:16:10,317 - INFO - Starting user U3589 in cluster 2 (index 608)
2025-04-03 23:16:12,266 - INFO - Starting user U462407 in cluster 0 (index 374)
2025-04-03 23:16:24,555 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:16:30,909 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 23:16:31,223 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:16:43,811 - ERROR - Failed on user U3589 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:16:43,812 - INFO - Starting user U401633 in cluster 2 (index 609)
2025-04-03 23:17:01,021 - INFO - Starting user U480380 in cluster 1 (index 384)
2025-04-03 23:17:04,971 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:17:09,698 - INFO - Starting user U270846 in cluster 0 (index 375)
2025-04-03 23:17:17,468 - ERROR - Failed on user U401633 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:17:17,471 - INFO - Starting user U95460 in cluster 2 (index 610)
2025-04-03 23:17:35,830 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:17:39,665 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:17:43,383 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 23:17:59,612 - ERROR - Failed on user U95460 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:17:59,613 - INFO - Starting user U590170 in cluster 2 (index 611)
2025-04-03 23:18:25,630 - INFO - Starting user U640747 in cluster 1 (index 385)
2025-04-03 23:18:32,051 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:18:33,999 - INFO - Starting user U655376 in cluster 0 (index 376)
2025-04-03 23:18:45,513 - ERROR - Failed on user U590170 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:18:45,514 - INFO - Starting user U211167 in cluster 2 (index 612)
2025-04-03 23:18:48,061 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:18:55,704 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:19:06,260 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:19:25,693 - ERROR - Failed on user U211167 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:19:25,693 - INFO - Starting user U123942 in cluster 2 (index 613)
2025-04-03 23:19:34,110 - INFO - Starting user U305637 in cluster 0 (index 377)
2025-04-03 23:19:41,191 - INFO - Starting user U222698 in cluster 1 (index 386)
2025-04-03 23:19:47,314 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:19:58,562 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:20:02,639 - ERROR - Failed on user U123942 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:20:02,640 - INFO - Starting user U475210 in cluster 2 (index 614)
2025-04-03 23:20:02,811 - INFO - history_titles:[] empty for user: U475210!!!
2025-04-03 23:20:06,739 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:20:35,027 - INFO - Starting user U351237 in cluster 0 (index 378)
2025-04-03 23:20:36,684 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:20:46,079 - INFO - Starting user U394912 in cluster 1 (index 387)
2025-04-03 23:20:52,027 - ERROR - Failed on user U475210 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:20:52,029 - INFO - Starting user U276194 in cluster 2 (index 615)
2025-04-03 23:21:13,645 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:21:15,047 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:21:25,755 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:21:27,027 - ERROR - Failed on user U276194 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:21:27,027 - INFO - Starting user U553900 in cluster 2 (index 616)
2025-04-03 23:21:48,588 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:22:04,493 - ERROR - Failed on user U553900 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:22:04,494 - INFO - Starting user U422721 in cluster 2 (index 617)
2025-04-03 23:22:19,278 - INFO - Starting user U700140 in cluster 0 (index 379)
2025-04-03 23:22:25,540 - INFO - Starting user U495742 in cluster 1 (index 388)
2025-04-03 23:22:37,504 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:22:43,991 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:22:50,772 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:22:53,366 - ERROR - Failed on user U422721 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:22:53,367 - INFO - Starting user U179941 in cluster 2 (index 618)
2025-04-03 23:23:16,261 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:23:32,891 - ERROR - Failed on user U179941 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:23:32,892 - INFO - Starting user U453516 in cluster 2 (index 619)
2025-04-03 23:23:44,543 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U154950', 'user_index_in_cluster': 370, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U304653', 'user_index_in_cluster': 371, 'num_candidates': 4518, 'num_history_articles': 50, 'original_history_len': 129, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U206140', 'user_index_in_cluster': 372, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 57, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.001669449081803005, 'ndcg_1000': 0.06643831819958669, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0014991763681440275, 'ndcg_2000': 0.12452071682560763, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U615478', 'user_index_in_cluster': 373, 'num_candidates': 4522, 'num_history_articles': 48, 'original_history_len': 48, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U462407', 'user_index_in_cluster': 374, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 94, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U270846', 'user_index_in_cluster': 375, 'num_candidates': 4517, 'num_history_articles': 50, 'original_history_len': 96, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.16666666666666666, 'ap_200': 0.0058823529411764705, 'ndcg_200': 0.04079380290880158, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.3333333333333333, 'ap_500': 0.006787330316742082, 'ndcg_500': 0.07848761915778452, 'num_recommendations_500': 500, 'precision_1000': 0.003, 'recall_1000': 0.5, 'ap_1000': 0.005951420401365857, 'ndcg_1000': 0.11049099740671481, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.005190309487070905, 'ndcg_2000': 0.13951116952795484, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U655376', 'user_index_in_cluster': 376, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U305637', 'user_index_in_cluster': 377, 'num_candidates': 4524, 'num_history_articles': 39, 'original_history_len': 39, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U351237', 'user_index_in_cluster': 378, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 128, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U700140', 'user_index_in_cluster': 379, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 126, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:23:44,602 - INFO - Starting user U173101 in cluster 0 (index 380)
2025-04-03 23:23:51,522 - INFO - Starting user U155784 in cluster 1 (index 389)
2025-04-03 23:23:55,004 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:24:06,146 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:24:08,716 - ERROR - Failed on user U453516 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:24:08,717 - INFO - Starting user U531146 in cluster 2 (index 620)
2025-04-03 23:24:12,652 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:24:31,530 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:24:49,616 - ERROR - Failed on user U531146 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:24:49,616 - INFO - Starting user U523691 in cluster 2 (index 621)
2025-04-03 23:24:52,048 - INFO - Starting user U113346 in cluster 0 (index 381)
2025-04-03 23:24:59,781 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U454435', 'user_index_in_cluster': 380, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 245, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.4, 'ap_500': 0.004045149083317022, 'ndcg_500': 0.08041171285364149, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.004045149083317022, 'ndcg_1000': 0.08041171285364149, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.8, 'ap_2000': 0.0031026025816317756, 'ndcg_2000': 0.1441199165605404, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U677394', 'user_index_in_cluster': 381, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 157, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.0022727272727272726, 'ndcg_500': 0.1138351243484765, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0022727272727272726, 'ndcg_1000': 0.1138351243484765, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0022727272727272726, 'ndcg_2000': 0.1138351243484765, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U472351', 'user_index_in_cluster': 382, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 129, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0022522522522522522, 'ndcg_500': 0.0533413026454454, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0022522522522522522, 'ndcg_1000': 0.0533413026454454, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0022522522522522522, 'ndcg_2000': 0.0533413026454454, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U356326', 'user_index_in_cluster': 383, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 241, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U480380', 'user_index_in_cluster': 384, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 186, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0033783783783783786, 'ndcg_500': 0.05712935169932979, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.00293142521403391, 'ndcg_1000': 0.1057359200501314, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.00293142521403391, 'ndcg_2000': 0.1057359200501314, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U640747', 'user_index_in_cluster': 385, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 161, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U222698', 'user_index_in_cluster': 386, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 142, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U394912', 'user_index_in_cluster': 387, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 166, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.3333333333333333, 'ap_10': 0.125, 'ndcg_10': 0.1480409554829326, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.3333333333333333, 'ap_20': 0.125, 'ndcg_20': 0.1480409554829326, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.125, 'ndcg_50': 0.1480409554829326, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.125, 'ndcg_100': 0.1480409554829326, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.125, 'ndcg_200': 0.1480409554829326, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.125, 'ndcg_500': 0.1480409554829326, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.06370336943441636, 'ndcg_1000': 0.19641801257088543, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.06370336943441636, 'ndcg_2000': 0.19641801257088543, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U495742', 'user_index_in_cluster': 388, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 308, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.002173913043478261, 'ndcg_500': 0.06929295065382322, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.002173913043478261, 'ndcg_1000': 0.06929295065382322, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.002173913043478261, 'ndcg_2000': 0.06929295065382322, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U155784', 'user_index_in_cluster': 389, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 186, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.1, 'recall_10': 0.3333333333333333, 'ap_10': 0.1, 'ndcg_10': 0.13565197343244778, 'num_recommendations_10': 10, 'precision_20': 0.05, 'recall_20': 0.3333333333333333, 'ap_20': 0.1, 'ndcg_20': 0.13565197343244778, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.3333333333333333, 'ap_50': 0.1, 'ndcg_50': 0.13565197343244778, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.3333333333333333, 'ap_100': 0.1, 'ndcg_100': 0.13565197343244778, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.3333333333333333, 'ap_200': 0.1, 'ndcg_200': 0.13565197343244778, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.6666666666666666, 'ap_500': 0.05340136054421769, 'ndcg_500': 0.19284920138892495, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.05340136054421769, 'ndcg_1000': 0.19284920138892495, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.05340136054421769, 'ndcg_2000': 0.19284920138892495, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:24:59,831 - INFO - Starting user U230010 in cluster 1 (index 390)
2025-04-03 23:25:11,725 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:25:20,466 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 23:25:25,280 - ERROR - Failed on user U523691 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:25:25,280 - INFO - Starting user U109826 in cluster 2 (index 622)
2025-04-03 23:25:28,196 - INFO - Starting user U64648 in cluster 1 (index 391)
2025-04-03 23:25:30,679 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:25:42,754 - INFO - Starting user U350488 in cluster 0 (index 382)
2025-04-03 23:25:46,827 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:26:00,133 - ERROR - Failed on user U109826 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:26:00,134 - INFO - Starting user U605977 in cluster 2 (index 623)
2025-04-03 23:26:05,123 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:26:07,509 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:26:24,589 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:26:40,601 - INFO - Starting user U83776 in cluster 0 (index 383)
2025-04-03 23:26:46,026 - ERROR - Failed on user U605977 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:26:46,027 - INFO - Starting user U629437 in cluster 2 (index 624)
2025-04-03 23:26:46,804 - INFO - Starting user U447090 in cluster 1 (index 392)
2025-04-03 23:27:03,967 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:27:12,614 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:27:16,204 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:27:33,904 - ERROR - Failed on user U629437 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:27:33,905 - INFO - Starting user U605774 in cluster 2 (index 625)
2025-04-03 23:27:51,514 - INFO - Starting user U680978 in cluster 0 (index 384)
2025-04-03 23:27:56,176 - INFO - Starting user U709861 in cluster 1 (index 393)
2025-04-03 23:27:56,347 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:28:08,582 - ERROR - Failed on user U605774 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:28:08,583 - INFO - Starting user U499760 in cluster 2 (index 626)
2025-04-03 23:28:13,133 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:28:16,795 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:28:31,930 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:28:48,529 - ERROR - Failed on user U499760 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:28:48,529 - INFO - Starting user U259885 in cluster 2 (index 627)
2025-04-03 23:28:59,920 - INFO - Starting user U485272 in cluster 0 (index 385)
2025-04-03 23:29:05,396 - INFO - Starting user U67783 in cluster 1 (index 394)
2025-04-03 23:29:08,788 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:29:21,484 - ERROR - Failed on user U259885 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:29:21,485 - INFO - Starting user U115338 in cluster 2 (index 628)
2025-04-03 23:29:36,410 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:29:36,801 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:29:41,546 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:30:02,954 - ERROR - Failed on user U115338 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:30:02,955 - INFO - Starting user U150155 in cluster 2 (index 629)
2025-04-03 23:30:19,699 - INFO - Starting user U115422 in cluster 1 (index 395)
2025-04-03 23:30:22,004 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:30:25,354 - INFO - Starting user U394458 in cluster 0 (index 386)
2025-04-03 23:30:36,922 - ERROR - Failed on user U150155 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:30:36,923 - INFO - Starting user U268359 in cluster 2 (index 630)
2025-04-03 23:30:39,625 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:30:45,200 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:31:09,948 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:31:16,298 - INFO - Starting user U331069 in cluster 1 (index 396)
2025-04-03 23:31:22,618 - INFO - Starting user U278844 in cluster 0 (index 387)
2025-04-03 23:31:27,696 - ERROR - Failed on user U268359 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:31:27,697 - INFO - Starting user U345250 in cluster 2 (index 631)
2025-04-03 23:31:34,172 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:31:40,770 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:31:45,414 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:32:03,103 - ERROR - Failed on user U345250 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:32:03,104 - INFO - Starting user U489907 in cluster 2 (index 632)
2025-04-03 23:32:18,893 - INFO - Starting user U640657 in cluster 1 (index 397)
2025-04-03 23:32:21,164 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:32:22,636 - INFO - Starting user U19623 in cluster 0 (index 388)
2025-04-03 23:32:34,543 - ERROR - Failed on user U489907 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:32:34,543 - INFO - Starting user U400018 in cluster 2 (index 633)
2025-04-03 23:32:39,832 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:32:44,070 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:32:53,512 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:33:10,191 - ERROR - Failed on user U400018 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:33:10,191 - INFO - Starting user U330972 in cluster 2 (index 634)
2025-04-03 23:33:17,404 - INFO - Starting user U8242 in cluster 1 (index 398)
2025-04-03 23:33:21,995 - INFO - Starting user U399124 in cluster 0 (index 389)
2025-04-03 23:33:28,308 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:33:35,698 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:33:40,296 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:33:42,271 - ERROR - Failed on user U330972 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:33:42,272 - INFO - Starting user U411856 in cluster 2 (index 635)
2025-04-03 23:34:00,326 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:34:11,892 - INFO - Starting user U151706 in cluster 1 (index 399)
2025-04-03 23:34:16,211 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U173101', 'user_index_in_cluster': 380, 'num_candidates': 4524, 'num_history_articles': 40, 'original_history_len': 40, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0006653359946773121, 'ndcg_2000': 0.04446205651565151, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U113346', 'user_index_in_cluster': 381, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 54, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U350488', 'user_index_in_cluster': 382, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 71, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U83776', 'user_index_in_cluster': 383, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 117, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U680978', 'user_index_in_cluster': 384, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 74, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.002074688796680498, 'ndcg_500': 0.11215943571127349, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.002074688796680498, 'ndcg_1000': 0.11215943571127349, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.002074688796680498, 'ndcg_2000': 0.11215943571127349, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U485272', 'user_index_in_cluster': 385, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 114, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U394458', 'user_index_in_cluster': 386, 'num_candidates': 4521, 'num_history_articles': 31, 'original_history_len': 31, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U278844', 'user_index_in_cluster': 387, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 93, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U19623', 'user_index_in_cluster': 388, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 56, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U399124', 'user_index_in_cluster': 389, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 111, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:34:16,223 - INFO - Starting user U34177 in cluster 0 (index 390)
2025-04-03 23:34:17,402 - ERROR - Failed on user U411856 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:34:17,402 - INFO - Starting user U16787 in cluster 2 (index 636)
2025-04-03 23:34:30,245 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:34:34,837 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:34:36,723 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:34:53,698 - ERROR - Failed on user U16787 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:34:53,698 - INFO - Starting user U196032 in cluster 2 (index 637)
2025-04-03 23:35:06,026 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U230010', 'user_index_in_cluster': 390, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 129, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U64648', 'user_index_in_cluster': 391, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 124, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U447090', 'user_index_in_cluster': 392, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 276, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U709861', 'user_index_in_cluster': 393, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 148, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U67783', 'user_index_in_cluster': 394, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 125, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U115422', 'user_index_in_cluster': 395, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 153, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U331069', 'user_index_in_cluster': 396, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 188, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 1.0, 'ap_2000': 0.0011120726853131257, 'ndcg_2000': 0.11882948515216417, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U640657', 'user_index_in_cluster': 397, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 215, 'num_future_clicks': 6, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.16666666666666666, 'ap_100': 0.014084507042253521, 'ndcg_100': 0.04904474745285811, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.16666666666666666, 'ap_200': 0.014084507042253521, 'ndcg_200': 0.04904474745285811, 'num_recommendations_200': 200, 'precision_500': 0.004, 'recall_500': 0.3333333333333333, 'ap_500': 0.010918222513374824, 'ndcg_500': 0.08679074344525282, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.010918222513374824, 'ndcg_1000': 0.08679074344525282, 'num_recommendations_1000': 1000, 'precision_2000': 0.002, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.006518054628452008, 'ndcg_2000': 0.14344416729215415, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U8242', 'user_index_in_cluster': 398, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 219, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U151706', 'user_index_in_cluster': 399, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 283, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:35:06,054 - INFO - Starting user U417864 in cluster 1 (index 400)
2025-04-03 23:35:11,416 - INFO - Starting user U148744 in cluster 0 (index 391)
2025-04-03 23:35:12,603 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:35:24,392 - ERROR - Failed on user U196032 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:35:24,392 - INFO - Starting user U258789 in cluster 2 (index 638)
2025-04-03 23:35:32,649 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:35:36,999 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:35:42,504 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:36:02,726 - ERROR - Failed on user U258789 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:36:02,728 - INFO - Starting user U7929 in cluster 2 (index 639)
2025-04-03 23:36:11,072 - INFO - Starting user U361956 in cluster 1 (index 401)
2025-04-03 23:36:15,724 - INFO - Starting user U8933 in cluster 0 (index 392)
2025-04-03 23:36:24,181 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:36:29,872 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:36:34,231 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:36:38,241 - ERROR - Failed on user U7929 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:36:38,242 - INFO - Starting user U573492 in cluster 2 (index 640)
2025-04-03 23:36:56,632 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:37:09,620 - INFO - Starting user U677543 in cluster 0 (index 393)
2025-04-03 23:37:13,724 - INFO - Starting user U395639 in cluster 1 (index 402)
2025-04-03 23:37:14,946 - ERROR - Failed on user U573492 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:37:14,947 - INFO - Starting user U666492 in cluster 2 (index 641)
2025-04-03 23:37:27,980 - INFO - lens: newsdf:101527candidate_pool:5910,candidate_pool:5910
2025-04-03 23:37:31,889 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:37:32,537 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:37:52,047 - ERROR - Failed on user U666492 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:37:52,048 - INFO - Starting user U647632 in cluster 2 (index 642)
2025-04-03 23:38:11,796 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:38:11,819 - INFO - Starting user U83268 in cluster 0 (index 394)
2025-04-03 23:38:15,498 - INFO - Starting user U587560 in cluster 1 (index 403)
2025-04-03 23:38:27,595 - ERROR - Failed on user U647632 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:38:27,596 - INFO - Starting user U551369 in cluster 2 (index 643)
2025-04-03 23:38:39,526 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:38:42,450 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:38:46,052 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:38:56,300 - ERROR - Failed on user U551369 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:38:56,301 - INFO - Starting user U470506 in cluster 2 (index 644)
2025-04-03 23:39:13,922 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:39:16,381 - INFO - Starting user U155269 in cluster 0 (index 395)
2025-04-03 23:39:20,981 - INFO - Starting user U489942 in cluster 1 (index 404)
2025-04-03 23:39:31,951 - ERROR - Failed on user U470506 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:39:31,952 - INFO - Starting user U256716 in cluster 2 (index 645)
2025-04-03 23:39:34,968 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:39:38,883 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:40:01,960 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:40:14,262 - INFO - Starting user U252366 in cluster 0 (index 396)
2025-04-03 23:40:18,599 - INFO - Starting user U245679 in cluster 1 (index 405)
2025-04-03 23:40:20,032 - ERROR - Failed on user U256716 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:40:20,033 - INFO - Starting user U352143 in cluster 2 (index 646)
2025-04-03 23:40:33,951 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-03 23:40:38,249 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:40:39,807 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:40:56,749 - ERROR - Failed on user U352143 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:40:56,750 - INFO - Starting user U61372 in cluster 2 (index 647)
2025-04-03 23:41:16,641 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:41:27,287 - INFO - Starting user U601104 in cluster 1 (index 406)
2025-04-03 23:41:31,601 - INFO - Starting user U544085 in cluster 0 (index 397)
2025-04-03 23:41:32,371 - ERROR - Failed on user U61372 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:41:32,372 - INFO - Starting user U622386 in cluster 2 (index 648)
2025-04-03 23:41:51,565 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:41:58,379 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:42:02,876 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:42:05,487 - ERROR - Failed on user U622386 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:42:05,487 - INFO - Starting user U406393 in cluster 2 (index 649)
2025-04-03 23:42:26,018 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:42:40,819 - INFO - Starting user U567098 in cluster 0 (index 398)
2025-04-03 23:42:42,154 - ERROR - Failed on user U406393 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:42:42,154 - INFO - Starting user U370326 in cluster 2 (index 650)
2025-04-03 23:42:44,002 - INFO - Starting user U183799 in cluster 1 (index 407)
2025-04-03 23:42:59,808 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:43:00,832 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:43:03,197 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:43:18,923 - ERROR - Failed on user U370326 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:43:18,923 - INFO - Starting user U173980 in cluster 2 (index 651)
2025-04-03 23:43:36,214 - INFO - Starting user U512509 in cluster 0 (index 399)
2025-04-03 23:43:40,490 - INFO - Starting user U593919 in cluster 1 (index 408)
2025-04-03 23:43:41,004 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:43:53,548 - ERROR - Failed on user U173980 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:43:53,549 - INFO - Starting user U217304 in cluster 2 (index 652)
2025-04-03 23:43:56,327 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:44:01,594 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:44:14,132 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:44:31,866 - ERROR - Failed on user U217304 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:44:31,867 - INFO - Starting user U226640 in cluster 2 (index 653)
2025-04-03 23:44:40,093 - INFO - Starting user U99798 in cluster 1 (index 409)
2025-04-03 23:44:43,154 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U34177', 'user_index_in_cluster': 390, 'num_candidates': 4524, 'num_history_articles': 49, 'original_history_len': 49, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U148744', 'user_index_in_cluster': 391, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 181, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.0038461538461538464, 'ndcg_500': 0.04862788006721814, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.0038461538461538464, 'ndcg_1000': 0.04862788006721814, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 0.75, 'ap_2000': 0.002543040865689002, 'ndcg_2000': 0.12403736408471892, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U8933', 'user_index_in_cluster': 392, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 175, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U677543', 'user_index_in_cluster': 393, 'num_candidates': 4520, 'num_history_articles': 50, 'original_history_len': 113, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U83268', 'user_index_in_cluster': 394, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 116, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U155269', 'user_index_in_cluster': 395, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 105, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U252366', 'user_index_in_cluster': 396, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 53, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U544085', 'user_index_in_cluster': 397, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 89, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U567098', 'user_index_in_cluster': 398, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 50, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U512509', 'user_index_in_cluster': 399, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 67, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:44:43,183 - INFO - Starting user U639308 in cluster 0 (index 400)
2025-04-03 23:44:50,815 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:45:00,611 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:45:06,188 - ERROR - Failed on user U226640 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:45:06,189 - INFO - Starting user U224515 in cluster 2 (index 654)
2025-04-03 23:45:11,683 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:45:12,160 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U417864', 'user_index_in_cluster': 400, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 167, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U361956', 'user_index_in_cluster': 401, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 144, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U395639', 'user_index_in_cluster': 402, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 104, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.3333333333333333, 'ap_500': 0.0022522522522522522, 'ndcg_500': 0.0533413026454454, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.0022522522522522522, 'ndcg_1000': 0.0533413026454454, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0022522522522522522, 'ndcg_2000': 0.0533413026454454, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U587560', 'user_index_in_cluster': 403, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 111, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 1.0, 'ap_50': 0.02564102564102564, 'ndcg_50': 0.18790182470910757, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 1.0, 'ap_100': 0.02564102564102564, 'ndcg_100': 0.18790182470910757, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 1.0, 'ap_200': 0.02564102564102564, 'ndcg_200': 0.18790182470910757, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 1.0, 'ap_500': 0.02564102564102564, 'ndcg_500': 0.18790182470910757, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.02564102564102564, 'ndcg_1000': 0.18790182470910757, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.02564102564102564, 'ndcg_2000': 0.18790182470910757, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U489942', 'user_index_in_cluster': 404, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 206, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U245679', 'user_index_in_cluster': 405, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 200, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U601104', 'user_index_in_cluster': 406, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 196, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U183799', 'user_index_in_cluster': 407, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 279, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.02, 'recall_50': 0.25, 'ap_50': 0.022727272727272728, 'ndcg_50': 0.07108348368841133, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.25, 'ap_100': 0.022727272727272728, 'ndcg_100': 0.07108348368841133, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.25, 'ap_200': 0.022727272727272728, 'ndcg_200': 0.07108348368841133, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.022727272727272728, 'ndcg_500': 0.07108348368841133, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.022727272727272728, 'ndcg_1000': 0.07108348368841133, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.25, 'ap_2000': 0.022727272727272728, 'ndcg_2000': 0.07108348368841133, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U593919', 'user_index_in_cluster': 408, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 248, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.25, 'ap_500': 0.0038461538461538464, 'ndcg_500': 0.04862788006721814, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.5, 'ap_1000': 0.003127896200185357, 'ndcg_1000': 0.08887862110994935, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.003127896200185357, 'ndcg_2000': 0.08887862110994935, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U99798', 'user_index_in_cluster': 409, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 131, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.0009845164395121523, 'ndcg_2000': 0.07456805658483633, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:45:12,190 - INFO - Starting user U619905 in cluster 1 (index 410)
2025-04-03 23:45:20,716 - INFO - Starting user U541417 in cluster 0 (index 401)
2025-04-03 23:45:26,315 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:45:40,188 - ERROR - Failed on user U224515 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:45:40,188 - INFO - Starting user U47620 in cluster 2 (index 655)
2025-04-03 23:45:40,681 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:45:43,687 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:46:03,782 - INFO - Starting user U293912 in cluster 0 (index 402)
2025-04-03 23:46:04,861 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:46:12,144 - INFO - Starting user U307161 in cluster 1 (index 411)
2025-04-03 23:46:19,152 - ERROR - Failed on user U47620 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:46:19,152 - INFO - Starting user U512156 in cluster 2 (index 656)
2025-04-03 23:46:23,605 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:46:32,232 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:46:39,370 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:46:56,914 - ERROR - Failed on user U512156 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:46:56,914 - INFO - Starting user U368209 in cluster 2 (index 657)
2025-04-03 23:47:04,522 - INFO - Starting user U80812 in cluster 1 (index 412)
2025-04-03 23:47:08,284 - INFO - Starting user U634148 in cluster 0 (index 403)
2025-04-03 23:47:16,392 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:47:23,327 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:47:29,752 - ERROR - Failed on user U368209 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:47:29,752 - INFO - Starting user U234784 in cluster 2 (index 658)
2025-04-03 23:47:33,939 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:47:44,330 - INFO - Starting user U308714 in cluster 1 (index 413)
2025-04-03 23:47:50,372 - INFO - Starting user U242203 in cluster 0 (index 404)
2025-04-03 23:47:51,571 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:48:03,490 - ERROR - Failed on user U234784 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:48:03,490 - INFO - Starting user U343756 in cluster 2 (index 659)
2025-04-03 23:48:06,561 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:48:09,414 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:48:23,711 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:48:40,192 - ERROR - Failed on user U343756 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:48:40,192 - INFO - Starting user U649864 in cluster 2 (index 660)
2025-04-03 23:48:47,197 - INFO - Starting user U488570 in cluster 0 (index 405)
2025-04-03 23:48:51,103 - INFO - Starting user U610970 in cluster 1 (index 414)
2025-04-03 23:48:59,654 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:49:06,920 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:49:10,139 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:49:12,772 - ERROR - Failed on user U649864 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:49:12,772 - INFO - Starting user U83271 in cluster 2 (index 661)
2025-04-03 23:49:33,749 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:49:51,680 - ERROR - Failed on user U83271 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:49:51,681 - INFO - Starting user U297811 in cluster 2 (index 662)
2025-04-03 23:49:55,793 - INFO - Starting user U459506 in cluster 0 (index 406)
2025-04-03 23:49:59,327 - INFO - Starting user U195381 in cluster 1 (index 415)
2025-04-03 23:50:11,065 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:50:15,169 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:50:18,288 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:50:24,858 - ERROR - Failed on user U297811 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:50:24,860 - INFO - Starting user U352653 in cluster 2 (index 663)
2025-04-03 23:50:44,832 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:50:56,778 - INFO - Starting user U448984 in cluster 1 (index 416)
2025-04-03 23:51:00,804 - INFO - Starting user U267024 in cluster 0 (index 407)
2025-04-03 23:51:01,596 - ERROR - Failed on user U352653 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:51:01,597 - INFO - Starting user U22525 in cluster 2 (index 664)
2025-04-03 23:51:20,399 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:51:21,180 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:51:24,379 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-03 23:51:33,629 - ERROR - Failed on user U22525 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:51:33,629 - INFO - Starting user U514864 in cluster 2 (index 665)
2025-04-03 23:51:53,246 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:51:58,554 - INFO - Starting user U33172 in cluster 1 (index 417)
2025-04-03 23:52:04,491 - INFO - Starting user U60151 in cluster 0 (index 408)
2025-04-03 23:52:10,039 - ERROR - Failed on user U514864 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:52:10,040 - INFO - Starting user U538860 in cluster 2 (index 666)
2025-04-03 23:52:17,545 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:52:24,499 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:52:29,533 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:52:46,397 - ERROR - Failed on user U538860 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:52:46,397 - INFO - Starting user U263427 in cluster 2 (index 667)
2025-04-03 23:52:57,808 - INFO - Starting user U188593 in cluster 0 (index 409)
2025-04-03 23:53:01,640 - INFO - Starting user U451468 in cluster 1 (index 418)
2025-04-03 23:53:07,396 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:53:17,130 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:53:20,321 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:53:20,897 - ERROR - Failed on user U263427 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:53:20,898 - INFO - Starting user U183441 in cluster 2 (index 668)
2025-04-03 23:53:41,227 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:53:56,805 - INFO - Starting user U92697 in cluster 1 (index 419)
2025-04-03 23:53:58,150 - ERROR - Failed on user U183441 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:53:58,150 - INFO - Starting user U270385 in cluster 2 (index 669)
2025-04-03 23:54:00,708 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U639308', 'user_index_in_cluster': 400, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 61, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U541417', 'user_index_in_cluster': 401, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U293912', 'user_index_in_cluster': 402, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 62, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U634148', 'user_index_in_cluster': 403, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 85, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 1.0, 'ap_1000': 0.0019193857965451055, 'ndcg_1000': 0.11076765756975603, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0019193857965451055, 'ndcg_2000': 0.11076765756975603, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U242203', 'user_index_in_cluster': 404, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U488570', 'user_index_in_cluster': 405, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 70, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U459506', 'user_index_in_cluster': 406, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 88, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U267024', 'user_index_in_cluster': 407, 'num_candidates': 4521, 'num_history_articles': 50, 'original_history_len': 52, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U60151', 'user_index_in_cluster': 408, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 140, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U188593', 'user_index_in_cluster': 409, 'num_candidates': 4527, 'num_history_articles': 41, 'original_history_len': 41, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:54:00,728 - INFO - Starting user U453788 in cluster 0 (index 410)
2025-04-03 23:54:18,308 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-03 23:54:21,290 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:54:23,714 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:54:33,134 - ERROR - Failed on user U270385 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:54:33,135 - INFO - Starting user U61039 in cluster 2 (index 670)
2025-04-03 23:54:57,544 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:54:59,147 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U619905', 'user_index_in_cluster': 410, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 153, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U307161', 'user_index_in_cluster': 411, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 149, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U80812', 'user_index_in_cluster': 412, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 121, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.0006887052341597796, 'ndcg_2000': 0.05836818259194251, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U308714', 'user_index_in_cluster': 413, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 198, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U610970', 'user_index_in_cluster': 414, 'num_candidates': 4523, 'num_history_articles': 50, 'original_history_len': 174, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U195381', 'user_index_in_cluster': 415, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 139, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.3333333333333333, 'ap_2000': 0.0005157297576070139, 'ndcg_2000': 0.04296699876206555, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U448984', 'user_index_in_cluster': 416, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 133, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.0025188916876574307, 'ndcg_500': 0.039269994954610754, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.002679900389283261, 'ndcg_1000': 0.07511641909612254, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.002679900389283261, 'ndcg_2000': 0.07511641909612254, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U33172', 'user_index_in_cluster': 417, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 372, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U451468', 'user_index_in_cluster': 418, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 237, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 1.0, 'ap_2000': 0.0006711409395973154, 'ndcg_2000': 0.0948580798371205, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U92697', 'user_index_in_cluster': 419, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 201, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-03 23:54:59,161 - INFO - Starting user U403828 in cluster 1 (index 420)
2025-04-03 23:55:07,913 - INFO - Starting user U42974 in cluster 0 (index 411)
2025-04-03 23:55:15,641 - ERROR - Failed on user U61039 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:55:15,641 - INFO - Starting user U17551 in cluster 2 (index 671)
2025-04-03 23:55:19,274 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:55:27,796 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-03 23:55:35,003 - INFO - lens: newsdf:101527candidate_pool:5907,candidate_pool:5907
2025-04-03 23:55:53,778 - ERROR - Failed on user U17551 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:55:53,779 - INFO - Starting user U607543 in cluster 2 (index 672)
2025-04-03 23:55:58,396 - INFO - Starting user U8064 in cluster 1 (index 421)
2025-04-03 23:56:02,171 - INFO - Starting user U354393 in cluster 0 (index 412)
2025-04-03 23:56:12,500 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:56:17,729 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:56:21,021 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:56:25,989 - ERROR - Failed on user U607543 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:56:25,990 - INFO - Starting user U162260 in cluster 2 (index 673)
2025-04-03 23:56:45,722 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:56:51,360 - INFO - Starting user U263241 in cluster 1 (index 422)
2025-04-03 23:56:57,492 - INFO - Starting user U283442 in cluster 0 (index 413)
2025-04-03 23:57:02,819 - ERROR - Failed on user U162260 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:57:02,820 - INFO - Starting user U57635 in cluster 2 (index 674)
2025-04-03 23:57:16,300 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-03 23:57:20,556 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:57:22,135 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:57:37,433 - ERROR - Failed on user U57635 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:57:37,433 - INFO - Starting user U666223 in cluster 2 (index 675)
2025-04-03 23:57:57,798 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:58:02,498 - INFO - Starting user U690425 in cluster 1 (index 423)
2025-04-03 23:58:09,054 - INFO - Starting user U369900 in cluster 0 (index 414)
2025-04-03 23:58:14,159 - ERROR - Failed on user U666223 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:58:14,159 - INFO - Starting user U484664 in cluster 2 (index 676)
2025-04-03 23:58:20,574 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-03 23:58:27,355 - INFO - lens: newsdf:101527candidate_pool:5908,candidate_pool:5908
2025-04-03 23:58:32,593 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:58:49,481 - ERROR - Failed on user U484664 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:58:49,482 - INFO - Starting user U105763 in cluster 2 (index 677)
2025-04-03 23:59:02,452 - INFO - Starting user U629670 in cluster 0 (index 415)
2025-04-03 23:59:05,752 - INFO - Starting user U707162 in cluster 1 (index 424)
2025-04-03 23:59:09,191 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:59:20,879 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:59:23,182 - ERROR - Failed on user U105763 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-03 23:59:23,182 - INFO - Starting user U481974 in cluster 2 (index 678)
2025-04-03 23:59:23,776 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-03 23:59:43,472 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-03 23:59:57,824 - INFO - Starting user U513467 in cluster 0 (index 416)
2025-04-04 00:00:01,950 - INFO - Starting user U567783 in cluster 1 (index 425)
2025-04-04 00:00:03,441 - ERROR - Failed on user U481974 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:00:03,442 - INFO - Starting user U458885 in cluster 2 (index 679)
2025-04-04 00:00:20,903 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-04 00:00:23,172 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-04 00:00:24,986 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-04 00:00:45,134 - ERROR - Failed on user U458885 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:00:45,134 - INFO - Starting user U525391 in cluster 2 (index 680)
2025-04-04 00:00:57,112 - INFO - Starting user U526075 in cluster 0 (index 417)
2025-04-04 00:01:00,040 - INFO - Starting user U586055 in cluster 1 (index 426)
2025-04-04 00:01:02,665 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-04 00:01:14,537 - INFO - lens: newsdf:101527candidate_pool:5909,candidate_pool:5909
2025-04-04 00:01:16,343 - ERROR - Failed on user U525391 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:01:16,343 - INFO - Starting user U349920 in cluster 2 (index 681)
2025-04-04 00:01:17,078 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-04 00:01:35,949 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-04 00:01:47,911 - INFO - Starting user U262646 in cluster 0 (index 418)
2025-04-04 00:01:51,884 - ERROR - Failed on user U349920 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:01:51,884 - INFO - Starting user U74753 in cluster 2 (index 682)
2025-04-04 00:01:52,452 - INFO - Starting user U272966 in cluster 1 (index 427)
2025-04-04 00:02:09,004 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-04 00:02:12,347 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-04 00:02:14,625 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-04 00:02:32,470 - ERROR - Failed on user U74753 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:02:32,471 - INFO - Starting user U457707 in cluster 2 (index 683)
2025-04-04 00:02:49,166 - INFO - Starting user U234293 in cluster 1 (index 428)
2025-04-04 00:02:51,265 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-04 00:02:53,323 - INFO - Starting user U459658 in cluster 0 (index 419)
2025-04-04 00:03:04,775 - ERROR - Failed on user U457707 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:03:04,776 - INFO - Starting user U570721 in cluster 2 (index 684)
2025-04-04 00:03:07,550 - INFO - lens: newsdf:101527candidate_pool:5916,candidate_pool:5916
2025-04-04 00:03:21,468 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-04 00:03:21,543 - INFO - Starting user U684528 in cluster 1 (index 429)
2025-04-04 00:03:22,945 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-04 00:03:32,108 - INFO - Writing partial rows: [{'cluster_id': 0, 'user_id': 'U453788', 'user_index_in_cluster': 410, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 107, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U42974', 'user_index_in_cluster': 411, 'num_candidates': 4522, 'num_history_articles': 50, 'original_history_len': 52, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U354393', 'user_index_in_cluster': 412, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 102, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U283442', 'user_index_in_cluster': 413, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 166, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U369900', 'user_index_in_cluster': 414, 'num_candidates': 4518, 'num_history_articles': 40, 'original_history_len': 40, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.6666666666666666, 'ap_1000': 0.002022183473796377, 'ndcg_1000': 0.09932314481438068, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.6666666666666666, 'ap_2000': 0.002022183473796377, 'ndcg_2000': 0.09932314481438068, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U629670', 'user_index_in_cluster': 415, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 125, 'num_future_clicks': 1, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U513467', 'user_index_in_cluster': 416, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 91, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U526075', 'user_index_in_cluster': 417, 'num_candidates': 4519, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U262646', 'user_index_in_cluster': 418, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 69, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 0, 'user_id': 'U459658', 'user_index_in_cluster': 419, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 52, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.01, 'recall_100': 0.5, 'ap_100': 0.01818181818181818, 'ndcg_100': 0.10558114683788161, 'num_recommendations_100': 100, 'precision_200': 0.005, 'recall_200': 0.5, 'ap_200': 0.01818181818181818, 'ndcg_200': 0.10558114683788161, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.5, 'ap_500': 0.01818181818181818, 'ndcg_500': 0.10558114683788161, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.5, 'ap_1000': 0.01818181818181818, 'ndcg_1000': 0.10558114683788161, 'num_recommendations_1000': 1000, 'precision_2000': 0.0005, 'recall_2000': 0.5, 'ap_2000': 0.01818181818181818, 'ndcg_2000': 0.10558114683788161, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-04 00:03:32,121 - INFO - Starting user U608717 in cluster 0 (index 420)
2025-04-04 00:03:37,795 - ERROR - Failed on user U570721 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:03:37,795 - INFO - Starting user U359042 in cluster 2 (index 685)
2025-04-04 00:03:49,563 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-04 00:03:50,444 - INFO - lens: newsdf:101527candidate_pool:5912,candidate_pool:5912
2025-04-04 00:04:01,178 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-04 00:04:18,248 - ERROR - Failed on user U359042 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:04:18,249 - INFO - Starting user U407937 in cluster 2 (index 686)
2025-04-04 00:04:29,359 - INFO - Starting user U517556 in cluster 0 (index 421)
2025-04-04 00:04:33,381 - INFO - Writing partial rows: [{'cluster_id': 1, 'user_id': 'U403828', 'user_index_in_cluster': 420, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 213, 'num_future_clicks': 4, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.001, 'recall_1000': 0.25, 'ap_1000': 0.001004016064257028, 'ndcg_1000': 0.039189080100623434, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.5, 'ap_2000': 0.00118976594134447, 'ndcg_2000': 0.07634405804451502, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U8064', 'user_index_in_cluster': 421, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 247, 'num_future_clicks': 18, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.006, 'recall_500': 0.16666666666666666, 'ap_500': 0.004679847135862775, 'ndcg_500': 0.05258679201311187, 'num_recommendations_500': 500, 'precision_1000': 0.006, 'recall_1000': 0.3333333333333333, 'ap_1000': 0.005679279559368526, 'ndcg_1000': 0.10042946986335057, 'num_recommendations_1000': 1000, 'precision_2000': 0.0055, 'recall_2000': 0.6111111111111112, 'ap_2000': 0.005897272081321741, 'ndcg_2000': 0.17283242077245692, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U263241', 'user_index_in_cluster': 422, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 215, 'num_future_clicks': 3, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0015, 'recall_2000': 1.0, 'ap_2000': 0.0014595907429775252, 'ndcg_2000': 0.13602378585097152, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U690425', 'user_index_in_cluster': 423, 'num_candidates': 4527, 'num_history_articles': 50, 'original_history_len': 110, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U707162', 'user_index_in_cluster': 424, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 154, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U567783', 'user_index_in_cluster': 425, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 230, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U586055', 'user_index_in_cluster': 426, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 227, 'num_future_clicks': 5, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.002, 'recall_500': 0.2, 'ap_500': 0.002277904328018223, 'ndcg_500': 0.03862274366818804, 'num_recommendations_500': 500, 'precision_1000': 0.002, 'recall_1000': 0.4, 'ap_1000': 0.0023554971761745617, 'ndcg_1000': 0.07364277553132284, 'num_recommendations_1000': 1000, 'precision_2000': 0.001, 'recall_2000': 0.4, 'ap_2000': 0.0023554971761745617, 'ndcg_2000': 0.07364277553132284, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U272966', 'user_index_in_cluster': 427, 'num_candidates': 4524, 'num_history_articles': 50, 'original_history_len': 141, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U234293', 'user_index_in_cluster': 428, 'num_candidates': 4526, 'num_history_articles': 50, 'original_history_len': 87, 'num_future_clicks': 0, 'precision_5': 0.0, 'recall_5': 0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}, {'cluster_id': 1, 'user_id': 'U684528', 'user_index_in_cluster': 429, 'num_candidates': 4525, 'num_history_articles': 50, 'original_history_len': 188, 'num_future_clicks': 2, 'precision_5': 0.0, 'recall_5': 0.0, 'ap_5': 0.0, 'ndcg_5': 0.0, 'num_recommendations_5': 5, 'precision_10': 0.0, 'recall_10': 0.0, 'ap_10': 0.0, 'ndcg_10': 0.0, 'num_recommendations_10': 10, 'precision_20': 0.0, 'recall_20': 0.0, 'ap_20': 0.0, 'ndcg_20': 0.0, 'num_recommendations_20': 20, 'precision_50': 0.0, 'recall_50': 0.0, 'ap_50': 0.0, 'ndcg_50': 0.0, 'num_recommendations_50': 50, 'precision_100': 0.0, 'recall_100': 0.0, 'ap_100': 0.0, 'ndcg_100': 0.0, 'num_recommendations_100': 100, 'precision_200': 0.0, 'recall_200': 0.0, 'ap_200': 0.0, 'ndcg_200': 0.0, 'num_recommendations_200': 200, 'precision_500': 0.0, 'recall_500': 0.0, 'ap_500': 0.0, 'ndcg_500': 0.0, 'num_recommendations_500': 500, 'precision_1000': 0.0, 'recall_1000': 0.0, 'ap_1000': 0.0, 'ndcg_1000': 0.0, 'num_recommendations_1000': 1000, 'precision_2000': 0.0, 'recall_2000': 0.0, 'ap_2000': 0.0, 'ndcg_2000': 0.0, 'num_recommendations_2000': 2000, 'status': 'DONE'}]
2025-04-04 00:04:33,392 - INFO - Starting user U483267 in cluster 1 (index 430)
2025-04-04 00:04:36,658 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-04 00:04:48,167 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-04 00:04:49,593 - ERROR - Failed on user U407937 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:04:49,594 - INFO - Starting user U598565 in cluster 2 (index 687)
2025-04-04 00:04:52,942 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
2025-04-04 00:05:08,339 - INFO - lens: newsdf:101527candidate_pool:5915,candidate_pool:5915
2025-04-04 00:05:25,495 - INFO - Starting user U132652 in cluster 0 (index 422)
2025-04-04 00:05:26,908 - ERROR - Failed on user U598565 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:05:26,909 - INFO - Starting user U540308 in cluster 2 (index 688)
2025-04-04 00:05:29,799 - INFO - Starting user U292804 in cluster 1 (index 431)
2025-04-04 00:05:47,075 - INFO - lens: newsdf:101527candidate_pool:5911,candidate_pool:5911
2025-04-04 00:05:47,352 - INFO - lens: newsdf:101527candidate_pool:5913,candidate_pool:5913
2025-04-04 00:05:50,362 - INFO - lens: newsdf:101527candidate_pool:5917,candidate_pool:5917
2025-04-04 00:06:00,200 - ERROR - Failed on user U540308 in cluster 2 with error: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]
Traceback (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level
    candidate_scores = score_candidates_ensemble_batch(
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch
    preds = score_candidates_in_batch(history_tensor, candidate_tensors, model, batch_size)
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch
    preds = model.predict(
  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/t/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/test_recommender.py", line 30, in <module>

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 2688, in main

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 1072, in run_cluster_experiments_user_level

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 996, in score_candidates_ensemble_batch

  File "/home/t/24-11-9/news/new/news/mirror/newest-version/4/2/news/backend/cluster_utils.py", line 979, in score_candidates_in_batch

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 562, in predict

  File "/home/t/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py", line 259, in one_step_on_data_distributed

Out of memory while trying to allocate 2385439360 bytes.
	 [[{{node StatefulPartitionedCall}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.
 [Op:__inference_one_step_on_data_distributed_44214]

2025-04-04 00:06:00,201 - INFO - Starting user U622225 in cluster 2 (index 689)
2025-04-04 00:06:18,499 - INFO - lens: newsdf:101527candidate_pool:5914,candidate_pool:5914
