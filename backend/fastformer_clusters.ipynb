{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2114b1-c9d7-405f-bb2b-bbbd0d7ad64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 12:49:42.443149: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 12:49:42.451846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-28 12:49:42.462376: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-28 12:49:42.465450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 12:49:42.473540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 12:49:43.304963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/t/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Number of columns in user_category_profiles: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n",
      "Cluster 0: 44551 training samples, 11138 validation samples.\n",
      "Cluster 1: 48725 training samples, 12182 validation samples.\n",
      "Cluster 2: 53212 training samples, 13304 validation samples.\n",
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Vocabulary Size: 88583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████▌                                | 1214479/2232748 [04:02<03:23, 5011.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m behaviors_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbehaviors.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m data_dir, vocab_size, max_history_length, max_title_length, news_df, train_df, behaviors_df, user_category_profiles, clustered_data, tokenizer, num_clusters \u001b[38;5;241m=\u001b[39m init(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_train_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnews_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnews_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbehaviours_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbehaviors_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_category_profiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_category_profiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_title_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_history_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train cluster-specific models\u001b[39;00m\n\u001b[1;32m     17\u001b[0m models \u001b[38;5;241m=\u001b[39m train_cluster_models(\n\u001b[1;32m     18\u001b[0m     clustered_data\u001b[38;5;241m=\u001b[39mclustered_data,\n\u001b[1;32m     19\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/24-11-9/news/new/news/mirror/news/backend/utils.py:416\u001b[0m, in \u001b[0;36mprepare_train_df\u001b[0;34m(data_dir, news_file, behaviours_file, user_category_profiles, num_clusters, fraction, max_title_length, max_history_length)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Parse user history\u001b[39;00m\n\u001b[1;32m    415\u001b[0m history_ids \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistoryText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistoryText\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m--> 416\u001b[0m history_texts \u001b[38;5;241m=\u001b[39m [news_text_dict\u001b[38;5;241m.\u001b[39mget(nid, [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mmax_title_length) \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m history_ids]\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Limit history length\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history_texts) \u001b[38;5;241m<\u001b[39m max_history_length:\n",
      "File \u001b[0;32m~/24-11-9/news/new/news/mirror/news/backend/utils.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Parse user history\u001b[39;00m\n\u001b[1;32m    415\u001b[0m history_ids \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistoryText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistoryText\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m--> 416\u001b[0m history_texts \u001b[38;5;241m=\u001b[39m [news_text_dict\u001b[38;5;241m.\u001b[39mget(nid, [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mmax_title_length) \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m history_ids]\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Limit history length\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history_texts) \u001b[38;5;241m<\u001b[39m max_history_length:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import build_and_load_weights, get_models, init, prepare_train_df, train_cluster_models\n",
    "news_file = 'news.tsv'\n",
    "behaviors_file = 'behaviors.tsv'\n",
    "data_dir, vocab_size, max_history_length, max_title_length, news_df, train_df, behaviors_df, user_category_profiles, clustered_data, tokenizer, num_clusters = init(False, False)\n",
    "clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters = prepare_train_df(\n",
    "    data_dir=data_dir,\n",
    "    news_file=news_file,\n",
    "    behaviours_file=behaviors_file,\n",
    "    user_category_profiles=user_category_profiles,\n",
    "    num_clusters=3,\n",
    "    fraction=1,\n",
    "    max_title_length=30,\n",
    "    max_history_length=50\n",
    ")\n",
    "\n",
    "# Train cluster-specific models\n",
    "models = train_cluster_models(\n",
    "    clustered_data=clustered_data,\n",
    "    tokenizer=tokenizer,\n",
    "    vocab_size=vocab_size,\n",
    "    max_history_length=max_history_length,\n",
    "    max_title_length=max_title_length,\n",
    "    num_clusters=num_clusters,\n",
    "    batch_size=64,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71c1b1d-2631-4dce-ba32-c9b7e47aeaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 20:58:10.930882: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 20:58:10.971503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-01 20:58:11.005877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-01 20:58:11.023087: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 20:58:11.056417: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-01 20:58:12.013483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/t/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Number of columns in user_category_profiles: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n",
      "Cluster 0: 44551 training samples, 11138 validation samples.\n",
      "Cluster 1: 48725 training samples, 12182 validation samples.\n",
      "Cluster 2: 53212 training samples, 13304 validation samples.\n",
      "\n",
      "Loading model for Cluster 0 from fastformer_cluster_0_full_balanced_1_epoch.weights.h5\n",
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Number of columns in user_category_profiles: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n",
      "Cluster 0: 44551 training samples, 11138 validation samples.\n",
      "Cluster 1: 48725 training samples, 12182 validation samples.\n",
      "Cluster 2: 53212 training samples, 13304 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740855568.865741   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855568.866970   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.025505   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.025605   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.025632   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.025800   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.528599   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.528651   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.528670   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.528681   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.528692   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.528703   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.574517   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.574661   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1740855569.574889   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-01 20:59:29.575176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1740855569.575304   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-01 20:59:29.575310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 1, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1740855569.575404   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-01 20:59:29.575556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9517 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1740855569.578911   54032 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-01 20:59:29.578999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 5660 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:05:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740855575.725307   54555 service.cc:146] XLA service 0x7f5a78018c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1740855575.725694   54555 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 SUPER, Compute Capability 8.9\n",
      "I0000 00:00:1740855575.725697   54555 service.cc:154]   StreamExecutor device (1): NVIDIA GeForce RTX 3050, Compute Capability 8.6\n",
      "2025-03-01 20:59:36.008871: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-01 20:59:36.451063: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2025-03-01 20:59:37.556578: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_464', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-03-01 20:59:37.900795: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_464', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-03-01 20:59:37.944121: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_464', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-03-01 20:59:38.071315: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_464', 196 bytes spill stores, 168 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740855583.335409   54555 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/t/tf/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 32 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for Cluster 0 into fastformer_cluster_0_full_balanced_1_epoch.h5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model for Cluster 1 from fastformer_cluster_1_full_balanced_1_epoch.weights.h5\n",
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Number of columns in user_category_profiles: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n",
      "Cluster 0: 44551 training samples, 11138 validation samples.\n",
      "Cluster 1: 48725 training samples, 12182 validation samples.\n",
      "Cluster 2: 53212 training samples, 13304 validation samples.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tf/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 32 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for Cluster 1 into fastformer_cluster_1_full_balanced_1_epoch.h5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model for Cluster 2 from fastformer_cluster_2_full_balanced_1_epoch.weights.h5\n",
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "Number of columns in user_category_profiles: 18\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of unique users in behaviors_df: 711222\n",
      "Number of users in user_category_profiles: 711222\n",
      "Number of missing UserIDs in user_category_profiles: 0\n",
      "Cluster 0: 44551 training samples, 11138 validation samples.\n",
      "Cluster 1: 48725 training samples, 12182 validation samples.\n",
      "Cluster 2: 53212 training samples, 13304 validation samples.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tf/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 32 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for Cluster 2 into fastformer_cluster_2_full_balanced_1_epoch.h5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning models list\n",
      "{0: <Functional name=functional, built=True>, 1: <Functional name=functional_1, built=True>, 2: <Functional name=functional_2, built=True>}\n",
      "{0: <Functional name=functional, built=True>, 1: <Functional name=functional_1, built=True>, 2: <Functional name=functional_2, built=True>}\n"
     ]
    }
   ],
   "source": [
    "from utils import get_models\n",
    "models = get_models()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2747c3fa-3611-4cb1-af69-f8466e726bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-25 06:32:19.425189: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-25 06:32:19.553275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-25 06:32:19.607693: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-25 06:32:19.621497: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 06:32:19.716996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-25 06:32:20.680723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/t/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news data:\n",
      "   NewsID   Category               SubCategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               Title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             URL  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                       TitleEntities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                    AbstractEntities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "\n",
      "Loaded behaviors data:\n",
      "   ImpressionID   UserID                    Time  \\\n",
      "0             1   U87243  11/10/2019 11:30:54 AM   \n",
      "1             2  U598644   11/12/2019 1:45:29 PM   \n",
      "2             3  U532401  11/13/2019 11:23:03 AM   \n",
      "3             4  U593596  11/12/2019 12:24:09 PM   \n",
      "4             5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                         HistoryText  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         Impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f464d25c610>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/t/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils import build_and_load_weights, train_models, init\n",
    "\n",
    "def is_colab():\n",
    "    return 'COLAB_GPU' in os.environ\n",
    "zip_file = f\"MINDlarge_train.zip\"\n",
    "#valid_zip_file = f\"MINDlarge_test.zip\"\n",
    "valid_zip_file = f\"MINDlarge_dev.zip\"\n",
    "data_dir = 'dataset/train/'  # Adjust path as necessary\n",
    "valid_data_dir = 'dataset/valid/'  # Adjust path as necessary\n",
    "if is_colab():\n",
    "    print(\"Running on Google colab\")\n",
    "    data_dir = '/content/train/'\n",
    "    valid_data_dir = '/content/valid/'\n",
    "#data_dir = 'dataset/small/train/'  # Adjust path as necessary\n",
    "#zip_file = f\"MINDsmall_train.zip\"\n",
    "zip_file_path = f\"{data_dir}{zip_file}\"\n",
    "valid_zip_file_path = f\"{valid_data_dir}{valid_zip_file}\"\n",
    "local_model_path = hf_hub_download(\n",
    "    repo_id=f\"Teemu5/news\",\n",
    "    filename=zip_file,\n",
    "    local_dir=data_dir\n",
    ")\n",
    "local_model_path = hf_hub_download(\n",
    "    repo_id=f\"Teemu5/news\",\n",
    "    filename=valid_zip_file,\n",
    "    local_dir=valid_data_dir\n",
    ")\n",
    "# Get the directory where the zip file is located\n",
    "output_folder = os.path.dirname(zip_file_path)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_folder)\n",
    "if is_colab():\n",
    "  valid_output_folder = os.path.dirname(valid_zip_file_path)\n",
    "  with zipfile.ZipFile(valid_zip_file_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(os.path.dirname(valid_output_folder))\n",
    "news_file = 'news.tsv'\n",
    "behaviors_file = 'behaviors.tsv'\n",
    "\n",
    "# Load news data\n",
    "news_path = os.path.join(data_dir, news_file)\n",
    "news_df = pd.read_csv(\n",
    "    news_path,\n",
    "    sep='\\t',\n",
    "    names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"Loaded news data:\")\n",
    "print(news_df.head())\n",
    "\n",
    "# Load behaviors data\n",
    "behaviors_path = os.path.join(data_dir, behaviors_file)\n",
    "behaviors_df = pd.read_csv(\n",
    "    behaviors_path,\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded behaviors data:\")\n",
    "print(behaviors_df.head())\n",
    "# Handle missing 'HistoryText' by replacing NaN with empty string\n",
    "behaviors_df['HistoryText'] = behaviors_df['HistoryText'].fillna('')\n",
    "\n",
    "# Create a NewsID to Category mapping\n",
    "newsid_to_category = news_df.set_index('NewsID')['Category'].to_dict()\n",
    "\n",
    "# Function to extract categories from HistoryText\n",
    "def extract_categories(history_text):\n",
    "    if not history_text:\n",
    "        return []\n",
    "    news_ids = history_text.split(' ')\n",
    "    categories = [newsid_to_category.get(news_id, 'Unknown') for news_id in news_ids]\n",
    "    return categories\n",
    "\n",
    "# Apply the function to extract categories\n",
    "behaviors_df['HistoryCategories'] = behaviors_df['HistoryText'].apply(extract_categories)\n",
    "\n",
    "print(\"\\nSample HistoryCategories:\")\n",
    "print(behaviors_df[['UserID', 'HistoryCategories']].head())\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to hold category counts per user\n",
    "user_category_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Populate the dictionary\n",
    "for idx, row in behaviors_df.iterrows():\n",
    "    user_id = row['UserID']\n",
    "    categories = row['HistoryCategories']\n",
    "    for category in categories:\n",
    "        user_category_counts[user_id][category] += 1\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "user_category_profiles = pd.DataFrame(user_category_counts).T.fillna(0)\n",
    "\n",
    "# Optionally, rename columns to indicate category\n",
    "user_category_profiles.columns = [f'Category_{cat}' for cat in user_category_profiles.columns]\n",
    "\n",
    "print(\"\\nCreated user_category_profiles:\")\n",
    "print(user_category_profiles.head())\n",
    "print(f\"\\nShape of user_category_profiles: {user_category_profiles.shape}\")\n",
    "# Handle missing 'HistoryText' by replacing NaN with empty string\n",
    "behaviors_df['HistoryText'] = behaviors_df['HistoryText'].fillna('')\n",
    "\n",
    "# Create a NewsID to Category mapping\n",
    "newsid_to_category = news_df.set_index('NewsID')['Category'].to_dict()\n",
    "\n",
    "# Get all unique UserIDs from behaviors_df\n",
    "unique_user_ids = behaviors_df['UserID'].unique()\n",
    "\n",
    "# Function to extract categories from HistoryText\n",
    "def extract_categories(history_text):\n",
    "    if not history_text:\n",
    "        return []\n",
    "    news_ids = history_text.split(' ')\n",
    "    categories = [newsid_to_category.get(news_id, 'Unknown') for news_id in news_ids]\n",
    "    return categories\n",
    "\n",
    "# Apply the function to extract categories\n",
    "behaviors_df['HistoryCategories'] = behaviors_df['HistoryText'].apply(extract_categories)\n",
    "\n",
    "# Explode 'HistoryCategories' to have one category per row\n",
    "behaviors_exploded = behaviors_df[['UserID', 'HistoryCategories']].explode('HistoryCategories')\n",
    "\n",
    "# Replace missing categories with 'Unknown'\n",
    "behaviors_exploded['HistoryCategories'] = behaviors_exploded['HistoryCategories'].fillna('Unknown')\n",
    "\n",
    "# Create a cross-tabulation (pivot table) of counts\n",
    "user_category_counts = pd.crosstab(\n",
    "    index=behaviors_exploded['UserID'],\n",
    "    columns=behaviors_exploded['HistoryCategories']\n",
    ")\n",
    "\n",
    "# Rename columns to include 'Category_' prefix\n",
    "user_category_counts.columns = [f'Category_{col}' for col in user_category_counts.columns]\n",
    "\n",
    "# Reindex to include all users, filling missing values with zero\n",
    "user_category_profiles = user_category_counts.reindex(unique_user_ids, fill_value=0)\n",
    "\n",
    "print(f\"\\nCreated user_category_profiles with {user_category_profiles.shape[0]} users and {user_category_profiles.shape[1]} categories.\")\n",
    "\n",
    "# Determine top N categories\n",
    "top_n = 20\n",
    "category_counts = news_df['Category'].value_counts()\n",
    "top_categories = category_counts.nlargest(top_n).index.tolist()\n",
    "\n",
    "# Get the category names without the 'Category_' prefix\n",
    "user_category_columns = user_category_profiles.columns.str.replace('Category_', '')\n",
    "\n",
    "# Filter columns in user_category_profiles that are in top_categories\n",
    "filtered_columns = user_category_profiles.columns[user_category_columns.isin(top_categories)]\n",
    "\n",
    "# Create filtered_user_category_profiles with these columns\n",
    "filtered_user_category_profiles = user_category_profiles[filtered_columns]\n",
    "\n",
    "# Identify columns that are not in top_categories to sum them into 'Category_Other'\n",
    "other_columns = user_category_profiles.columns[~user_category_columns.isin(top_categories)]\n",
    "\n",
    "# Sum the 'Other' categories\n",
    "filtered_user_category_profiles['Category_Other'] = user_category_profiles[other_columns].sum(axis=1)\n",
    "\n",
    "# Now, get the actual categories present after filtering\n",
    "actual_categories = filtered_columns.str.replace('Category_', '').tolist()\n",
    "\n",
    "# Add 'Other' to the list\n",
    "actual_categories.append('Other')\n",
    "\n",
    "# Assign new column names\n",
    "filtered_user_category_profiles.columns = [f'Category_{cat}' for cat in actual_categories]\n",
    "print(\"\\nFiltered user_category_profiles with Top N Categories and 'Other':\")\n",
    "print(filtered_user_category_profiles.head())\n",
    "print(f\"\\nShape of filtered_user_category_profiles: {filtered_user_category_profiles.shape}\")\n",
    "\n",
    "# Save the user_category_profiles to a file for future use\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "filtered_user_category_profiles.to_pickle(user_category_profiles_path)\n",
    "user_category_profiles = filtered_user_category_profiles\n",
    "print(f\"\\nSaved user_category_profiles to {user_category_profiles_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0340fc-fdb5-4829-9ada-e0ae091be118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns in user_category_profiles: {len(user_category_profiles.columns)}\")\n",
    "print(f\"Number of new column names: {len(actual_categories)}\")\n",
    "# Number of unique users in behaviors_df\n",
    "unique_user_ids = behaviors_df['UserID'].unique()\n",
    "print(f\"Number of unique users in behaviors_df: {len(unique_user_ids)}\")\n",
    "# Number of unique users in behaviors_df\n",
    "unique_user_ids = behaviors_df['UserID'].unique()\n",
    "print(f\"Number of unique users in behaviors_df: {len(unique_user_ids)}\")\n",
    "\n",
    "# Number of users in user_category_profiles\n",
    "user_profile_ids = user_category_profiles.index.unique()\n",
    "print(f\"Number of users in user_category_profiles: {len(user_profile_ids)}\")\n",
    "\n",
    "# Find missing UserIDs\n",
    "missing_user_ids = set(unique_user_ids) - set(user_profile_ids)\n",
    "print(f\"Number of missing UserIDs in user_category_profiles: {len(missing_user_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29cb85-56bd-4062-95ff-3252eb6e920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Imports and Constants] ---\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dot, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- [Cleaning Function] ---\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# --- [Data Preparation Function] ---\n",
    "def prepare_train_df(\n",
    "    data_dir,\n",
    "    news_file,\n",
    "    behaviours_file,\n",
    "    user_category_profiles,\n",
    "    num_clusters=3,\n",
    "    fraction=1,\n",
    "    max_title_length=30,\n",
    "    max_history_length=50\n",
    "):\n",
    "    # Load news data\n",
    "    news_path = os.path.join(data_dir, news_file)\n",
    "    news_df = pd.read_csv(\n",
    "        news_path,\n",
    "        sep='\\t',\n",
    "        names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'],\n",
    "        index_col=False\n",
    "    )\n",
    "    print(\"Loaded news data:\")\n",
    "    print(news_df.head())\n",
    "\n",
    "    # Load behaviors data\n",
    "    behaviors_path = os.path.join(data_dir, behaviours_file)\n",
    "    behaviors_df = pd.read_csv(\n",
    "        behaviors_path,\n",
    "        sep='\\t',\n",
    "        names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "        index_col=False\n",
    "    )\n",
    "    print(\"Loaded behaviors data:\")\n",
    "    print(behaviors_df.head())\n",
    "\n",
    "    # Clean titles and abstracts\n",
    "    news_df['CleanTitle'] = news_df['Title'].apply(clean_text)\n",
    "    news_df['CleanAbstract'] = news_df['Abstract'].apply(clean_text)\n",
    "\n",
    "    # Create a combined text field\n",
    "    news_df['CombinedText'] = news_df['CleanTitle'] + ' ' + news_df['CleanAbstract']\n",
    "\n",
    "    # Initialize and fit tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(news_df['CombinedText'].tolist())\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "    # Save tokenizer for future use\n",
    "    with open('tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    # Encode and pad CombinedText\n",
    "    news_df['EncodedText'] = tokenizer.texts_to_sequences(news_df['CombinedText'])\n",
    "    news_df['PaddedText'] = list(pad_sequences(news_df['EncodedText'], maxlen=max_title_length, padding='post', truncating='post'))\n",
    "\n",
    "    # Create a mapping from NewsID to PaddedText\n",
    "    news_text_dict = dict(zip(news_df['NewsID'], news_df['PaddedText']))\n",
    "\n",
    "    # Function to parse impressions and labels\n",
    "    def parse_impressions(impressions):\n",
    "        impression_list = impressions.split()\n",
    "        news_ids = []\n",
    "        labels = []\n",
    "        for imp in impression_list:\n",
    "            try:\n",
    "                news_id, label = imp.split('-')\n",
    "                news_ids.append(news_id)\n",
    "                labels.append(int(label))\n",
    "            except ValueError:\n",
    "                # Handle cases where split does not result in two items\n",
    "                continue\n",
    "        return news_ids, labels\n",
    "\n",
    "    # Apply parsing to behaviors data\n",
    "    behaviors_df[['ImpressionNewsIDs', 'ImpressionLabels']] = behaviors_df['Impressions'].apply(\n",
    "        lambda x: pd.Series(parse_impressions(x))\n",
    "    )\n",
    "\n",
    "    # Initialize list for train samples\n",
    "    train_samples = []\n",
    "\n",
    "    # Iterate over behaviors to create train samples\n",
    "    for _, row in tqdm(behaviors_df.iterrows(), total=behaviors_df.shape[0]):\n",
    "        user_id = row['UserID']\n",
    "        user_cluster = row['Cluster'] if 'Cluster' in row else None  # Cluster will be assigned later\n",
    "\n",
    "        # Parse user history\n",
    "        history_ids = row['HistoryText'].split() if pd.notna(row['HistoryText']) else []\n",
    "        history_texts = [news_text_dict.get(nid, [0]*max_title_length) for nid in history_ids]\n",
    "\n",
    "        # Limit history length\n",
    "        if len(history_texts) < max_history_length:\n",
    "            padding = [[0]*max_title_length] * (max_history_length - len(history_texts))\n",
    "            history_texts = padding + history_texts\n",
    "        else:\n",
    "            history_texts = history_texts[-max_history_length:]\n",
    "\n",
    "        candidate_news_ids = row['ImpressionNewsIDs']\n",
    "        labels = row['ImpressionLabels']\n",
    "\n",
    "        for candidate_news_id, label in zip(candidate_news_ids, labels):\n",
    "            candidate_text = news_text_dict.get(candidate_news_id, [0]*max_title_length)\n",
    "            train_samples.append({\n",
    "                'UserID': user_id,\n",
    "                'HistoryTitles': history_texts,  # Renamed to 'HistoryTitles'\n",
    "                'CandidateTitleTokens': candidate_text,  # Renamed to match DataGenerator\n",
    "                'Label': label\n",
    "            })\n",
    "\n",
    "    # Create DataFrame from samples\n",
    "    train_df = pd.DataFrame(train_samples)\n",
    "    print(f\"Created train_df with {len(train_df)} samples.\")\n",
    "    print(\"Columns in train_df:\")\n",
    "    print(train_df.columns)\n",
    "    # --- [Clustering Users] ---\n",
    "    # Ensure 'UserID's match between user_category_profiles and behaviors_df\n",
    "    unique_user_ids = behaviors_df['UserID'].unique()\n",
    "    user_category_profiles = user_category_profiles.loc[unique_user_ids]\n",
    "\n",
    "    # Check for any missing 'UserID's\n",
    "    missing_user_ids = set(unique_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"Warning: {len(missing_user_ids)} 'UserID's are missing from user_category_profiles.\")\n",
    "        # Optionally handle missing users\n",
    "        # For this example, we'll remove these users from behaviors_df\n",
    "        behaviors_df = behaviors_df[~behaviors_df['UserID'].isin(missing_user_ids)]\n",
    "    else:\n",
    "        print(\"All 'UserID's are present in user_category_profiles.\")\n",
    "\n",
    "    # Perform clustering on user_category_profiles\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    user_clusters = kmeans.fit_predict(user_category_profiles)\n",
    "    print(f\"Assigned clusters to users. Number of clusters: {num_clusters}\")\n",
    "\n",
    "    # Create a DataFrame for user clusters\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'UserID': user_category_profiles.index,\n",
    "        'Cluster': user_clusters\n",
    "    })\n",
    "\n",
    "    # --- [Assign Clusters Using Map] ---\n",
    "    print(\"Assigning cluster labels to train_df using map...\")\n",
    "    user_cluster_mapping = dict(zip(user_cluster_df['UserID'], user_cluster_df['Cluster']))\n",
    "    train_df['Cluster'] = train_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "    # Verify cluster assignment\n",
    "    missing_clusters = train_df[train_df['Cluster'].isna()]\n",
    "    if not missing_clusters.empty:\n",
    "        print(f\"Warning: {len(missing_clusters)} samples have missing cluster assignments.\")\n",
    "        # Remove samples with missing cluster assignments\n",
    "        train_df = train_df.dropna(subset=['Cluster'])\n",
    "    else:\n",
    "        print(\"All samples have cluster assignments.\")\n",
    "\n",
    "    # Convert 'Cluster' column to integer type\n",
    "    train_df['Cluster'] = train_df['Cluster'].astype(int)\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "    # Assuming train_df has a 'Cluster' column indicating cluster assignments\n",
    "    # Find the minimum size among all clusters\n",
    "    min_cluster_size = train_df['Cluster'].value_counts().min()\n",
    "\n",
    "    # Initialize an empty list to hold balanced data\n",
    "    balanced_data = []\n",
    "\n",
    "    # Iterate over each cluster and sample data to balance\n",
    "    for cluster in train_df['Cluster'].unique():\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        balanced_cluster_data = cluster_data.sample(n=min_cluster_size, random_state=42)\n",
    "        balanced_data.append(balanced_cluster_data)\n",
    "\n",
    "    # Combine balanced data for all clusters\n",
    "    balanced_train_df = pd.concat(balanced_data)\n",
    "\n",
    "    # Update train_df with the balanced data\n",
    "    # --- [Label Balancing for 0/1 Classes] ---\n",
    "    # Count how many 0s and 1s we have\n",
    "    label_counts = balanced_train_df['Label'].value_counts()\n",
    "    min_label_count = label_counts.min()\n",
    "\n",
    "    balanced_labels = []\n",
    "    for label_value in balanced_train_df['Label'].unique():\n",
    "        label_data = balanced_train_df[balanced_train_df['Label'] == label_value]\n",
    "        # Downsample to the min_label_count to balance the label distribution\n",
    "        balanced_label_data = label_data.sample(n=min_label_count, random_state=42)\n",
    "        balanced_labels.append(balanced_label_data)\n",
    "\n",
    "    # Combine the label balanced data\n",
    "    final_balanced_train_df = pd.concat(balanced_labels, ignore_index=True)\n",
    "\n",
    "    # Shuffle the final dataset to mix up the rows\n",
    "    final_balanced_train_df = final_balanced_train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nAfter label balancing (0 vs 1):\")\n",
    "    print(final_balanced_train_df['Label'].value_counts())\n",
    "\n",
    "    # Now final_balanced_train_df is balanced both by cluster and by label\n",
    "    train_df = final_balanced_train_df\n",
    "    #train_df = balanced_train_df.reset_index(drop=True)\n",
    "\n",
    "    # Print summary of the balanced dataset\n",
    "    print(\"Balanced cluster sizes:\")\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        print(f\"Cluster {cluster}: {len(cluster_data)} samples\")\n",
    "    print(\"Balanced dataset:\")\n",
    "    print(train_df['Cluster'].value_counts())\n",
    "    \"\"\"\n",
    "    clustered_data_balanced = {}\n",
    "    min_cluster_size = float('inf')\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "        print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "        min_cluster_size = len(cluster_data) if len(cluster_data) < min_cluster_size else min_cluster_size\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        data = train_df[train_df['Cluster'] == cluster]\n",
    "        if len(data) > min_cluster_size:\n",
    "            clustered_data_balanced[cluster] = data.sample(n=min_cluster_size, random_state=42)\n",
    "        else:\n",
    "            clustered_data_balanced[cluster] = data\n",
    "\n",
    "    print(\"Balanced cluster sizes:\")\n",
    "    for cluster, data in clustered_data_balanced.items():\n",
    "        print(f\"Cluster {cluster}: {len(data)} samples\")\n",
    "    \"\"\"\n",
    "    # --- [Sampling] ---\n",
    "    # Optionally perform random sampling\n",
    "    print(f\"Original size: {len(train_df)}\")\n",
    "    train_df_sampled = train_df.sample(frac=fraction, random_state=42)\n",
    "    print(f\"Sampled size: {len(train_df_sampled)}\")\n",
    "\n",
    "    # Optionally, set train_df to sampled\n",
    "    train_df = train_df_sampled\n",
    "    print(\"Columns in sampled train_df:\")\n",
    "    print(train_df.columns)\n",
    "    print(f\"Cluster:{train_df['Cluster']}\")\n",
    "    # --- [Split Data for Each Cluster] ---\n",
    "    print(\"Splitting data into training and validation sets for each cluster...\")\n",
    "    clustered_data = {}\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_data = train_df[train_df['Cluster'] == cluster]\n",
    "\n",
    "        if cluster_data.empty:\n",
    "            print(f\"No data for Cluster {cluster}. Skipping...\")\n",
    "            continue  # Skip to the next cluster\n",
    "\n",
    "        train_data, val_data = train_test_split(cluster_data, test_size=0.2, random_state=42)\n",
    "        clustered_data[cluster] = {\n",
    "            'train': train_data.reset_index(drop=True),\n",
    "            'val': val_data.reset_index(drop=True)\n",
    "        }\n",
    "        print(f\"Cluster {cluster}: {len(train_data)} training samples, {len(val_data)} validation samples.\")\n",
    "\n",
    "    return clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters\n",
    "\n",
    "# --- [DataGenerator Class] ---\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, max_history_length=50, max_title_length=30):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_history_length = max_history_length\n",
    "        self.max_title_length = max_title_length\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        #print(f\"[DataGenerator] Initialized with {len(self.df)} samples and batch_size={self.batch_size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        length = int(np.ceil(len(self.df) / self.batch_size))\n",
    "        #print(f\"[DataGenerator] Number of batches per epoch: {length}\")\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, len(self.df))\n",
    "        batch_indices = self.indices[start:end]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "\n",
    "        # Debugging: Print batch information\n",
    "        #print(f\"[DataGenerator] Generating batch {idx+1}/{self.__len__()} with samples {start} to {end}\")\n",
    "\n",
    "        if len(batch_df) == 0:\n",
    "            print(f\"[DataGenerator] Warning: Batch {idx} is empty.\")\n",
    "            return None, None\n",
    "\n",
    "        # Initialize batches\n",
    "        history_batch = []\n",
    "        candidate_batch = []\n",
    "        labels_batch = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            # Get tokenized history titles\n",
    "            history_titles = row['HistoryTitles']  # List of lists of integers\n",
    "\n",
    "            # Pad each title in history\n",
    "            history_titles_padded = pad_sequences(\n",
    "                history_titles,\n",
    "                maxlen=self.max_title_length,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0\n",
    "            )\n",
    "\n",
    "            # Pad or truncate the history to MAX_HISTORY_LENGTH\n",
    "            if len(history_titles_padded) < self.max_history_length:\n",
    "                padding = np.zeros((self.max_history_length - len(history_titles_padded), self.max_title_length), dtype='int32')\n",
    "                history_titles_padded = np.vstack([padding, history_titles_padded])\n",
    "            else:\n",
    "                history_titles_padded = history_titles_padded[-self.max_history_length:]\n",
    "\n",
    "            # Get candidate title tokens\n",
    "            candidate_title = row['CandidateTitleTokens']  # List of integers\n",
    "            candidate_title_padded = pad_sequences(\n",
    "                [candidate_title],\n",
    "                maxlen=self.max_title_length,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0\n",
    "            )[0]\n",
    "\n",
    "            # Append to batches\n",
    "            history_batch.append(history_titles_padded)\n",
    "            candidate_batch.append(candidate_title_padded)\n",
    "            labels_batch.append(row['Label'])\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        history_batch = np.array(history_batch, dtype='int32')  # Shape: (batch_size, MAX_HISTORY_LENGTH, MAX_TITLE_LENGTH)\n",
    "        candidate_batch = np.array(candidate_batch, dtype='int32')  # Shape: (batch_size, MAX_TITLE_LENGTH)\n",
    "        labels_batch = np.array(labels_batch, dtype='float32')  # Shape: (batch_size,)\n",
    "        inputs = {\n",
    "            'history_input': history_batch,\n",
    "            'candidate_input': candidate_batch\n",
    "        }\n",
    "\n",
    "        # Debugging: Print shapes\n",
    "        #print(f\"[DataGenerator] Batch shapes - history_input: {history_batch.shape}, candidate_input: {candidate_batch.shape}, labels: {labels_batch.shape}\")\n",
    "        return inputs, labels_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "# --- [Fastformer Model Classes and Functions] ---\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Softmax, Multiply, Embedding, TimeDistributed, LayerNormalization\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class SqueezeLayer(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super(SqueezeLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SqueezeLayer, self).get_config()\n",
    "        config.update({'axis': self.axis})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ExpandDimsLayer(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super(ExpandDimsLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ExpandDimsLayer, self).get_config()\n",
    "        config.update({'axis': self.axis})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class SumPooling(Layer):\n",
    "    def __init__(self, axis=1, **kwargs):\n",
    "        super(SumPooling, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_sum(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SumPooling, self).get_config()\n",
    "        config.update({'axis': self.axis})\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Fastformer(Layer):\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        super(Fastformer, self).__init__(**kwargs)\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head * size_per_head\n",
    "\n",
    "        self.WQ = None\n",
    "        self.WK = None\n",
    "        self.WV = None\n",
    "        self.WO = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = Dense(self.output_dim, use_bias=False, name='WQ')\n",
    "        self.WK = Dense(self.output_dim, use_bias=False, name='WK')\n",
    "        self.WV = Dense(self.output_dim, use_bias=False, name='WV')\n",
    "        self.WO = Dense(self.output_dim, use_bias=False, name='WO')\n",
    "        super(Fastformer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 2:\n",
    "            Q_seq, K_seq = inputs\n",
    "            Q_mask = None\n",
    "            K_mask = None\n",
    "        elif len(inputs) == 4:\n",
    "            Q_seq, K_seq, Q_mask, K_mask = inputs\n",
    "\n",
    "        batch_size = tf.shape(Q_seq)[0]\n",
    "        seq_len = tf.shape(Q_seq)[1]\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.WQ(Q_seq)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        K = self.WK(K_seq)  # Shape: (batch_size, seq_len, output_dim)\n",
    "        V = self.WV(K_seq)  # Shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.nb_head, self.size_per_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.nb_head, self.size_per_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.nb_head, self.size_per_head))\n",
    "\n",
    "        # Compute global query and key\n",
    "        global_q = tf.reduce_mean(Q, axis=1, keepdims=True)  # (batch_size, 1, nb_head, size_per_head)\n",
    "        global_k = tf.reduce_mean(K, axis=1, keepdims=True)  # (batch_size, 1, nb_head, size_per_head)\n",
    "\n",
    "        # Compute attention weights\n",
    "        weights = global_q * K + global_k * Q  # (batch_size, seq_len, nb_head, size_per_head)\n",
    "        weights = tf.reduce_sum(weights, axis=-1)  # (batch_size, seq_len, nb_head)\n",
    "        weights = tf.nn.softmax(weights, axis=1)  # Softmax over seq_len\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        weights = tf.expand_dims(weights, axis=-1)  # (batch_size, seq_len, nb_head, 1)\n",
    "        context = weights * V  # (batch_size, seq_len, nb_head, size_per_head)\n",
    "\n",
    "        # Combine heads\n",
    "        context = tf.reshape(context, (batch_size, seq_len, self.output_dim))\n",
    "\n",
    "        # Final projection\n",
    "        output = self.WO(context)  # (batch_size, seq_len, output_dim)\n",
    "\n",
    "        return output  # Output shape: (batch_size, seq_len, output_dim)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Fastformer, self).get_config()\n",
    "        config.update({\n",
    "            'nb_head': self.nb_head,\n",
    "            'size_per_head': self.size_per_head\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class NewsEncoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, dropout_rate=0.2, nb_head=8, size_per_head=32, **kwargs):\n",
    "        super(NewsEncoder, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "\n",
    "        # Define sub-layers\n",
    "        self.embedding_layer = Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            name='embedding_layer'\n",
    "        )\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        self.dense = Dense(1)\n",
    "        self.softmax = Softmax(axis=1)\n",
    "        self.squeeze = SqueezeLayer(axis=-1)\n",
    "        self.expand_dims = ExpandDimsLayer(axis=-1)\n",
    "        self.sum_pooling = SumPooling(axis=1)\n",
    "\n",
    "        self.fastformer_layer = Fastformer(nb_head=self.nb_head, size_per_head=self.size_per_head, name='fastformer_layer')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(NewsEncoder, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Create mask\n",
    "        mask = tf.cast(tf.not_equal(inputs, 0), dtype='float32')  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Embedding\n",
    "        title_emb = self.embedding_layer(inputs)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        title_emb = self.dropout(title_emb)\n",
    "\n",
    "        # Fastformer\n",
    "        hidden_emb = self.fastformer_layer([title_emb, title_emb, mask, mask])  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        hidden_emb = self.dropout(hidden_emb)\n",
    "\n",
    "        # Attention-based Pooling\n",
    "        attention_scores = self.dense(hidden_emb)  # Shape: (batch_size, seq_len, 1)\n",
    "        attention_scores = self.squeeze(attention_scores)  # Shape: (batch_size, seq_len)\n",
    "        attention_weights = self.softmax(attention_scores)  # Shape: (batch_size, seq_len)\n",
    "        attention_weights = self.expand_dims(attention_weights)  # Shape: (batch_size, seq_len, 1)\n",
    "        multiplied = Multiply()([hidden_emb, attention_weights])  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        news_vector = self.sum_pooling(multiplied)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        return news_vector  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NewsEncoder, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'nb_head': self.nb_head,\n",
    "            'size_per_head': self.size_per_head\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class MaskLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaskLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Create mask: cast to float32 any position that is not equal to zero\n",
    "        mask = tf.cast(tf.not_equal(inputs, 0), dtype='float32')\n",
    "        return mask\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MaskLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class UserEncoder(Layer):\n",
    "    def __init__(self, news_encoder_layer, embedding_dim=256, **kwargs):\n",
    "        super(UserEncoder, self).__init__(**kwargs)\n",
    "        self.news_encoder_layer = news_encoder_layer\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.fastformer = Fastformer(nb_head=8, size_per_head=32, name='user_fastformer')\n",
    "        self.dense = Dense(1)\n",
    "        self.squeeze = SqueezeLayer(axis=-1)\n",
    "        self.softmax = Softmax(axis=1)\n",
    "        self.expand_dims = ExpandDimsLayer(axis=-1)\n",
    "        self.sum_pooling = SumPooling(axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, MAX_HISTORY_LENGTH, MAX_TITLE_LENGTH)\n",
    "        # Encode each news article in the history\n",
    "        news_vectors = TimeDistributed(self.news_encoder_layer)(inputs)  # Shape: (batch_size, MAX_HISTORY_LENGTH, embedding_dim)\n",
    "\n",
    "        # Step 1: Create a boolean mask\n",
    "        mask = tf.not_equal(inputs, 0)  # Shape: (batch_size, MAX_HISTORY_LENGTH, MAX_TITLE_LENGTH), dtype=bool\n",
    "\n",
    "        # Step 2: Reduce along the last axis\n",
    "        mask = tf.reduce_any(mask, axis=-1)  # Shape: (batch_size, MAX_HISTORY_LENGTH), dtype=bool\n",
    "\n",
    "        # Step 3: Cast to float32 if needed\n",
    "        mask = tf.cast(mask, dtype='float32')  # Shape: (batch_size, MAX_HISTORY_LENGTH), dtype=float32\n",
    "\n",
    "        # Fastformer\n",
    "        hidden_emb = self.fastformer([news_vectors, news_vectors, mask, mask])  # Shape: (batch_size, MAX_HISTORY_LENGTH, embedding_dim)\n",
    "        hidden_emb = self.dropout(hidden_emb)\n",
    "        hidden_emb = self.layer_norm(hidden_emb)\n",
    "\n",
    "        # Attention-based Pooling over history\n",
    "        attention_scores = self.dense(hidden_emb)  # Shape: (batch_size, MAX_HISTORY_LENGTH, 1)\n",
    "        attention_scores = self.squeeze(attention_scores)  # Shape: (batch_size, MAX_HISTORY_LENGTH)\n",
    "        attention_weights = self.softmax(attention_scores)  # Shape: (batch_size, MAX_HISTORY_LENGTH)\n",
    "        attention_weights = self.expand_dims(attention_weights)  # Shape: (batch_size, MAX_HISTORY_LENGTH, 1)\n",
    "        multiplied = Multiply()([hidden_emb, attention_weights])  # Shape: (batch_size, MAX_HISTORY_LENGTH, embedding_dim)\n",
    "        user_vector = self.sum_pooling(multiplied)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        return user_vector  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(UserEncoder, self).get_config()\n",
    "        config.update({\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# --- [Build Model Function] ---\n",
    "def build_model(vocab_size, max_title_length=30, max_history_length=50, embedding_dim=256, nb_head=8, size_per_head=32, dropout_rate=0.2):\n",
    "    # Define Inputs\n",
    "    history_input = Input(shape=(max_history_length, max_title_length), dtype='int32', name='history_input')\n",
    "    candidate_input = Input(shape=(max_title_length,), dtype='int32', name='candidate_input')\n",
    "\n",
    "    # Instantiate NewsEncoder Layer\n",
    "    news_encoder_layer = NewsEncoder(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        dropout_rate=dropout_rate,\n",
    "        nb_head=nb_head,\n",
    "        size_per_head=size_per_head,\n",
    "        name='news_encoder'\n",
    "    )\n",
    "\n",
    "    # Encode Candidate News\n",
    "    candidate_vector = news_encoder_layer(candidate_input)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    # Encode User History\n",
    "    user_vector = UserEncoder(news_encoder_layer, embedding_dim=embedding_dim, name='user_encoder')(history_input)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "    # Scoring Function: Dot Product between User and Candidate Vectors\n",
    "    score = Dot(axes=-1)([user_vector, candidate_vector])  # Shape: (batch_size, 1)\n",
    "    score = Activation('sigmoid')(score)  # Shape: (batch_size, 1)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs={'history_input': history_input, 'candidate_input': candidate_input}, outputs=score)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='AUC')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def build_and_load_weights2(weights_file):\n",
    "    model = build_model(\n",
    "        vocab_size=vocab_size,\n",
    "        max_title_length=max_title_length,\n",
    "        max_history_length=max_history_length,\n",
    "        embedding_dim=256,\n",
    "        nb_head=8,\n",
    "        size_per_head=32,\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "\n",
    "    # Manually build the model\n",
    "    input_shapes = {\n",
    "        'history_input': (None, max_history_length, max_title_length),\n",
    "        'candidate_input': (None, max_title_length)\n",
    "    }\n",
    "    # Prepare dummy inputs\n",
    "    import numpy as np\n",
    "\n",
    "    dummy_history_input = np.zeros((1, 50, 30), dtype=np.int32)\n",
    "    dummy_candidate_input = np.zeros((1, 30), dtype=np.int32)\n",
    "\n",
    "    # Build the model by passing dummy data\n",
    "    model.predict({'history_input': dummy_history_input, 'candidate_input': dummy_candidate_input})\n",
    "    #model.build(input_shapes)\n",
    "    model.load_weights(weights_file)\n",
    "    return model\n",
    "# --- [Training Function] ---\n",
    "def train_cluster_models(clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters, batch_size=64, epochs=5):\n",
    "    models = {}\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        m_name = f'fastformer_cluster_{cluster}_full_balanced_1_epoch'\n",
    "        weights_file = f'{m_name}.weights.h5'\n",
    "        model_h5_file = f'{m_name}.h5'\n",
    "        model_file = f'{m_name}'\n",
    "        if cluster in [0,1,2,3]:\n",
    "            print(f\"\\nLoading model for Cluster {cluster} from {weights_file}\")\n",
    "            local_model_path = hf_hub_download(\n",
    "                repo_id=f\"Teemu5/news\",\n",
    "                filename=weights_file,\n",
    "                local_dir=\".\"\n",
    "            )\n",
    "            model = build_and_load_weights(weights_file)\n",
    "            models[cluster] = model\n",
    "            continue\n",
    "        print(f\"\\nTraining model for Cluster {cluster} into {weights_file}\")\n",
    "        # Retrieve training and validation data\n",
    "        train_data = clustered_data[cluster]['train']\n",
    "        val_data = clustered_data[cluster]['val']\n",
    "\n",
    "        print(f\"Cluster {cluster} - Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "\n",
    "        # Create data generators\n",
    "        train_generator = DataGenerator(train_data, batch_size, max_history_length, max_title_length)\n",
    "        val_generator = DataGenerator(val_data, batch_size, max_history_length, max_title_length)\n",
    "\n",
    "        steps_per_epoch = len(train_generator)\n",
    "        validation_steps = len(val_generator)\n",
    "\n",
    "        # Build model\n",
    "        model = build_model(\n",
    "            vocab_size=vocab_size,\n",
    "            max_title_length=max_title_length,\n",
    "            max_history_length=max_history_length,\n",
    "            embedding_dim=256,\n",
    "            nb_head=8,\n",
    "            size_per_head=32,\n",
    "            dropout_rate=0.2\n",
    "        )\n",
    "        print(model.summary())\n",
    "\n",
    "        # Define callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_AUC',\n",
    "            patience=2,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        csv_logger = CSVLogger(f'training_log_cluster_{cluster}.csv', append=True)\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            f'best_model_cluster_{cluster}.keras',\n",
    "            monitor='val_AUC',\n",
    "            mode='max',\n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            train_generator,\n",
    "            epochs=epochs,\n",
    "            #steps_per_epoch=steps_per_epoch,\n",
    "            #validation_data=val_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[early_stopping, csv_logger, model_checkpoint]\n",
    "        )\n",
    "\n",
    "        # Save model weights\n",
    "        model.save_weights(weights_file)\n",
    "        print(f\"Saved model weights for Cluster {cluster} into {weights_file}.\")\n",
    "        model.save(model_h5_file)\n",
    "        print(f\"Saved model for Cluster {cluster} into {model_h5_file}.\")\n",
    "\n",
    "        # Store the model\n",
    "        models[cluster] = model\n",
    "        # Clear memory\n",
    "        del train_data, val_data, train_generator, val_generator, model\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    print(\"Returning models list\")\n",
    "    print(models)\n",
    "    return models\n",
    "\n",
    "# --- [Recommendation Function] ---\n",
    "def recommend_news(user_id, user_cluster_df, models, candidate_texts, history_texts, max_history_length=50, max_title_length=30):\n",
    "    # Determine the user's cluster\n",
    "    cluster = user_cluster_df.get(user_id)\n",
    "    if cluster is None:\n",
    "        print(f\"User {user_id} not found in any cluster.\")\n",
    "        return None\n",
    "\n",
    "    # Retrieve the corresponding model\n",
    "    model = models.get(cluster)\n",
    "    if model is None:\n",
    "        print(f\"No model trained for Cluster {cluster}.\")\n",
    "        return None\n",
    "\n",
    "    # Prepare input data for the user\n",
    "    history_padded = pad_sequences(\n",
    "        [history_texts],\n",
    "        maxlen=max_history_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (1, max_history_length, max_title_length)\n",
    "\n",
    "    candidate_padded = pad_sequences(\n",
    "        [candidate_texts],\n",
    "        maxlen=max_title_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (1, max_title_length)\n",
    "\n",
    "    inputs = {\n",
    "        'history_input': history_padded,\n",
    "        'candidate_input': candidate_padded\n",
    "    }\n",
    "\n",
    "    # Generate prediction\n",
    "    prediction = model.predict(inputs)[0][0]  # Get the prediction score\n",
    "\n",
    "    # Return the prediction score\n",
    "    return prediction\n",
    "# --- [Main Execution] ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to data files\n",
    "    news_file = 'news.tsv'\n",
    "    behaviors_file = 'behaviors.tsv'\n",
    "\n",
    "    # Load behaviors data to get unique UserIDs\n",
    "    behaviors_path = os.path.join(data_dir, behaviors_file)\n",
    "    behaviors_df = pd.read_csv(\n",
    "        behaviors_path,\n",
    "        sep='\\t',\n",
    "        names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    # Extract unique UserIDs\n",
    "    unique_user_ids = behaviors_df['UserID'].unique()\n",
    "    print(f\"Number of unique users in behaviors_df: {len(unique_user_ids)}\")\n",
    "\n",
    "    # Create dummy user_category_profiles with matching UserIDs\n",
    "    #user_category_profiles = pd.DataFrame(\n",
    "    #    np.random.rand(len(unique_user_ids), 10),  # One row per user\n",
    "    #    index=unique_user_ids,\n",
    "    #    columns=[f'feature_{j}' for j in range(10)]\n",
    "    #)\n",
    "\n",
    "    # Prepare clustered data\n",
    "    clustered_data, tokenizer, vocab_size, max_history_length, max_title_length, num_clusters = prepare_train_df(\n",
    "        data_dir=data_dir,\n",
    "        news_file=news_file,\n",
    "        behaviours_file=behaviors_file,\n",
    "        user_category_profiles=user_category_profiles,\n",
    "        num_clusters=3,\n",
    "        fraction=1,\n",
    "        max_title_length=30,\n",
    "        max_history_length=50\n",
    "    )\n",
    "\n",
    "    # Train cluster-specific models\n",
    "    models = train_cluster_models(\n",
    "        clustered_data=clustered_data,\n",
    "        tokenizer=tokenizer,\n",
    "        vocab_size=vocab_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length,\n",
    "        num_clusters=num_clusters,\n",
    "        batch_size=64,\n",
    "        epochs=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9e880-2c16-4dea-a3d6-62fa4dc02940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "user_profiles_scaled = scaler.fit_transform(filtered_user_category_profiles)\n",
    "sum_of_squared_distances = []\n",
    "K = range(1, 10)  # Try more clusters\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(user_profiles_scaled)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101119d7-77b1-4eab-9903-8118ebf2de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Define data directory and file paths\n",
    "data_dir = 'small/valid/'  # Adjust if your files are in a different directory\n",
    "news_file = 'news.tsv'\n",
    "valid_behaviors_file = 'behaviors.tsv'  # Validation behaviors file\n",
    "\n",
    "# Load news data (if not already loaded)\n",
    "news_path = os.path.join(data_dir, news_file)\n",
    "news_df = pd.read_csv(\n",
    "    news_path,\n",
    "    sep='\\t',\n",
    "    names=['NewsID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbstractEntities'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"Loaded news data:\")\n",
    "print(news_df.head())\n",
    "\n",
    "# Load validation behaviors data\n",
    "valid_behaviors_path = os.path.join(data_dir, valid_behaviors_file)\n",
    "valid_behaviors_df = pd.read_csv(\n",
    "    valid_behaviors_path,\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'HistoryText', 'Impressions'],\n",
    "    index_col=False\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded validation behaviors data:\")\n",
    "print(valid_behaviors_df.head())\n",
    "# Check for missing 'HistoryText' entries\n",
    "missing_history = valid_behaviors_df['HistoryText'].isna().sum()\n",
    "print(f\"Number of entries with missing 'HistoryText': {missing_history}\")\n",
    "print(len(valid_behaviors_df['HistoryText']))\n",
    "valid_behaviors_df['HistoryText'] = valid_behaviors_df['HistoryText'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc610f7-bafb-45ba-ba33-81088dcfceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "# Load the user_category_profiles\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "\n",
    "# --- [Perform Clustering] ---\n",
    "\n",
    "# Optionally, standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the user profiles\n",
    "user_profiles_scaled = scaler.fit_transform(user_category_profiles)\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved scaler to {scaler_path}\")\n",
    "\n",
    "# Initialize the clustering model\n",
    "num_clusters = 3  # Adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit the clustering model\n",
    "kmeans.fit(user_profiles_scaled)\n",
    "\n",
    "# Save the clustering model for future use\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "with open(clustering_model_path, 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "print(f\"Saved KMeans clustering model to {clustering_model_path}\")\n",
    "\n",
    "# Assign clusters to users\n",
    "user_clusters = kmeans.predict(user_profiles_scaled)\n",
    "\n",
    "# Add the cluster assignments to the user profiles\n",
    "user_category_profiles['Cluster'] = user_clusters\n",
    "\n",
    "# Save the cluster assignments\n",
    "user_cluster_df = user_category_profiles[['Cluster']]\n",
    "user_cluster_df_path = 'user_cluster_df.pkl'\n",
    "user_cluster_df.to_pickle(user_cluster_df_path)\n",
    "print(f\"Saved user cluster assignments to {user_cluster_df_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8b871-0d46-4cbf-8d82-8fa7c8abbde1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- [DataGenerator Class] ---\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, max_history_length, max_title_length):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_history_length = max_history_length\n",
    "        self.max_title_length = max_title_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if batch.empty:\n",
    "            raise ValueError(f\"Batch index {idx} is empty.\")\n",
    "\n",
    "        # Extract data\n",
    "        history_tokens = batch['HistoryTokens'].tolist()  # List of lists of lists\n",
    "        candidate_titles = batch['CandidateTitleTokens'].tolist()\n",
    "        labels = batch['Label'].astype(np.float32).values  # Ensure labels are float32\n",
    "\n",
    "        # Pad candidate titles\n",
    "        candidate_titles_padded = pad_sequences(\n",
    "            candidate_titles,\n",
    "            maxlen=self.max_title_length,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "        # Initialize list to store padded histories\n",
    "        history_padded_batch = []\n",
    "\n",
    "        for user_history in history_tokens:\n",
    "            # user_history is a list of lists (titles)\n",
    "            if not user_history:\n",
    "                # If user has no history, create a zero matrix\n",
    "                padded_user_history = np.zeros((self.max_history_length, self.max_title_length), dtype=np.int32)\n",
    "            else:\n",
    "                # Pad each title in the history\n",
    "                padded_titles = pad_sequences(\n",
    "                    user_history,\n",
    "                    maxlen=self.max_title_length,\n",
    "                    padding='post',\n",
    "                    truncating='post',\n",
    "                    value=0\n",
    "                )\n",
    "\n",
    "                # Pad/truncate the history to max_history_length\n",
    "                if len(padded_titles) < self.max_history_length:\n",
    "                    pad_length = self.max_history_length - len(padded_titles)\n",
    "                    # Create padding titles\n",
    "                    pad_titles = np.zeros((pad_length, self.max_title_length), dtype=np.int32)\n",
    "                    # Concatenate padded titles with padding titles\n",
    "                    padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "                else:\n",
    "                    # Truncate to max_history_length\n",
    "                    padded_user_history = padded_titles[:self.max_history_length]\n",
    "\n",
    "            history_padded_batch.append(padded_user_history)\n",
    "\n",
    "        # Convert list to numpy array\n",
    "        history_padded_batch = np.array(history_padded_batch, dtype=np.int32)\n",
    "\n",
    "        # Create a dictionary for inputs\n",
    "        inputs = {\n",
    "            'history_input': history_padded_batch,\n",
    "            'candidate_input': candidate_titles_padded\n",
    "        }\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "# --- [Loading and Preparing Data] ---\n",
    "\n",
    "# Load or initialize your tokenizer\n",
    "tokenizer_path = 'tokenizer.pkl'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"\\nLoaded existing tokenizer.\")\n",
    "else:\n",
    "    # If not, initialize and fit a new tokenizer (not recommended if training was already done)\n",
    "    tokenizer = Tokenizer(num_words=63346, oov_token='<OOV>')  # Adjust num_words as per your setup\n",
    "    # Assuming 'news_df' is already loaded\n",
    "    tokenizer.fit_on_texts(news_df['Title'])\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\"\\nInitialized and fitted a new tokenizer.\")\n",
    "\n",
    "# Convert titles to sequences\n",
    "news_df['TitleTokens'] = tokenizer.texts_to_sequences(news_df['Title'])\n",
    "\n",
    "print(\"\\nSample tokenized titles:\")\n",
    "print(news_df[['NewsID', 'Title', 'TitleTokens']].head())\n",
    "\n",
    "# Create a mapping from NewsID to TitleTokens\n",
    "newsid_to_tokens = dict(zip(news_df['NewsID'], news_df['TitleTokens']))\n",
    "\n",
    "# Function to parse impressions and create labeled data\n",
    "def parse_impressions(row):\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    history_titles = row['HistoryText'].split(' ') if isinstance(row['HistoryText'], str) else []\n",
    "    data = []\n",
    "    for impression in impressions:\n",
    "        if '-' not in impression:\n",
    "            continue  # Skip malformed impressions\n",
    "        news_id, label = impression.split('-')\n",
    "        title_tokens = newsid_to_tokens.get(news_id, [])\n",
    "        data.append({\n",
    "            'UserID': row['UserID'],\n",
    "            'HistoryTitles': history_titles,  # List of NewsIDs\n",
    "            'CandidateTitleTokens': title_tokens,\n",
    "            'Label': int(label)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Apply the function to each row in the validation behaviors with a progress bar\n",
    "test_data = []\n",
    "for idx, row in tqdm(valid_behaviors_df.iterrows(), total=valid_behaviors_df.shape[0], desc=\"Processing behaviors\"):\n",
    "    parsed = parse_impressions(row)\n",
    "    test_data.extend(parsed)\n",
    "\n",
    "# Create a DataFrame from the test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"\\nCreated test_df with samples:\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nTotal test samples: {len(test_df)}\")\n",
    "\n",
    "# Function to convert NewsIDs in history to token sequences\n",
    "def convert_history(news_ids):\n",
    "    return [newsid_to_tokens.get(news_id, []) for news_id in news_ids]\n",
    "\n",
    "# Apply the function to convert history\n",
    "test_df['HistoryTokens'] = test_df['HistoryTitles'].apply(convert_history)\n",
    "\n",
    "print(\"\\nSample HistoryTokens:\")\n",
    "print(test_df[['UserID', 'HistoryTitles', 'HistoryTokens']].head())\n",
    "\n",
    "# --- [Loading Clustering Artifacts] ---\n",
    "\n",
    "# Paths to clustering artifacts\n",
    "user_cluster_path = 'user_cluster_df.pkl'\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "\n",
    "# Check if user_cluster_df.pkl exists\n",
    "if os.path.exists(user_cluster_path):\n",
    "    user_cluster_df = pd.read_pickle(user_cluster_path)\n",
    "    print(\"\\nLoaded user_cluster_df.\")\n",
    "else:\n",
    "    print(\"\\nuser_cluster_df.pkl not found. Recreating cluster assignments.\")\n",
    "    # Load the clustering model\n",
    "    if os.path.exists(clustering_model_path):\n",
    "        with open(clustering_model_path, 'rb') as f:\n",
    "            clustering_model = pickle.load(f)\n",
    "        print(\"\\nLoaded clustering model.\")\n",
    "    else:\n",
    "        print(\"Clustering model not found. Please ensure you have saved the clustering model during training.\")\n",
    "        raise FileNotFoundError(\"Clustering model not found.\")\n",
    "\n",
    "    # Load user_category_profiles\n",
    "    if os.path.exists(user_category_profiles_path):\n",
    "        user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "        print(\"\\nLoaded user_category_profiles.\")\n",
    "    else:\n",
    "        print(\"user_category_profiles.pkl not found. Cannot proceed without user profiles.\")\n",
    "        raise FileNotFoundError(\"User profiles not found.\")\n",
    "\n",
    "    # Load scaler\n",
    "    if os.path.exists(scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"\\nLoaded scaler for user profiles.\")\n",
    "    else:\n",
    "        print(\"Scaler not found. Please ensure you have saved the scaler during training.\")\n",
    "        raise FileNotFoundError(\"Scaler not found.\")\n",
    "\n",
    "    # Ensure test users are included in user_category_profiles\n",
    "    test_user_ids = test_df['UserID'].unique()\n",
    "    missing_user_ids = set(test_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"\\nAdding {len(missing_user_ids)} new users to user_category_profiles.\")\n",
    "        new_user_profiles = pd.DataFrame(0, index=missing_user_ids, columns=user_category_profiles.columns)\n",
    "        user_category_profiles = pd.concat([user_category_profiles, new_user_profiles])\n",
    "        print(\"Added new user profiles for test users.\")\n",
    "\n",
    "    # Scale user profiles\n",
    "    user_profiles_for_clustering = user_category_profiles.loc[test_user_ids]\n",
    "    user_profiles_scaled = scaler.transform(user_profiles_for_clustering)\n",
    "\n",
    "    # Assign clusters\n",
    "    test_user_clusters = clustering_model.predict(user_profiles_scaled)\n",
    "\n",
    "    # Create user_cluster_df with 'UserID' as index\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'Cluster': test_user_clusters\n",
    "    }, index=user_profiles_for_clustering.index)\n",
    "\n",
    "    # Save the cluster assignments\n",
    "    user_cluster_df.to_pickle('user_cluster_df.pkl')\n",
    "    print(\"\\nAssigned clusters to test users and saved to 'user_cluster_df.pkl'.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(user_cluster_df.head())\n",
    "\n",
    "# Check columns\n",
    "print(user_cluster_df.columns)\n",
    "\n",
    "# Check if 'UserID' is a column\n",
    "if 'UserID' in user_cluster_df.columns:\n",
    "    print(\"'UserID' is a column.\")\n",
    "else:\n",
    "    print(\"'UserID' is not a column. It might be the index.\")\n",
    "\n",
    "# Map clusters to test_df\n",
    "user_cluster_mapping = user_cluster_df['Cluster']  # Series with UserID as index\n",
    "\n",
    "test_df['Cluster'] = test_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "# Handle users not found in user_cluster_df\n",
    "missing_clusters = test_df['Cluster'].isna().sum()\n",
    "if missing_clusters > 0:\n",
    "    print(f\"\\nNumber of users without cluster assignment: {missing_clusters}\")\n",
    "    test_df['Cluster'] = test_df['Cluster'].fillna(0)  # Assign to Cluster 0 or any default cluster\n",
    "    print(\"Assigned default cluster 0 to users without cluster assignment.\")\n",
    "\n",
    "print(\"\\nAssigned clusters to test_df:\")\n",
    "print(test_df['Cluster'].value_counts())\n",
    "\n",
    "# --- [Creating Test Data per Cluster] ---\n",
    "\n",
    "# Define the number of clusters (ensure it matches your training)\n",
    "num_clusters = 3  # Adjust as needed\n",
    "\n",
    "# Initialize a dictionary to hold test data per cluster\n",
    "cluster_test_data = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = test_df[test_df['Cluster'] == cluster].reset_index(drop=True)\n",
    "    cluster_test_data[cluster] = cluster_data\n",
    "    print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "\n",
    "# Define maximum lengths (should match training)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Define a function to create TensorFlow Dataset with output_signature\n",
    "def add_output_sig(datagen, max_history_length, max_title_length):\n",
    "    output_signature = (\n",
    "        {\n",
    "            'history_input': tf.TensorSpec(shape=(None, max_history_length, max_title_length), dtype=tf.int32),\n",
    "            'candidate_input': tf.TensorSpec(shape=(None, max_title_length), dtype=tf.int32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    # Define a generator function\n",
    "    def generator():\n",
    "        for i in range(len(datagen)):\n",
    "            inputs, labels = datagen[i]\n",
    "            \n",
    "            # Assert non-empty inputs\n",
    "            assert inputs['history_input'].shape[0] > 0, f\"Batch {i} has empty history_input.\"\n",
    "            assert inputs['candidate_input'].shape[0] > 0, f\"Batch {i} has empty candidate_input.\"\n",
    "            assert labels.shape[0] > 0, f\"Batch {i} has empty labels.\"\n",
    "            \n",
    "            yield inputs, labels\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # Optional optimizations\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Initialize test DataGenerators per cluster\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    datagen = DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    )\n",
    "    test_generators[cluster] = add_output_sig(datagen, max_history_length, max_title_length)\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "\n",
    "# --- [Evaluating Models] ---\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster}\")\n",
    "    \n",
    "    model = models.get(cluster)  # Assuming 'models' is a dict with cluster as key\n",
    "    if model is None:\n",
    "        print(f\"No model found for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        results = model.evaluate(\n",
    "            test_generator,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model for Cluster {cluster}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Metrics for Cluster {cluster}:\")\n",
    "    for metric_name, value in zip(model.metrics_names, results):\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# --- [Generating Classification Reports] ---\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nGenerating Classification Report for Cluster {cluster}\")\n",
    "    \n",
    "    model = models.get(cluster)\n",
    "    if model is None:\n",
    "        print(f\"No model found for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Generate predictions\n",
    "        predictions = model.predict(test_generator, verbose=1)\n",
    "        predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Retrieve true labels\n",
    "        true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(true_labels, predicted_labels, digits=4)\n",
    "        print(f\"Classification Report for Cluster {cluster}:\\n{report}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for Cluster {cluster}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for Cluster {cluster}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78a452-c58b-4ea2-ae47-acf531ac3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = {0: 1.0, 1: 1.0, 2: 1.0}\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cluster_validation_metrics = {}\n",
    "\n",
    "def add_output_sig(datagen, max_history_length, max_title_length):\n",
    "    output_signature = (\n",
    "        {\n",
    "            'history_input': tf.TensorSpec(shape=(None, max_history_length, max_title_length), dtype=tf.int32),\n",
    "            'candidate_input': tf.TensorSpec(shape=(None, max_title_length), dtype=tf.int32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    # Define a generator function\n",
    "    def generator():\n",
    "        for i in range(len(datagen)):\n",
    "            inputs, labels = datagen[i]\n",
    "            \n",
    "            # Assert non-empty inputs\n",
    "            assert inputs['history_input'].shape[0] > 0, f\"Batch {i} has empty history_input.\"\n",
    "            assert inputs['candidate_input'].shape[0] > 0, f\"Batch {i} has empty candidate_input.\"\n",
    "            assert labels.shape[0] > 0, f\"Batch {i} has empty labels.\"\n",
    "            \n",
    "            yield inputs, labels\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # Optional optimizations\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    datagen = DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    )\n",
    "    test_generators[cluster] = add_output_sig(datagen, max_history_length, max_title_length)\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster} on Validation Data\")\n",
    "    \n",
    "    model = models.get(cluster)\n",
    "    if model is None:\n",
    "        print(f\"No model found for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    # SHOULD THERE BE VALIDATION DATASET?????????\n",
    "    test_generator = test_generators.get(cluster)  # Ensure you have validation_generators similar to test_generators\n",
    "    if test_generator is None:\n",
    "        print(f\"No validation data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "    # Compute AUC score\n",
    "    auc = roc_auc_score(true_labels, predictions)\n",
    "    cluster_validation_metrics[cluster] = auc\n",
    "    print(f\"Cluster {cluster} Validation AUC: {auc:.4f}\")\n",
    "# Extract AUC scores\n",
    "auc_scores = np.array(list(cluster_validation_metrics.values()))\n",
    "clusters = list(cluster_validation_metrics.keys())\n",
    "\n",
    "# Normalize AUC scores to sum to 1\n",
    "normalized_weights = auc_scores / np.sum(auc_scores)\n",
    "\n",
    "# Map clusters to normalized weights\n",
    "model_weights = dict(zip(clusters, normalized_weights))\n",
    "\n",
    "print(\"\\nModel Weights based on Validation AUC:\")\n",
    "for cluster, weight in model_weights.items():\n",
    "    print(f\"Cluster {cluster}: Weight = {weight:.4f}\")\n",
    "def recommend_news_weighted_averaging(models, model_weights, candidate_texts, history_texts, max_history_length=50, max_title_length=30):\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    # Prepare candidate input\n",
    "    candidate_padded = pad_sequences(\n",
    "        [candidate_texts],\n",
    "        maxlen=max_title_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (1, max_title_length)\n",
    "\n",
    "    # Prepare history input\n",
    "    if len(history_texts) == 0:\n",
    "        # If user has no history, create a zero matrix\n",
    "        padded_user_history = np.zeros((max_history_length, max_title_length), dtype=np.int32)\n",
    "    else:\n",
    "        # Pad each title in the history to max_title_length\n",
    "        padded_titles = pad_sequences(\n",
    "            history_texts,  # Correct usage\n",
    "            maxlen=max_title_length,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0\n",
    "        )  # Shape: (num_titles_in_history, max_title_length)\n",
    "\n",
    "        # Pad or truncate the history to max_history_length\n",
    "        if len(padded_titles) < max_history_length:\n",
    "            pad_length = max_history_length - len(padded_titles)\n",
    "            # Create padding titles\n",
    "            pad_titles = np.zeros((pad_length, max_title_length), dtype=np.int32)\n",
    "            # Concatenate padded titles with padding titles\n",
    "            padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "        else:\n",
    "            # Truncate to max_history_length\n",
    "            padded_user_history = padded_titles[:max_history_length]\n",
    "\n",
    "    # Add batch dimension\n",
    "    history_padded = np.expand_dims(padded_user_history, axis=0)  # Shape: (1, max_history_length, max_title_length)\n",
    "\n",
    "    inputs = {\n",
    "        'history_input': history_padded,\n",
    "        'candidate_input': candidate_padded\n",
    "    }\n",
    "\n",
    "    # Get weighted predictions\n",
    "    weighted_sum = 0\n",
    "    total_weight = sum(model_weights.values())\n",
    "\n",
    "    for cluster, model in models.items():\n",
    "        prediction = model.predict(inputs)[0][0]\n",
    "        weight = model_weights.get(cluster, 1)\n",
    "        weighted_sum += prediction * weight\n",
    "\n",
    "    final_prediction = weighted_sum / total_weight\n",
    "\n",
    "    return final_prediction\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set your desired batch size\n",
    "batch_size = 1024  # Adjust as needed\n",
    "\n",
    "# Number of samples and batches\n",
    "num_samples = len(test_df)\n",
    "num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "ensemble_true_labels = []\n",
    "ensemble_predictions = []\n",
    "\n",
    "print(\"\\nEvaluating Weighted Averaging Ensemble Model in Batches\")\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing test batches\"):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "    batch_df = test_df.iloc[start_idx:end_idx]\n",
    "\n",
    "    candidate_texts_batch = batch_df['CandidateTitleTokens'].tolist()\n",
    "    history_texts_batch = batch_df['HistoryTokens'].tolist()\n",
    "    labels_batch = batch_df['Label'].tolist()\n",
    "\n",
    "    # Prepare candidate inputs\n",
    "    candidate_padded_batch = pad_sequences(\n",
    "        candidate_texts_batch,\n",
    "        maxlen=max_title_length,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        value=0\n",
    "    )  # Shape: (batch_size, max_title_length)\n",
    "\n",
    "    # Prepare history inputs\n",
    "    history_padded_batch = []\n",
    "    for history_texts in history_texts_batch:\n",
    "        if len(history_texts) == 0:\n",
    "            # If user has no history, create a zero matrix\n",
    "            padded_user_history = np.zeros((max_history_length, max_title_length), dtype=np.int32)\n",
    "        else:\n",
    "            # Pad each title in the history to max_title_length\n",
    "            padded_titles = pad_sequences(\n",
    "                history_texts,\n",
    "                maxlen=max_title_length,\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0\n",
    "            )  # Shape: (num_titles_in_history, max_title_length)\n",
    "\n",
    "            # Pad or truncate the history to max_history_length\n",
    "            if len(padded_titles) < max_history_length:\n",
    "                pad_length = max_history_length - len(padded_titles)\n",
    "                # Create padding titles\n",
    "                pad_titles = np.zeros((pad_length, max_title_length), dtype=np.int32)\n",
    "                # Concatenate padded titles with padding titles\n",
    "                padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "            else:\n",
    "                # Truncate to max_history_length\n",
    "                padded_user_history = padded_titles[:max_history_length]\n",
    "\n",
    "        history_padded_batch.append(padded_user_history)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    history_padded_batch = np.array(history_padded_batch)  # Shape: (batch_size, max_history_length, max_title_length)\n",
    "\n",
    "    inputs = {\n",
    "        'history_input': history_padded_batch,\n",
    "        'candidate_input': candidate_padded_batch\n",
    "    }\n",
    "\n",
    "    # Get weighted predictions\n",
    "    total_weight = sum(model_weights.values())\n",
    "    weighted_sum = np.zeros(len(batch_df))\n",
    "\n",
    "    for cluster, model in models.items():\n",
    "        # Suppress progress bar by setting verbose=0\n",
    "        predictions = model.predict(inputs, verbose=0).flatten()\n",
    "        weight = model_weights.get(cluster, 1)\n",
    "        weighted_sum += predictions * weight\n",
    "\n",
    "    final_predictions = weighted_sum / total_weight\n",
    "\n",
    "    ensemble_true_labels.extend(labels_batch)\n",
    "    ensemble_predictions.extend(final_predictions)\n",
    "\n",
    "# After processing all batches, proceed with evaluation metrics as before\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "ensemble_true_labels = np.array(ensemble_true_labels)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_predicted_labels = (ensemble_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "auc = roc_auc_score(ensemble_true_labels, ensemble_predictions)\n",
    "accuracy = accuracy_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "precision = precision_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "recall = recall_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "f1 = f1_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "\n",
    "print(\"\\nWeighted Averaging Ensemble Model Performance:\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(ensemble_true_labels, ensemble_predicted_labels, digits=4)\n",
    "print(\"\\nClassification Report for Weighted Averaging Ensemble Model:\\n\", report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(ensemble_true_labels, ensemble_predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Weighted Averaging Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Example: Collect individual model metrics\n",
    "cluster_metrics = []\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    # Retrieve true labels and predictions for each cluster\n",
    "    true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "    predictions = models[cluster].predict(test_generators[cluster]).flatten()\n",
    "    predicted_labels = (predictions >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    auc_cluster = roc_auc_score(true_labels, predictions)\n",
    "    accuracy_cluster = accuracy_score(true_labels, predicted_labels)\n",
    "    precision_cluster = precision_score(true_labels, predicted_labels)\n",
    "    recall_cluster = recall_score(true_labels, predicted_labels)\n",
    "    f1_cluster = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    cluster_metrics.append({\n",
    "        'Model': f'Cluster {cluster} Model',\n",
    "        'AUC': auc_cluster,\n",
    "        'Accuracy': accuracy_cluster,\n",
    "        'Precision': precision_cluster,\n",
    "        'Recall': recall_cluster,\n",
    "        'F1 Score': f1_cluster\n",
    "    })\n",
    "\n",
    "# Add ensemble model metrics\n",
    "cluster_metrics.append({\n",
    "    'Model': 'Weighted Averaging Ensemble',\n",
    "    'AUC': auc,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "performance_df = pd.DataFrame(cluster_metrics)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(performance_df)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot AUC scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='AUC', data=performance_df)\n",
    "plt.title('AUC Score Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=performance_df)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7f922-05b9-476d-93fc-1539d349c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute metrics\n",
    "auc = roc_auc_score(ensemble_true_labels, ensemble_predictions)\n",
    "accuracy = accuracy_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "precision = precision_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "recall = recall_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "f1 = f1_score(ensemble_true_labels, ensemble_predicted_labels)\n",
    "\n",
    "print(\"\\nWeighted Averaging Ensemble Model Performance:\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(ensemble_true_labels, ensemble_predicted_labels, digits=4)\n",
    "print(\"\\nClassification Report for Weighted Averaging Ensemble Model:\\n\", report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(ensemble_true_labels, ensemble_predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Weighted Averaging Ensemble Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Example: Collect individual model metrics\n",
    "cluster_metrics = []\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    # Retrieve true labels and predictions for each cluster\n",
    "    true_labels = cluster_test_data[cluster]['Label'].astype(np.float32).values\n",
    "    predictions = models[cluster].predict(test_generators[cluster]).flatten()\n",
    "    predicted_labels = (predictions >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    auc_cluster = roc_auc_score(true_labels, predictions)\n",
    "    accuracy_cluster = accuracy_score(true_labels, predicted_labels)\n",
    "    precision_cluster = precision_score(true_labels, predicted_labels)\n",
    "    recall_cluster = recall_score(true_labels, predicted_labels)\n",
    "    f1_cluster = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    cluster_metrics.append({\n",
    "        'Model': f'Cluster {cluster} Model',\n",
    "        'AUC': auc_cluster,\n",
    "        'Accuracy': accuracy_cluster,\n",
    "        'Precision': precision_cluster,\n",
    "        'Recall': recall_cluster,\n",
    "        'F1 Score': f1_cluster\n",
    "    })\n",
    "\n",
    "# Add ensemble model metrics\n",
    "cluster_metrics.append({\n",
    "    'Model': 'Weighted Averaging Ensemble',\n",
    "    'AUC': auc,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "performance_df = pd.DataFrame(cluster_metrics)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(performance_df)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot AUC scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='AUC', data=performance_df)\n",
    "plt.title('AUC Score Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=performance_df)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069ef22-89f4-4b97-840c-611d0cb8da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    model = models[cluster]\n",
    "    # Assuming 'model' is your Keras model\n",
    "    #plot_model(model, to_file=f\"model_architecture_{cluster}.png\", show_shapes=True, show_layer_names=True)\n",
    "    SVG(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6972612-4cee-4fa2-9e77-ca3fa78d2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, max_history_length, max_title_length):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_history_length = max_history_length\n",
    "        self.max_title_length = max_title_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # Extract data\n",
    "        history_tokens = batch['HistoryTokens'].tolist()  # List of lists of lists\n",
    "        candidate_titles = batch['CandidateTitleTokens'].tolist()\n",
    "        labels = batch['Label'].astype(np.float32).values  # Ensure labels are float32\n",
    "\n",
    "        # Pad candidate titles\n",
    "        candidate_titles_padded = pad_sequences(\n",
    "            candidate_titles,\n",
    "            maxlen=self.max_title_length,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0\n",
    "        )\n",
    "\n",
    "        # Initialize list to store padded histories\n",
    "        history_padded_batch = []\n",
    "\n",
    "        for user_history in history_tokens:\n",
    "            # user_history is a list of lists (titles)\n",
    "            if not user_history:\n",
    "                # If user has no history, create a zero matrix\n",
    "                padded_user_history = np.zeros((self.max_history_length, self.max_title_length), dtype=np.int32)\n",
    "            else:\n",
    "                # Pad each title in the history\n",
    "                padded_titles = pad_sequences(\n",
    "                    user_history,\n",
    "                    maxlen=self.max_title_length,\n",
    "                    padding='post',\n",
    "                    truncating='post',\n",
    "                    value=0\n",
    "                )\n",
    "\n",
    "                # Pad/truncate the history to max_history_length\n",
    "                if len(padded_titles) < self.max_history_length:\n",
    "                    pad_length = self.max_history_length - len(padded_titles)\n",
    "                    # Create padding titles\n",
    "                    pad_titles = np.zeros((pad_length, self.max_title_length), dtype=np.int32)\n",
    "                    # Concatenate padded titles with padding titles\n",
    "                    padded_user_history = np.vstack([padded_titles, pad_titles])\n",
    "                else:\n",
    "                    # Truncate to max_history_length\n",
    "                    padded_user_history = padded_titles[:self.max_history_length]\n",
    "\n",
    "            history_padded_batch.append(padded_user_history)\n",
    "\n",
    "        # Convert list to numpy array\n",
    "        history_padded_batch = np.array(history_padded_batch, dtype=np.int32)\n",
    "\n",
    "        # Create a dictionary for inputs\n",
    "        inputs = {\n",
    "            'history_input': history_padded_batch,\n",
    "            'candidate_input': candidate_titles_padded\n",
    "        }\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "# --- [Loading and Preparing Data] ---\n",
    "\n",
    "# Load or initialize your tokenizer\n",
    "tokenizer_path = 'tokenizer.pkl'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"\\nLoaded existing tokenizer.\")\n",
    "else:\n",
    "    # If not, initialize and fit a new tokenizer (not recommended if training was already done)\n",
    "    tokenizer = Tokenizer(num_words=63346, oov_token='<OOV>')  # Adjust num_words as per your setup\n",
    "    tokenizer.fit_on_texts(news_df['Title'])\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\"\\nInitialized and fitted a new tokenizer.\")\n",
    "\n",
    "# Convert titles to sequences\n",
    "news_df['TitleTokens'] = tokenizer.texts_to_sequences(news_df['Title'])\n",
    "\n",
    "print(\"\\nSample tokenized titles:\")\n",
    "print(news_df[['NewsID', 'Title', 'TitleTokens']].head())\n",
    "\n",
    "# Create a mapping from NewsID to TitleTokens\n",
    "newsid_to_tokens = dict(zip(news_df['NewsID'], news_df['TitleTokens']))\n",
    "\n",
    "# Function to parse impressions and create labeled data\n",
    "def parse_impressions(row):\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    history_titles = row['HistoryText'].split(' ') if isinstance(row['HistoryText'], str) else []\n",
    "    data = []\n",
    "    for impression in impressions:\n",
    "        if '-' not in impression:\n",
    "            continue  # Skip malformed impressions\n",
    "        news_id, label = impression.split('-')\n",
    "        title_tokens = newsid_to_tokens.get(news_id, [])\n",
    "        data.append({\n",
    "            'UserID': row['UserID'],\n",
    "            'HistoryTitles': history_titles,  # List of NewsIDs\n",
    "            'CandidateTitleTokens': title_tokens,\n",
    "            'Label': int(label)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Apply the function to each row in the validation behaviors with a progress bar\n",
    "test_data = []\n",
    "for idx, row in tqdm(valid_behaviors_df.iterrows(), total=valid_behaviors_df.shape[0], desc=\"Processing behaviors\"):\n",
    "    parsed = parse_impressions(row)\n",
    "    test_data.extend(parsed)\n",
    "\n",
    "# Create a DataFrame from the test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"\\nCreated test_df with samples:\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nTotal test samples: {len(test_df)}\")\n",
    "\n",
    "# Function to convert NewsIDs in history to token sequences\n",
    "def convert_history(news_ids):\n",
    "    return [newsid_to_tokens.get(news_id, []) for news_id in news_ids]\n",
    "\n",
    "# Apply the function to convert history\n",
    "test_df['HistoryTokens'] = test_df['HistoryTitles'].apply(convert_history)\n",
    "\n",
    "print(\"\\nSample HistoryTokens:\")\n",
    "print(test_df[['UserID', 'HistoryTitles', 'HistoryTokens']].head())\n",
    "\n",
    "# --- [Loading Clustering Artifacts] ---\n",
    "\n",
    "# Load user_cluster_df or recreate it\n",
    "user_cluster_path = 'user_cluster_df.pkl'\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "\n",
    "# Check if user_cluster_df.pkl exists\n",
    "if os.path.exists(user_cluster_path):\n",
    "    user_cluster_df = pd.read_pickle(user_cluster_path)\n",
    "    print(\"\\nLoaded user_cluster_df.\")\n",
    "else:\n",
    "    print(\"\\nuser_cluster_df.pkl not found. Recreating cluster assignments.\")\n",
    "    \n",
    "    # Load the clustering model\n",
    "    if os.path.exists(clustering_model_path):\n",
    "        with open(clustering_model_path, 'rb') as f:\n",
    "            clustering_model = pickle.load(f)\n",
    "        print(\"\\nLoaded clustering model.\")\n",
    "    else:\n",
    "        print(\"Clustering model not found. Please ensure you have saved the clustering model during training.\")\n",
    "        # Handle the error or exit\n",
    "        raise FileNotFoundError(\"Clustering model not found.\")\n",
    "    \n",
    "    # Load user_category_profiles\n",
    "    if os.path.exists(user_category_profiles_path):\n",
    "        user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "        print(\"\\nLoaded user_category_profiles.\")\n",
    "    else:\n",
    "        print(\"user_category_profiles.pkl not found. Cannot proceed without user profiles.\")\n",
    "        raise FileNotFoundError(\"User profiles not found.\")\n",
    "    \n",
    "    # Ensure test users are included in user_category_profiles\n",
    "    test_user_ids = test_df['UserID'].unique()\n",
    "    missing_user_ids = set(test_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"\\nAdding {len(missing_user_ids)} new users to user_category_profiles.\")\n",
    "        new_user_profiles = pd.DataFrame(0, index=missing_user_ids, columns=user_category_profiles.columns)\n",
    "        user_category_profiles = user_category_profiles.append(new_user_profiles)\n",
    "    \n",
    "    # Load scaler\n",
    "    if os.path.exists(scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"\\nLoaded scaler for user profiles.\")\n",
    "    else:\n",
    "        print(\"Scaler not found. Please ensure you have saved the scaler during training.\")\n",
    "        raise FileNotFoundError(\"Scaler not found.\")\n",
    "    \n",
    "    # Scale user profiles\n",
    "    user_profiles_for_clustering = user_category_profiles.loc[test_user_ids]\n",
    "    user_profiles_scaled = scaler.transform(user_profiles_for_clustering)\n",
    "    \n",
    "    # Assign clusters\n",
    "    test_user_clusters = clustering_model.predict(user_profiles_scaled)\n",
    "    \n",
    "    # Create user_cluster_df\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'UserID': user_profiles_for_clustering.index,\n",
    "        'Cluster': test_user_clusters\n",
    "    })\n",
    "    user_cluster_df.to_pickle('user_cluster_df.pkl')\n",
    "    print(\"\\nAssigned clusters to test users and saved to 'user_cluster_df.pkl'.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(user_cluster_df.head())\n",
    "\n",
    "# Check columns\n",
    "print(user_cluster_df.columns)\n",
    "\n",
    "# Check if 'UserID' is a column\n",
    "if 'UserID' in user_cluster_df.columns:\n",
    "    print(\"'UserID' is a column.\")\n",
    "else:\n",
    "    print(\"'UserID' is not a column. It might be the index.\")\n",
    "user_cluster_mapping = user_cluster_df['Cluster']\n",
    "\n",
    "# Perform the mapping\n",
    "test_df['Cluster'] = test_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "# Handle users not found in user_cluster_df\n",
    "missing_clusters = test_df['Cluster'].isna().sum()\n",
    "if missing_clusters > 0:\n",
    "    print(f\"\\nNumber of users without cluster assignment: {missing_clusters}\")\n",
    "    test_df['Cluster'].fillna(0, inplace=True)  # Assign to Cluster 0 or any default cluster\n",
    "    print(\"Assigned default cluster 0 to users without cluster assignment.\")\n",
    "\n",
    "print(\"\\nAssigned clusters to test_df:\")\n",
    "print(test_df['Cluster'].value_counts())\n",
    "\n",
    "# --- [Creating Test Data per Cluster] ---\n",
    "\n",
    "# Initialize a dictionary to hold test data per cluster\n",
    "cluster_test_data = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = test_df[test_df['Cluster'] == cluster].reset_index(drop=True)\n",
    "    cluster_test_data[cluster] = cluster_data\n",
    "    print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "\n",
    "# Define maximum lengths (should match training)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "batch_size = 64  # Adjust as needed\n",
    "def add_output_sig(datagen):\n",
    "    output_signature = (\n",
    "        {\n",
    "            'history_input': tf.TensorSpec(shape=(None, max_history_length, max_title_length), dtype=tf.int32),\n",
    "            'candidate_input': tf.TensorSpec(shape=(None, max_title_length), dtype=tf.int32),\n",
    "        },\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: datagen,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    dataset = dataset.repeat(1)\n",
    "\n",
    "    # Optional caching and prefetching\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "# Initialize test DataGenerators per cluster\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    test_generators[cluster] = add_output_sig(DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    ))\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "\n",
    "# --- [Evaluating Models] ---\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = model.evaluate(\n",
    "        test_generator,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Metrics for Cluster {cluster}:\")\n",
    "    for metric_name, value in zip(model.metrics_names, results):\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# --- [Generating Classification Reports] ---\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(models)\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nGenerating Classification Report for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(test_generator, verbose=1)\n",
    "    predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Retrieve true labels\n",
    "    true_labels = cluster_test_data[cluster]['Label'].values\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(true_labels, predicted_labels, digits=4)\n",
    "    print(f\"Classification Report for Cluster {cluster}:\\n{report}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for Cluster {cluster}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792ea8c-f030-47cc-af4c-479eeb4fe53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- [Loading and Preparing Data] ---\n",
    "\n",
    "# Load or initialize your tokenizer\n",
    "tokenizer_path = 'tokenizer.pkl'\n",
    "if os.path.exists(tokenizer_path):\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    print(\"\\nLoaded existing tokenizer.\")\n",
    "else:\n",
    "    # If not, initialize and fit a new tokenizer (not recommended if training was already done)\n",
    "    tokenizer = Tokenizer(num_words=63346, oov_token='<OOV>')  # Adjust num_words as per your setup\n",
    "    tokenizer.fit_on_texts(news_df['Title'])\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(\"\\nInitialized and fitted a new tokenizer.\")\n",
    "\n",
    "# Convert titles to sequences\n",
    "news_df['TitleTokens'] = tokenizer.texts_to_sequences(news_df['Title'])\n",
    "\n",
    "print(\"\\nSample tokenized titles:\")\n",
    "print(news_df[['NewsID', 'Title', 'TitleTokens']].head())\n",
    "\n",
    "# Create a mapping from NewsID to TitleTokens\n",
    "newsid_to_tokens = dict(zip(news_df['NewsID'], news_df['TitleTokens']))\n",
    "\n",
    "# Function to parse impressions and create labeled data\n",
    "def parse_impressions(row):\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    history_titles = row['HistoryText'].split(' ') if isinstance(row['HistoryText'], str) else []\n",
    "    data = []\n",
    "    for impression in impressions:\n",
    "        if '-' not in impression:\n",
    "            continue  # Skip malformed impressions\n",
    "        news_id, label = impression.split('-')\n",
    "        title_tokens = newsid_to_tokens.get(news_id, [])\n",
    "        data.append({\n",
    "            'UserID': row['UserID'],\n",
    "            'HistoryTitles': history_titles,  # List of NewsIDs\n",
    "            'CandidateTitleTokens': title_tokens,\n",
    "            'Label': int(label)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Apply the function to each row in the validation behaviors with a progress bar\n",
    "test_data = []\n",
    "for idx, row in tqdm(valid_behaviors_df.iterrows(), total=valid_behaviors_df.shape[0], desc=\"Processing behaviors\"):\n",
    "    parsed = parse_impressions(row)\n",
    "    test_data.extend(parsed)\n",
    "\n",
    "# Create a DataFrame from the test data\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"\\nCreated test_df with samples:\")\n",
    "print(test_df.head())\n",
    "print(f\"\\nTotal test samples: {len(test_df)}\")\n",
    "\n",
    "# Function to convert NewsIDs in history to token sequences\n",
    "def convert_history(news_ids):\n",
    "    return [newsid_to_tokens.get(news_id, []) for news_id in news_ids]\n",
    "\n",
    "# Apply the function to convert history\n",
    "test_df['HistoryTokens'] = test_df['HistoryTitles'].apply(convert_history)\n",
    "\n",
    "print(\"\\nSample HistoryTokens:\")\n",
    "print(test_df[['UserID', 'HistoryTitles', 'HistoryTokens']].head())\n",
    "\n",
    "# --- [Loading Clustering Artifacts] ---\n",
    "\n",
    "# Paths to clustering artifacts\n",
    "user_cluster_path = 'user_cluster_df.pkl'\n",
    "clustering_model_path = 'kmeans_user_clusters.pkl'\n",
    "user_category_profiles_path = 'user_category_profiles.pkl'\n",
    "scaler_path = 'user_profiles_scaler.pkl'\n",
    "\n",
    "# Check if user_cluster_df.pkl exists\n",
    "if os.path.exists(user_cluster_path):\n",
    "    user_cluster_df = pd.read_pickle(user_cluster_path)\n",
    "    print(\"\\nLoaded user_cluster_df.\")\n",
    "else:\n",
    "    print(\"\\nuser_cluster_df.pkl not found. Recreating cluster assignments.\")\n",
    "    # Load the clustering model\n",
    "    if os.path.exists(clustering_model_path):\n",
    "        with open(clustering_model_path, 'rb') as f:\n",
    "            clustering_model = pickle.load(f)\n",
    "        print(\"\\nLoaded clustering model.\")\n",
    "    else:\n",
    "        print(\"Clustering model not found. Please ensure you have saved the clustering model during training.\")\n",
    "        raise FileNotFoundError(\"Clustering model not found.\")\n",
    "    \n",
    "    # Load user_category_profiles\n",
    "    if os.path.exists(user_category_profiles_path):\n",
    "        user_category_profiles = pd.read_pickle(user_category_profiles_path)\n",
    "        print(\"\\nLoaded user_category_profiles.\")\n",
    "    else:\n",
    "        print(\"user_category_profiles.pkl not found. Cannot proceed without user profiles.\")\n",
    "        raise FileNotFoundError(\"User profiles not found.\")\n",
    "    \n",
    "    # Load scaler\n",
    "    if os.path.exists(scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        print(\"\\nLoaded scaler for user profiles.\")\n",
    "    else:\n",
    "        print(\"Scaler not found. Please ensure you have saved the scaler during training.\")\n",
    "        raise FileNotFoundError(\"Scaler not found.\")\n",
    "    \n",
    "    # Ensure test users are included in user_category_profiles\n",
    "    test_user_ids = test_df['UserID'].unique()\n",
    "    missing_user_ids = set(test_user_ids) - set(user_category_profiles.index)\n",
    "    if missing_user_ids:\n",
    "        print(f\"\\nAdding {len(missing_user_ids)} new users to user_category_profiles.\")\n",
    "        new_user_profiles = pd.DataFrame(0, index=missing_user_ids, columns=user_category_profiles.columns)\n",
    "        user_category_profiles = pd.concat([user_category_profiles, new_user_profiles])\n",
    "        print(\"Added new user profiles for test users.\")\n",
    "    \n",
    "    # Scale user profiles\n",
    "    user_profiles_for_clustering = user_category_profiles.loc[test_user_ids]\n",
    "    user_profiles_scaled = scaler.transform(user_profiles_for_clustering)\n",
    "    \n",
    "    # Assign clusters\n",
    "    test_user_clusters = clustering_model.predict(user_profiles_scaled)\n",
    "    \n",
    "    # Create user_cluster_df with 'UserID' as index\n",
    "    user_cluster_df = pd.DataFrame({\n",
    "        'Cluster': test_user_clusters\n",
    "    }, index=user_profiles_for_clustering.index)\n",
    "    \n",
    "    # Save the cluster assignments\n",
    "    user_cluster_df.to_pickle('user_cluster_df.pkl')\n",
    "    print(\"\\nAssigned clusters to test users and saved to 'user_cluster_df.pkl'.\")\n",
    "\n",
    "# Map clusters to test_df\n",
    "# Since 'UserID' is the index, access 'Cluster' directly\n",
    "user_cluster_mapping = user_cluster_df['Cluster']  # Series with UserID as index\n",
    "\n",
    "# Perform the mapping\n",
    "test_df['Cluster'] = test_df['UserID'].map(user_cluster_mapping)\n",
    "\n",
    "# Handle users not found in user_cluster_df\n",
    "missing_clusters = test_df['Cluster'].isna().sum()\n",
    "if missing_clusters > 0:\n",
    "    print(f\"\\nNumber of users without cluster assignment: {missing_clusters}\")\n",
    "    # Replace the following line to avoid FutureWarning\n",
    "    test_df['Cluster'] = test_df['Cluster'].fillna(0)  # Assign to Cluster 0 or any default cluster\n",
    "    print(\"Assigned default cluster 0 to users without cluster assignment.\")\n",
    "\n",
    "print(\"\\nAssigned clusters to test_df:\")\n",
    "print(test_df['Cluster'].value_counts())\n",
    "\n",
    "# --- [Creating Test Data per Cluster] ---\n",
    "\n",
    "# Define the number of clusters (ensure it matches your training)\n",
    "num_clusters = 3  # Adjust as needed\n",
    "\n",
    "# Initialize a dictionary to hold test data per cluster\n",
    "cluster_test_data = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = test_df[test_df['Cluster'] == cluster].reset_index(drop=True)\n",
    "    cluster_test_data[cluster] = cluster_data\n",
    "    print(f\"Cluster {cluster}: {len(cluster_data)} test samples.\")\n",
    "\n",
    "# Define maximum lengths (should match training)\n",
    "max_history_length = 50\n",
    "max_title_length = 30\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Define your DataGenerator class (ensure it's correctly implemented)\n",
    "# Example:\n",
    "# class DataGenerator(Sequence):\n",
    "#     ...\n",
    "\n",
    "# Initialize test DataGenerators per cluster\n",
    "test_generators = {}\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_data = cluster_test_data[cluster]\n",
    "    if cluster_data.empty:\n",
    "        print(f\"Cluster {cluster} has no test data. Skipping...\")\n",
    "        continue\n",
    "    test_generators[cluster] = add_output_sig(DataGenerator(\n",
    "        df=cluster_data,\n",
    "        batch_size=batch_size,\n",
    "        max_history_length=max_history_length,\n",
    "        max_title_length=max_title_length\n",
    "    ))\n",
    "    print(f\"Created test_generator for Cluster {cluster}.\")\n",
    "\n",
    "# --- [Evaluating Models] ---\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nEvaluating Model for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        results = model.evaluate(\n",
    "            test_generator,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model for Cluster {cluster}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test Metrics for Cluster {cluster}:\")\n",
    "    for metric_name, value in zip(model.metrics_names, results):\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# --- [Generating Classification Reports] ---\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nGenerating Classification Report for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        print(f\"No test data available for Cluster {cluster}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Generate predictions\n",
    "        predictions = model.predict(test_generator, verbose=1)\n",
    "        predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Retrieve true labels\n",
    "        true_labels = cluster_test_data[cluster]['Label'].values\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(true_labels, predicted_labels, digits=4)\n",
    "        print(f\"Classification Report for Cluster {cluster}:\\n{report}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for Cluster {cluster}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report for Cluster {cluster}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6cc0c-7d92-49cd-aa02-3ed21961a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have access to training and validation logs\n",
    "for cluster in range(num_clusters):\n",
    "    # Load training and validation logs\n",
    "    train_log_path = f'training_log_cluster_{cluster}.csv'\n",
    "    try:\n",
    "        train_log_df = pd.read_csv(train_log_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Training log for Cluster {cluster} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Get the last epoch metrics\n",
    "    train_auc = train_log_df['AUC'].iloc[-1]\n",
    "    val_auc = train_log_df['val_AUC'].iloc[-1]\n",
    "    \n",
    "    # Retrieve test AUC\n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        test_auc = None\n",
    "    else:\n",
    "        results = model.evaluate(test_generator, verbose=0)\n",
    "        test_auc = results[model.metrics_names.index('AUC')]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster} Metrics:\")\n",
    "    print(f\"  Training AUC: {train_auc:.4f}\")\n",
    "    print(f\"  Validation AUC: {val_auc:.4f}\")\n",
    "    if test_auc is not None:\n",
    "        print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    else:\n",
    "        print(\"  Test AUC: Not Available\")\n",
    "import seaborn as sns\n",
    "\n",
    "for cluster, model in models.items():\n",
    "    print(f\"\\nVisualizing Prediction Distribution for Cluster {cluster}\")\n",
    "    \n",
    "    test_generator = test_generators.get(cluster)\n",
    "    if test_generator is None:\n",
    "        continue\n",
    "    \n",
    "    predictions = model.predict(test_generator, verbose=1).flatten()\n",
    "    true_labels = test_test_data[cluster]['Label'].values\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(predictions[true_labels == 1], shade=True, label='Positive Label')\n",
    "    sns.kdeplot(predictions[true_labels == 0], shade=True, label='Negative Label')\n",
    "    plt.title(f'Prediction Score Distribution for Cluster {cluster}')\n",
    "    plt.xlabel('Predicted Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# Example: Modify your model to output attention weights (if not already done)\n",
    "# Assuming your model is built to return attention weights\n",
    "\n",
    "# Create a separate model for attention extraction\n",
    "attention_model = Model(\n",
    "    inputs=model.input,\n",
    "    outputs=[model.get_layer('user_encoder').output[1], model.get_layer('news_encoder').output[1]]  # Adjust layer names as needed\n",
    ")\n",
    "\n",
    "# Select a sample from the test set\n",
    "sample = test_df.iloc[0]\n",
    "history_tokens = convert_history(sample['HistoryTitles'])\n",
    "candidate_tokens = sample['CandidateTitleTokens']\n",
    "\n",
    "# Prepare input\n",
    "history_padded = pad_sequences(\n",
    "    [history_tokens],\n",
    "    maxlen=max_history_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=0\n",
    ")\n",
    "candidate_padded = pad_sequences(\n",
    "    [candidate_tokens],\n",
    "    maxlen=max_title_length,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=0\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    'history_input': np.array(history_padded),\n",
    "    'candidate_input': np.array(candidate_padded)\n",
    "}\n",
    "\n",
    "# Get attention weights\n",
    "user_attention, candidate_attention = attention_model.predict(inputs)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap(user_attention, annot=False, cmap='viridis', cbar=False)\n",
    "plt.title('User History Attention Weights')\n",
    "plt.xlabel('History Sequence')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.heatmap(candidate_attention, annot=False, cmap='viridis', cbar=False)\n",
    "plt.title('Candidate News Attention Weights')\n",
    "plt.xlabel('Candidate Sequence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff986ae-34a5-495a-8d1c-a3e479417709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_batches(df, batch_size):\n",
    "    while True:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the data\n",
    "        for start in range(0, len(df), batch_size):\n",
    "            end = min(start + batch_size, len(df))\n",
    "            batch_df = df.iloc[start:end]\n",
    "            # Prepare inputs\n",
    "            history_batch = []\n",
    "            candidate_batch = []\n",
    "            labels_batch = []\n",
    "            for idx, row in batch_df.iterrows():\n",
    "                history_titles = row['HistoryTitles']  # Use the tokenized titles\n",
    "                # Pad each title in history\n",
    "                history_titles_padded = pad_sequences(history_titles, maxlen=max_title_length, padding='post', truncating='post', value=0)\n",
    "                # Pad or truncate the history to max_history_length\n",
    "                if len(history_titles_padded) < max_history_length:\n",
    "                    padding = np.zeros((max_history_length - len(history_titles_padded), max_title_length), dtype='int32')\n",
    "                    history_titles_padded = np.vstack([padding, history_titles_padded])\n",
    "                else:\n",
    "                    history_titles_padded = history_titles_padded[-max_history_length:]\n",
    "                candidate_title = row['CandidateTitleTokens']\n",
    "                candidate_title_padded = pad_sequences([candidate_title], maxlen=max_title_length, padding='post', truncating='post', value=0)[0]\n",
    "                history_batch.append(history_titles_padded)\n",
    "                candidate_batch.append(candidate_title_padded)\n",
    "                labels_batch.append(row['Label'])\n",
    "            yield [np.array(history_batch), np.array(candidate_batch)], np.array(labels_batch)\n",
    "# Assuming you have a test set prepared similarly\n",
    "test_generator = generate_batches(test_df, batch_size)\n",
    "test_steps = len(test_df) // batch_size\n",
    "# Evaluate using model.evaluate()\n",
    "evaluation = model.evaluate(test_generator, steps=test_steps)\n",
    "\n",
    "# Or compute metrics manually\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "for [history_batch, candidate_batch], labels_batch in test_generator:\n",
    "    preds = model.predict([history_batch, candidate_batch])\n",
    "    all_labels.extend(labels_batch)\n",
    "    all_preds.extend(preds)\n",
    "    if len(all_labels) >= len(test_df):\n",
    "        break\n",
    "all_labels = all_labels[:len(test_df)]\n",
    "all_preds = all_preds[:len(test_df)]\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(all_labels, all_preds)\n",
    "print(f'Test AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f5bbb-a88a-45bf-94fd-448e72a2bd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a547fc-f992-4730-b96d-b37710856427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f47309-0bae-42a2-8d2d-7609b8ec11f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ac5cc-6510-47f9-9f43-11cbb8d11083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
